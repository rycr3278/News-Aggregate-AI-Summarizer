Eric Schmidt has a 6-point plan for fighting election misinformation
https://www.technologyreview.com/2023/12/15/1085441/eric-schmidt-plan-for-fighting-election-misinformation/
December 15, 2023
Eric Schmidt
The former Google CEO hopes that companies, Congress, and regulators will take his advice on board—before it’s too late. The coming year will be one of seismic political shifts.Over 4 billionpeople will head to the polls in countries including the United States, Taiwan, India, and Indonesia, making 2024 the biggest election year in history. And election campaigns are using artificial intelligence in novel ways. Earlier this year in the US, the Republican presidential primary campaign of Florida governor Ron DeSantis posteddoctored imagesof Donald Trump; the Republican National Committee released anAI-created addepicting a dystopian future in response to Joe Biden’s announcing his reelection campaign; and just last month, Argentina’s presidential candidates each createdan abundance of AI-generated contentportraying the other party in an unflattering light. This surge in deepfakes heralds a new political playing field. Over the past year, AI was used in at least 16 countries to sow doubt, smear opponents, or influence public debate, according to areportreleased by Freedom House in October. We’ll need to brace ourselves for more chaos as key votes unfold across the world in 2024. The year ahead will also bring a paradigm shift for social media platforms. The role  of Facebook and others has conditioned our understanding of social media as centralized, global “public town squares” with a never-ending stream of content and frictionless feedback. Yet themayhemon X (a.k.a. Twitter) anddeclining useof Facebook among Gen Z—alongside the ascent of apps like TikTok and Discord—indicate that the future of social media may look very different. In pursuit of growth, platforms have embraced the amplification of emotions through attention-driven algorithms and recommendation-fueled feeds. But that’s taken agency away from users (we don’t control what we see) and has instead left us with conversations full of hate and discord, as well as a growing epidemic of mental-health problems among teens. That’s a far cry from the global, democratized one-world conversation the idealists dreamed of 15 years ago. With many users left adrift and losing faith in these platforms, it’s clear that maximizing revenue has ironically hurt business interests. Now, with AI starting to make social mediamuch more toxic, platforms and regulators need to act quickly to regain user trust and safeguard our democracy. Here I propose six technical approaches that platforms should double down on to protect their users. Regulations and laws will play a crucial role in incentivizing or mandating many of these actions. And while these reforms won’t solve all the problems of mis- and disinformation, they can help stem the tide ahead of elections next year. 1.Verify human users.We need to distinguish humans using social media from bots, holding both accountable if laws or policies are violated. This doesn’t mean divulging identities. Think of how we feel safe enough to hop into a stranger’s car because we see user reviews and know that Uber has verified the driver’s identity. Similarly, social media companies need to authenticate the human behind each account and introduce reputation-based functionality to encourage accounts to earn trust from the community. 2.Know every source.Knowing the provenance of the content and the time it entered the network can improve trust and safety. As a first step, using a time stamp and an encrypted (and not removable) IP address would guarantee an identifiable point of origin. Bad actors and their feeds—discoverable through the chain of custody—could be deprioritized or banned instead of being algorithmically amplified. While VPN traffic may deter detection, platforms can step up efforts to improve identification of VPNs. In a new report, Freedom House documents the ways governments are now using the tech to amplify censorship. 3.Identify deepfakes. In line with President Biden’s sweepingexecutive order on AI, which requires the Department of Commerce to develop guidance for watermarking AI-generated content, platforms should further develop detection and labeling tools. One way for platforms to start is to scan an existing database of images and tell the user if an image has no history (Google Images, for example, has begun to do this). AI systems can also be trained to detect the signatures of deepfakes, using large sets of truthful images contrasted with images labeled as fake. Such software can tell you when an image has a high likelihood of being a deepfake, similar to the “spam risk” notice you get on your phone when calls come in from certain numbers. 4.Filter advertisers.Companies can share a “safe list” of advertisers across platforms, approving those who comply with applicable advertising laws and conform professionally to the platforms’ advertising standards. Platforms also need to ramp up their scrutiny of political ads, adding prominent disclaimers if synthetic content is used. Meta, for example, announced this month that it would require political ads todisclosewhether they used AI. 5.Use real humans to help. There will, of course, be mistakes, and some untrustworthy content will slip through the protections. But the case of Wikipedia shows that misinformation can be policed by humans who follow clear and highly detailed content rules. Social media companies, too, should publish quality rules for content and enforce them by further equipping their trust and safety teams, and potentially augmenting those teams by providing tools to volunteers. How humans fend off an avalanche of AI-generated material from chatbots remains to be seen, but the task will be less daunting if trained AI systems are deployed to detect and filter out such content. 6.Invest in research.For all these approaches to work at scale, we’ll require long-term engagement, starting now. My philanthropic group is working to help create free, open-source testing frameworks for many AI trust and safety groups. Researchers, the government, and civil society will also need increased access to critical platform data. One promising bill is thePlatform Accountability and Transparency Act, which would, for example, require platforms to comply with data requests from projects approved by the National Science Foundation. With a concerted effort from companies, regulators, and Congress, we can adopt these proposals in the coming year, in time to make a difference. My worry is that everyone benefits from favorable mis- or disinformation to varying degrees: our citizens are amused by such content, our political leaders may campaign with it, and the media garners traffic by covering sensationalist examples. The existing incentive structures will make misinformation hard to eliminate. Social media platforms need to fundamentally rethink their design for the age of AI, especially as democracies face a historic test worldwide. It’s clear to me the future will be one of many decentralized online spaces that cater to every interest, reflect the views of real humans (not bots), and focus on concrete community concerns. But until that day comes, setting these guardrails in place will help ensure that platforms maintain a healthy standard of discourse and do not let opaque, engagement-driven algorithms allow AI-enabled election content to run rampant. Eric Schmidt was the CEO of Google from 2001 to 2011. He is currently cofounder of Schmidt Futures, a philanthropic initiative that bets early on exceptional people making the world better, applying science and technology, and bringing people together across fields Experts say its emphasis on content labeling, watermarking, and transparency represents important steps forward. Legislators are responding quickly after teens used AI to create nonconsensual sexually explicit images. Here's what you need to know. There’s still so much we don’t know about social media’s impact. But Meta president of global affairs Nick Clegg tells MIT Technology Review that he hopes new tools the company just released will start to change that.

Needle-free covid vaccines are (still) in the works
https://www.technologyreview.com/2023/12/15/1085389/needle-free-covid-vaccines-are-still-in-the-works/
December 15, 2023
Cassandra Willyard
Vaccines delivered through the nose or mouth should help stop infection where it begins. But researchers are still working to collect the data they need to prove it. This article first appeared in The Checkup, MIT Technology Review's weekly biotech newsletter. To receive it in your inbox every Thursday, and read articles like this first,sign up here. Covid shots do an admirable job of boosting our immune response enough to protect against serious illness, but they don’t boost immunity in the one spot we’d like them to: our airways. That’s why researchers have been working on vaccines you breathe into your lungs or spray into your nose. The idea is that these vaccines will elicit an immune response in the mucous membranes of your respiratory tract that might help stave off infection or, if you do become infected, make you less likely to transmit the virus. These “mucosal” covid vaccines aren’t available in the US or Europe, but they are in other parts of the world. When we last reported on efforts to develop a mucosal vaccinein 2022, two had been approved in China and India. Now five are in use in China, India, Iran, Indonesia, Morocco, and Russia. A couple dozen more candidates are in clinical trials. And many, many more are on the way. This week I came acrossa paper from a team in Chinadeveloping another inhalable vaccine. This vaccine differs from most others in at least one notable way: it is a powder, which means that it’s shelf stable and doesn’t need refrigeration. That would make it easier to transport and deliver, especially to places where refrigeration is difficult. This candidate won’t be available anytime soon. It’s still in preclinical development, along with more than a hundred other similar vaccines. But now that we’re almost four years out from the start of the pandemic, it seems like a good time to take stock. When will the US get its first mucosal covid vaccine? What will it look like? And will it work as intended? Only one mucosal vaccine—FluMist—has ever been approved in the US, and that happened two decades ago. But efforts to develop one for covid are moving quickly. So when might the US see its first mucosal covid vaccine? “Maybe never. But I think there’s increasing likelihood that it may happen before the end of 2024,” speculated Eric Topol, a cardiologist who has been following Covid research closely since 2020, in arecent newsletter. New covid vaccines inhaled through the nose and mouth could help prevent people from becoming infected or passing on the virus—but questions remain. The federal government is working to speed things along with an injection of cash through Project NextGen, a $5 billion effort to usher new and improved covid vaccines to market. In October, the Department of Health and Human Servicesannounced that nearly $20 million would go to two companiesdeveloping mucosal vaccines—Codagenix and CastleVax. That money will help the companies gear up for studies to test how well their vaccines work to prevent symptomatic infections. Codagenix’s candidate, a nasal vaccine called CoviLiv, is already part of a phase 3 global efficacy trial coordinated by the World Health Organization. And in October, the company reported results from a safety study in adults in the UK who had never been vaccinated for covid before. The nasal mist prompted robust immunity, at least as measured by markers in the blood. But evidence of an immune response in the blood doesn’t necessarily indicate an immune response in the mucosal lining of the airways. Or,as one physician puts it, “just like the ‘far, dark side of the Moon’, which is invisible from the earth, the mucosal response to pathogens is a far, dark side of immunity that is poorly or not visible from the peripheral blood and more complicated to probe than systemic immunity.” TBD. Different groups are trying a variety of strategies. The goal is to induce immunity in the airways that is robust, broad, and durable. But which strategy will succeed is a bit of a question mark at the moment. Mucosal vaccines fall into a few categories depending on how they’re administered and the platform they use. Some are sprays that are squirted into the nose (CovLiv, for example). Others are meant to be inhaled into the lungs (such as one developed by CanSinBIO in China). Sometimes these two routes of administration get lumped together, but they actually are very different, says Mangalakumari Jeyanathan, a researcher at McMaster University and coauthor ofan editorialthat accompanies the new inhalable-vaccine paper. With a nasal vaccine, the contents go into the nasal cavity. But Jeyanathan thinks inhaled vaccines, which go deep into the lungs, are likely to work better. Her team’s research suggests that nasal vaccines induce immune responses only in the upper respiratory tract, not in the lower respiratory tract. That means, she says, that if the vaccine doesn’t prevent infection, the lungs are still vulnerable, and “we really need the immune responses to prevent any sort of serious damage to the lung.” The vaccine outlined in the recent Nature paper is meant to be inhaled. It is a subunit vaccine, meaning it contains a portion of the pathogen. In this case, the subunit is actually a piece of cholera toxin that has been engineered to display a portion of SARS-CoV-2. These proteins are placed into microcapsules small enough to travel deep into the lungs. Maybe.Studies showthat people who have been infected and vaccinated do have better mucosal immunity than people who have been vaccinated but not infected. But Jeyanathan says her group has also seen quite a few people who have been infected and don’t have much mucosal immunity in their lungs. When they wash the lungs with saline to collect samples from the lower respiratory tract, they don’t find detectable T-cell responses. “It’s really sort of very strange,” she says. But it’s not just about whether you’ve got mucosal immunity. It also matters how broad that immunity is. One of the most problematic things about SARS-CoV-2 is that it’s constantly evolving. Each month seems to bring a new variant. The changes mainly affect the spike protein, the target of all current vaccines. But some groups are working to variant-proof their mucosal vaccines. Jeyanathan’s group is putting in parts of the interior of the covid virus, which aren’t apt to change as quickly as the portion that binds to cells. “So that way, we don't need to do this variant-chasing approach,” she says. Regulators are still trying to work out how to measure success. In some cases, companies can demonstrate vaccine effectiveness through surrogate markers such as antibody levels. That’s how the latest boosters were approved. But with mucosal vaccines, it’s not clear what surrogate marker would be most useful. Antibody levels in the nose or mouth? Or the abundance of certain immune cells? Inan editorial published a year ago, Peter Marks from the FDA and colleagues argued that vaccines that differ substantially from those already approved might need to be tested in large, randomized clinical trials. What we really want to see is that these next-generation vaccines outperform existing vaccines and curb transmission. That data isn’t in yet, and it could take years before we know whether mucosal vaccines actually do what we hope they will: stop the virus from spreading. Vertex, maker of the recently approved CRISPR sickle-cell therapy, has agreed to pay tens of millions of dollars to avoid any patent infringement lawsuits. Antonio Regaladohas the story. When the first two mucosal vaccines were approved in 2022, we publishedan explainerby Jessica Hamelzou. Wouldn’t it be wonderful if we had a vaccine that worked against all coronaviruses? One team’s mosaic nanoparticle may be the key to success,reports Adam Piore. The first gene therapies for sickle-cell disease have arrived, but patients in the countries with the greatest burden of the disease won’t be able to access them.  (NYT) CAR-T, a cell therapy developed to treat cancer, has seemingly eliminated autoimmune disease in 15 patients (Nature) The US Supreme Court plans to review a case that could affect access to the abortion medication mifepristone. (Washington Post) A single hormone seems to be to blame for morning sickness, a discovery that may lead to better treatments. (NYT) Vertex has been in the headlines for its newly approved sickle-cell therapy, but the company is also closing in on a non-opioid painkiller. Here’s a fascinating deep dive into the backstory. (Stat) New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. After deafness treatment, Yiyi can hear her mother and dance to the music. But why is it so noisy at night? As a patient enrolled in a clinical trial for Vertex’s new treatment, I was among the first to experience CRISPR’s transformative effects. Gene editing for sickle-cell is here. This is how researchers knew what DNA to change.

Google DeepMind used a large language model to solve an unsolvable math problem
https://www.technologyreview.com/2023/12/14/1085318/google-deepmind-large-language-model-solve-unsolvable-math-problem-cap-set/
December 14, 2023
Will Douglas Heaven
They had to throw away most of what it produced but there was gold among the garbage. Google DeepMind has used a large language model to crack a famous unsolved problem in pure mathematics. In apaper published in Naturetoday, the researchers say it is the first time a large language model has been used to discover a solution to a long-standing scientific puzzle—producing verifiable and valuable new information that did not previously exist. “It’s not in the training data—it wasn’t even known,” says coauthor Pushmeet Kohli, vice president of research at Google DeepMind. Large language models have a reputation for making things up, not for providing new facts. Google DeepMind’s new tool, called FunSearch, could change that. It shows that they can indeed make discoveries—if they are coaxed just so, and if you throw out the majority of what they come up with. FunSearch (so called because it searches for mathematical functions, not because it’s fun) continues a streak of discoveries in fundamental math and computer science that DeepMind has made using AI. FirstAlphaTensorfound a way to speed up a calculation at the heart of many different kinds of code, beating a 50-year record. ThenAlphaDevfound ways to make key algorithms used trillions of times a day run faster. Yet those tools did not use large language models. Built on top of DeepMind’s game-playing AI AlphaZero, both solved math problems by treating them as if they were puzzles in Go or chess. The trouble is that they are stuck in their lanes, says Bernardino Romera-Paredes, a researcher at the company who worked on both AlphaTensor and FunSearch: “AlphaTensor is great at matrix multiplication, but basically nothing else.” FunSearch takes a different tack. It combines a large language model called Codey, a version of Google’s PaLM 2 that isfine-tuned on computer code, with other systems that reject incorrect or nonsensical answers and plug good ones back in. “To be very honest with you, we have hypotheses, but we don’t know exactly why this works,” saysAlhussein Fawzi, a research scientist at Google DeepMind. “In the beginning of the project, we didn’t know whether this would work at all.” The researchers started by sketching out the problem they wanted to solve in Python, a popular programming language. But they left out the lines in the program that would specify how to solve it. That is where FunSearch comes in. It gets Codey to fill in the blanks—in effect, to suggest code that will solve the problem. A second algorithm then checks and scores what Codey comes up with. The best suggestions—even if not yet correct—are saved and given back to Codey, which tries to complete the program again. “Many will be nonsensical, some will be sensible, and a few will be truly inspired,” says Kohli. “You take those truly inspired ones and you say, ‘Okay, take these ones and repeat.’” After a couple of million suggestions and a few dozen repetitions of the overall process—which took a few days—FunSearch was able to come up with code that produced a correct and previously unknown solution to the cap set problem, which involves finding the largest size of a certain type of set. Imagine plotting dots on graph paper. The cap set problem is like trying to figure out how many dots you can put down without three of them ever forming a straight line. It’s super niche, but important. Mathematicians do not even agree on how to solve it, let alone what the solution is. (It is also connected to matrix multiplication, the computation thatAlphaTensor found a way to speed up.) Terence Tao at the University of California, Los Angeles, who has won many of the top awards in mathematics, including the Fields Medal, called the cap set problem “perhaps my favorite open question” in a 2007blog post. Tao is intrigued by what FunSearch can do. “This is a promising paradigm,” he says. “It is an interesting way to leverage the power of large language models.” A key advantage that FunSearch has over AlphaTensor is that it can, in theory, be used to find solutions to a wide range of problems. That’s because it produces code—a recipe for generating the solution, rather than the solution itself. Different code will solve different problems. FunSearch’s results are also easier to understand. A recipe is often clearer than the weird mathematical solution it produces, says Fawzi. To test its versatility, the researchers used FunSearch to approach another hard problem in math: the bin packing problem, which involves trying to pack items into as few bins as possible. This is important for a range of applications in computer science, from data center management to e-commerce. FunSearch came up with a way to solve it that’s faster than human-devised ones. Mathematicians are “still trying to figure out the best way to incorporate large language models into our research workflow in ways that harness their power while mitigating their drawbacks,” Tao says. “This certainly indicates one possible way forward.” The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems. It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it?

This new system can teach a robot a simple household task within 20 minutes
https://www.technologyreview.com/2023/12/14/1085231/new-system-teach-robot-household-task/
December 14, 2023
Rhiannon Williams
The Dobb-E domestic robotics system was trained in real people’s homes and could help solve the field’s data problem. A new system that teaches robots a domestic task in around 20 minutes could help the field of robotics overcome one of its biggest challenges:a lack of training data. The open-sourcesystem, called Dobb-E, was trained using data collected from real homes. It can help to teach a robot how to open an air fryer, close a door, or straighten a cushion, among other tasks. While other types of AI, such as large language models, are trained on huge repositories of data scraped from the internet, the same can’t be done with robots, because the data needs to be physically collected. This makes it a lot harder to build and scale training databases. Similarly, while it’s relatively easy to train robots to execute tasks inside a laboratory, these conditions don’t necessarily translate to the messy unpredictability of a real home. To combat these problems, the team came up with a simple, easily replicable way to collect the data needed to trainDobb-E—using an iPhone attached to a reacher-grabber stick, the kind typically used to pick up trash. Then they set the iPhone to record videos of what was happening. Volunteers in 22 homes in New York completed certain tasks using the stick, including opening and closing doors and drawers, turning lights on and off, and placing tissues in the trash. The iPhones’ lidar systems, motion sensors, and gyroscopes were used to record data on movement, depth, and rotation—important information when it comes to training a robot to replicate the actions on its own. After they’d collected just 13 hours’ worth of recordings in total, the team used the data to train an AI model to instruct a robot in how to carry out the actions. The model used self-supervised learning techniques, which teach neural networks to spot patterns in data sets by themselves, without being guided by labeled examples. The next step involved testing how reliably a commercially available robot called Stretch, which consists of a wheeled unit, a tall pole, and a retractable arm, was able to use the AI system to execute the tasks. An iPhone held in a 3D-printed mount was attached to Stretch’s arm to replicate the setup on the stick. The researchers tested the robot in 10 homes in New York over 30 days, and it completed 109 household tasks with an overall success rate of 81%. Each task typically took Dobb-E around 20 minutes to learn: five minutes of demonstration from a human using the stick and attached iPhone, followed by 15 minutes of fine-tuning, when the system compared its previous training with the new demonstration. Large language models combined with confidence scores help them recognize uncertainty. That could be key to making robots safe and trustworthy. Once the fine-tuning was complete, the robot was able to complete simple tasks like pouring from a cup, opening blinds and shower curtains, or pulling board-game boxes from a shelf. It could also perform multiple actions in quick succession, such as placing a can in a recycling bag and then lifting the bag. However, not every task was successful. The system was confused by reflective surfaces like mirrors. Also, because the robot’s center of gravity is low, tasks that require pulling something heavy at height, like opening fridge doors, proved too risky to attempt. The research represents tangible progress for the home robotics field, says Charlie C. Kemp, cofounder of the robotics firm Hello Robot and a former associate professor at Georgia Tech. Although the Dobb-E team used Hello Robot’s research robot, Kemp was not involved in the project. “The future of home robots is really coming. It’s not just some crazy dream anymore,” he says. “Scaling up data has always been a challenge in robotics, and this is a very creative, clever approach to that problem.” To date, Roomba and other robotic vacuum cleaners are the only real commercial home robot successes, says Jiajun Wu, an assistant professor of computer science at Stanford University who was not involved in the research. Their job is easier because Roombas don’t interact with objects—in fact, their aim is to avoid them. It’s much more challenging to develop home robots capable of doing a wider range of tasks, which is what this research could help advance. The NYU research team has made all elements of the project open source, and they’re hoping others will download the code and help expand the range of tasks that robots running Dobb-E will be able to achieve. “Our hope is that when we get more and more data, at some point when Dobb-E sees a new home, you don’t have to show it more examples,” says Lerrel Pinto, a computer science researcher at New York University who worked on the project. “We want to get to the point when we don’t have to teach the robot new tasks, because it already knows all the tasks in most houses,” he says. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems. It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it?

Two former Department of Energy staffers warn we’re doing carbon removal all wrong
https://www.technologyreview.com/2023/12/12/1085178/two-former-department-of-energy-staffers-warn-were-doing-carbon-removal-all-wrong/
December 12, 2023
James Temple
Sucking down greenhouse gas to cancel out corporate emissions may come at the expense of more pressing public needs. The carbon removal industry is just starting to take off, but some experts are warning that it’s already headed in the wrong direction. Two former staffers of the US agency responsible for advancing the technology argue that the profit-driven industry’s focus on cleaning up corporate emissions will come at the expense of helping to pull the planet back from dangerous levels of warming. Numerous studies have found that the world may have to remove tens of billions of tons of carbon dioxide from the atmosphere per year by around midcentury to keep global warming in check. These findings have spawned significant investments into startups developing carbon-sucking direct-air-capture factories and companies striving to harness the greenhouse-gas-trapping potential of plants, minerals, and the oceans. But a fundamental challenge is that carbon dioxide removal (CDR) isn’t a product that any person or company “needs,” in the traditional market sense. Rather, carrying it out provides a collective societal good, in the way that waste management does, only with larger global stakes. To date, it’s largely been funded by companies that are voluntarily paying for it as a form of corporate climate action in the face of rising investor, customer, employee, or regulatory pressures. That includes purchases of future removal through the$1 billion Frontier effort, started by Stripe and other companies. There’s also some growing government support in countries including the US, which isfunding carbon removal projects,offering a comparatively small amount of money to companiesthat provide the service andsubsidizingthose that store away carbon dioxide. Carbon emissions that cause climate change are on track to hit a record high this year, while efforts to remove them from the atmosphere are still minuscule. But in alengthy and pointed essaypublished in the journal Carbon Management on December 12, researchers Emily Grubert and Shuchi Talati argue there are rising dangers for the field. Both previously worked for the US Department of Energy’s Office of Fossil Energy and Carbon Management, which drove several of the recent US efforts to develop the industry. They write that the emergence of a for-profit, growth-focused sector selling a carbon removal product, instead of a publicly funded and coordinated effort more akin to waste management, “presents grave risks for the ability of CDR to enable net zero and net negative targets in general,” including keeping global warming at 1.5 ºC over preindustrial levels or pulling the planet back to that level. “If we missallocate our limited CDR resources and end up not having access to the capacity that can help meet the needs we really have, climatically, that’s a problem,” says Grubert, now an associate professor of sustainable energy policy at the University of Notre Dame. “It means we’re never going to get there.” One of their main concerns is that corporations have come to see carbon removal as a relatively simple and reliable way of canceling out ongoing climate pollution that they have other ways of cleaning up, which the authors refer to as “luxury” removal.That could significantly increase the total carbon removal the world would need to pull off, and effectively dedicate a large share of a limited resource to things that can be addressed directly. Moreover, it grants a significant slice of the world’s carbon removal capacity to profitable companies in rich nations rather than reserving it for higher-priority public goods, including allowing developing nations more time to reduce emissions; balancing out emissions from sectors we still don’t have ways of cleaning up, like agriculture; and drawing down historic emissions enough to bring global temperatures to safer levels. “You really need to save it for the stuff you can’t eliminate, not just the stuff that’s expensive to eliminate,” Grubert says. That means using carbon removal to address things like the emissions from the fertilizer used to feed populations in poor parts of the world, not for avoiding the hassle and expense of retrofitting a cement plant, she adds. “CDR cannot succeed at restorative and reparative goals if it is controlled by the same forces that created the problems it is trying to solve,” write Grubert and Talati, executive director of theAlliance for Just Deliberation on Solar Geoengineering. There is evidence that some companies have come to perceive carbon removal in the way that the authors describe. Earlier this year, Vicki Hollub, the chief executive of the oil and gas company Occidental, whichrecently acquireda direct-air-capture company,told the audienceat an energy conference: “We believe that our direct-capture technology is going to be the technology that helps to preserve our industry over time. This gives our industry a license to continue to operate for the 60, 70, 80 years that I think it’s going to be very much needed.” Part of the problem, the authors note, is that carbon removal is seen as  “unconstrained,” easily scaled to meet industry goals and climate needs. But in fact, it’s hard and expensive to do reliably. Direct-air-capture machines, for instance, require a lot of land and resources to build and a lot of energy to run, Talati says. That limits how big the sector can become and complicates the question of how much good the facilities do. Last week, the Global Carbon Project reported that the world’s technology-based carbon removal only sucked down about 10,000 tons this year, “significantly less than one-millionth of our fossil-fuel emissions,” as MIT Technology Reviewreported. Other means of carbon removal may be cheaper and more scalable, particularly methods that harness nature to do the job. But some of these approaches, including adding minerals to the oceans or sinking biomass in them, also raise concerns about environmental side effects or create added difficulties in certifying the climate benefits. Grubert and Talatai fear that growing market pressures, including the demand for low-cost carbon removal at high volume, could undermine how well such efforts are measured, reported, and verified over time. They add that the carbon removal market may simply replicate many of the problems in the traditional carbon offsets space, where researchershave foundthat efforts to plant trees or prevent deforestation often substantially exaggerate the amount of additional carbon trapped. Ultimately, the authors argue that the global task of drawing down billions of tons of carbon dioxide should largely be publicly funded, owned, and managed if we hope to achieve the global common good of stabilizing and repairing the climate. “There’s a role for the private sector, but our argument is that a purely profit-driven industry that’s currently operating with very little governance is going to go badly,” Talati says. “If we want to see this succeed, we can’t count on the self-governance of corporations, which we’ve seen fail over and over again, across every industry. The role of the public sector needs to be broadened and deepened.” Stripe didn’t respond to an inquiry before press time. But executives there have argued that Frontier is marshaling corporate funds and expertise to help build up an essential industry that will be needed to combat the dangers of climate change, enabling startups to move ahead with early demonstration projects and to test a variety of approaches to carbon removal. Major investors have also said that rising demand among corporations is helping to drive innovation and growth in the field. A spokesman for Heirloom, which is part of a team that recently secured Department of Energy funds to move ahead with a major direct-air-capture project in Louisiana, said it recognizes some of the risks that the authors raise and has taken steps to address them by committing to follow aclear set of corporate principles: “We believe decarbonization should be the #1 goal of climate mitigation, and CDR should be used for residual and legacy emissions. We feel strongly that CDR is not used as a fig leaf for emitting industries.” It uncovered systemic problems with offset markets and recommended that the public university system focus on cutting its direct emissions instead. Companies need to invest in energy-efficient infrastructure and optimize data practices, says Ian Clatworthy, director of data platform product marketing at Hitachi Vantara. A year ago, scientists generated net energy with a fusion reactor. This is what’s happened since then. Sustainable computing practices have the power to both infuse operational efficiencies and greatly reduce energy consumption, says Jen Huffstetler, chief product sustainability officer at Intel.

Vertex developed a CRISPR cure. It’s already on the hunt for something better.
https://www.technologyreview.com/2023/12/15/1085380/vertex-sickle-cell-pill-treatment/
December 15, 2023
Antonio Regalado
Thanks for the cure, CRISPR. Now companies want a pill to treat sickle-cell disease instead. The company that just got approval to sell the first gene-editing treatment in history, for sickle-cell disease, is already looking for an ordinary drug that could take its place. Vertex Pharmaceuticals has a 50-person team working “to make a pill that doesn’t do gene editing at all,” says David Altshuler, head of research at the Boston drug company. “We’re trying to out-innovate ourselves,” he says. Vertex won approval in the US to sell the world’s first treatment using CRISPR, the gene-editing technique, on December 8. It took eight years to develop, and at huge expense. Regulatory documents filed with the government during the approval process exceeded a million pages. Yet now that medicine’s CRISPR era has begun, some of the technique’s limitations are already visible. The company will pay rival Editas Medicine and the Broad Institute so that it can sell its breakthrough gene-editing treatment for sickle-cell disease. The treatment, called Casgevy, is both tough on patients and hugely expensive. Patients must spend several weeks in a hospital as doctors remove, genetically edit, and then reintroduce their bone-marrow stem cells, which make blood. The treatment will cost $2.2 million, not including hospital costs, according to Vertex. The company proved the gene fix can be a permanent remedy forpeople who have the most severe sickle-cell symptoms. These individuals, numbering around 16,000 in the U.S., suffer recurring pain crises when misshapen red blood cells block blood vessels in their bodies. But it’s unclear how many Americans will opt for gene editing. In an opinion column for MIT Technology Review, one patient who got the treatment,Jimi Olaghere, said the bone-marrow replacement an “intense months-long journey” that will create barriers to access. Previously, several gene therapieshave floundered in the marketplacebecause of a combination of high prices and too few patients. “It’s simultaneously a miracle and has a drawback that prevents wide use,” says Geoffrey von Maltzahn, a partner at Flagship Pioneering, who leads biotech ventures but was not involved in the sickle-cell treatment. “That is a common duality.” Such drawbacks are why a pill to alleviate sickle-cell, if developed, could sweep CRISPR from the playing field. A pill version could also resolve a brewing moral dilemma: Vertex so far has no plans to offer its gene-editing treatment in those countries where sickle-cell is most common. A wide ribbon of lower-income nations across the middle of Africa, including Nigeria and Ghana, account for 80% of sickle-cell cases but, according to US researchers, lack the hospitals, medical expertise, and money to implement this complex intervention. “One question I get a lot is: How are we going to get to the rest of the world?” says Altshuler. “And I think the answer is not by trying to do bone-marrow transplants in the rest of the world. It’s just too resource intensive, and the infrastructure is not there. I think the goal will be achieved sooner by finding another modality, like a pill that can be distributed much more effectively.” In an interview with MIT Technology Review, Altshuler outlined three ideas Vertex is exploring to improve on its breakthrough CRISPR treatment. One is to come up with a substitute for the intense chemotherapy that’s used to kill a person’s bone marrow and make space for the edited cells to take over. Vertex and other gene-editing companies, like Beam Therapeutics, say they are looking into gentler methods that could make the procedure easier for patients. A second strategy Vertex and other companies are exploring is called “in vivo” editing. That’s when gene-editing molecules are dripped directly into a person’s veins, or even injected like a vaccine, no transplant needed. To achieve in vivo editing for blood diseases, research groups are trying to develop homing systems—viruses or special nanoparticles—that would convey CRISPR directly to a person’s blood-making stem cells. Such “single shot” editing concepts have won substantial support from the Bill & Melinda Gates Foundation, which thinks it could help solve sickle-cell and HIV in Africa. But it remains at an experimental stage, and some question if it will ever be possible. The final idea is a conventional drug, the kind you swallow. That would be the easiest to distribute where it’s needed. Angela Koehler, a biochemist at MIT, says “broadly accessible” drugs with a “low barrier to access” would have the greatest impact on sickle-cell disease globally. “This does not diminish my excitement about the CRISPR-based approaches, but it partially explains the motivations of folks trying to develop ‘traditional’ drugs,” says Koehler. Sickle-cell is caused by defects in hemoglobin, the oxygen-carrying molecule in red blood cells. The CRISPR treatment stops the worst disease symptoms by making a targeted DNA edit that turns on “fetal hemoglobin,” a second version that we all have but is largely inactive after we’re born. By early 2019, Altshuler says, he had seen results from the first gene-edited patients who volunteered for the company’s trial. It was clear then that the theory was true: turning up fetal hemoglobin was a cure. Within weeks of seeing those results, Altshuler says he’d launched a hunt for a conventional drug that could do the same thing, even as the CRISPR program steamed ahead. “The goal is to achieve a similar profile with a pill instead of a gene editing,” he says. Reaching the whole world with a treatment was part of the motivation, but it wasn’t the only one. Part of what is driving Vertex is a painful lesson it learned following the 2011 launch of its breakthrough drug for hepatitis C, called Incivek. The drug had thefastest increase in sales for any product in history at the time, reaching $1.5 billion in a year. Yet within three years, Vertex had to stop selling Incivek after a competitor, Gilead Sciences, came up witha more effective alternative with fewer side effects. The brutal lesson: keep innovating. “Something I have never understood about biopharma: they discover the first medicine in a disease, and let other people eat their lunch,” Altshuler says. “They stop doing research and wait.” The pill hunt remains shrouded in secrecy—Altshuler won’t reveal any of the details, saying the lack of information in the public domain is part of what makes it an attractive project. But it’s likely that Vertex’s search centers on the same biological “target” that CRISPR changes. That is a gene calledBCL11A, which acts like a switch controlling fetal hemoglobin. Gene editing turns that gene down, allowing fetal hemoglobin to rise. It’s not easy to get an ordinary drug to copy that effect. The tricky part is that theBCL11Agene manufactures a transcription factor, a type of protein that’s floppy and formless and lacks the precise kinks and corners that chemists can aim drugs at. Indeed, such molecules have the reputation of being “undruggable.” According to Altshuler, no marketed drug currently works by binding to a transcription factor. Gene editing for sickle-cell is here. This is how researchers knew what DNA to change. Although the hunt for a drug has so far been low key and out of sight, clues have started to spill out, including some from other companies pursuing similar goals. This week, at a major blood-disorders meeting, the pharmaceutical company Novartis said it had screened several thousand molecules and found some that were able to raise fetal hemoglobin. Separately, a team at Children’s Hospital in Boston said at the same meeting it had made discoveries abouthow theBCL11Aprotein folds, highlighting potential ways drugs could act on it. That lab is led by Stuart Orkin, a scientistwhose discoveries about the fetal-hemoglobin switchwe recently profiled in MIT Technology Review. “Some people are trying to find new targets, but I don’t think there is anything else worth studying,” says Orkin. “I think it’s the only one that will get us to the other side of the problem.” Orkin says he’s been looking for a drug, too, but those attempts, some in collaboration with Koehler, have not yet paid off. “I can say we’ve tried a lot of things that don’t work,” he says. Orkin also still believes gene editing will be a better treatment, if you can get it. “If I had a child and the choice was a cure versus taking pills for life, I would go for the editing. If you can fix it, I would,” he says. “But many patients are not ready for the rigors of transplant, and many are not in a setting where it can be done. There aren’t enough hospitals or physicians.” And that is the irony of CRISPR’s first treatment. It can cure individuals but can’t yet conquer a disease. In fact, the problem of sickle-cell is only expanding. That is because countries with high rates of this inherited condition also have booming populations. Every year, more people, not fewer, suffer with the disease. CRISPR can’t yet reverse the trend, but a pill might. “It’s solved from a disease standpoint, but not a burden-of-disease standpoint,” says Orkin. “That is the next chapter. Sickle-cell is a big problem. And it’s growing, not shrinking.” New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. After deafness treatment, Yiyi can hear her mother and dance to the music. But why is it so noisy at night? As a patient enrolled in a clinical trial for Vertex’s new treatment, I was among the first to experience CRISPR’s transformative effects. Gene editing for sickle-cell is here. This is how researchers knew what DNA to change.

Now we know what OpenAI’s superalignment team has been up to
https://www.technologyreview.com/2023/12/14/1085344/openai-super-alignment-rogue-agi-gpt-4/
December 14, 2023
Will Douglas Heaven
The firm wants to prevent a superintelligence from going rogue. This is the first step. OpenAI has announced the first results from itssuperalignment team, the firm’s in-house initiative dedicated to preventing a superintelligence—a hypothetical future computer that can outsmart humans—from going rogue. Unlike many of the company’s announcements, this heralds no big breakthrough. In a low-key research paper, the team describes a technique that lets a less powerful large language model supervise a more powerful one—and suggests that this might be a small step toward figuring out how humans might supervise superhuman machines. Less than a month after OpenAI was rocked by a crisis when its CEO, Sam Altman, was fired by its oversight board (in an apparent coup led by chief scientist Ilya Sutskever) and then reinstated three days later, the message is clear: it’s back to business as usual. Yet OpenAI’s business is not usual. Many researchers still question whether machines will ever match human intelligence, let alone outmatch it. OpenAI’s team takes machines’ eventual superiority as given. “AI progress in the last few years has been just extraordinarily rapid,” says Leopold Aschenbrenner, a researcher on the superalignment team. “We’ve been crushing all the benchmarks, and that progress is continuing unabated.” For Aschenbrenner and others at the company, models with human-like abilities are just around the corner. “But it won’t stop there,” he says. “We’re going to have superhuman models, models that are much smarter than us. And that presents fundamental new technical challenges.” In July, Sutskever and fellow OpenAI scientist Jan Leike set up the superalignment team to address those challenges. “I’m doing it for my own self-interest,” Sutskevertold MIT Technology Review in September. “It’s obviously important that any superintelligence anyone builds does not go rogue. Obviously.” Amid speculation that Altman was fired for playing fast and loose with his company’s approach to AI safety, Sutskever’s superalignment team loomed behind the headlines. Many have been waiting to see exactly what it has been up to. The question the team wants to answer is how to rein in, or “align,” hypothetical future models that are far smarter than we are, known as superhuman models. Alignment means making sure a model does what you want it to do and does not do what you don’t want it to do. Superalignment applies this idea to superhuman models. One of the most widespread techniques used to align existing models is called reinforcement learning via human feedback. In a nutshell, human testers score a model’s responses, upvoting behavior that they want to see and downvoting behavior they don’t. This feedback is then used to train the model to produce only the kind of responses that human testers liked. This technique is a big part of what makes ChatGPT so engaging. The problem is that it requires humans to be able to tell what is and isn’t desirable behavior in the first place. But a superhuman model—the idea goes—might do things that a human tester can’t understand and thus would not be able to score. (It might even try to hide its true behavior from humans, Sutskever told us.) The researchers point out that the problem is hard to study because superhuman machines do not exist. So they used stand-ins. Instead of looking at how humans could supervise superhuman machines, they looked at how GPT-2, a model that OpenAI released five years ago, could supervise GPT-4, OpenAI’s latest and most powerful model. “If you can do that, it might be evidence that you can use similar techniques to have humans supervise superhuman models,” says Collin Burns, another researcher on the superalignment team. The team took GPT-2 and trained it to perform a handful of different tasks, including a set of chess puzzles and 22 common natural-language-processing tests that assess inference, sentiment analysis, and so on. They used GPT-2’s responses to those tests and puzzles to train GPT-4 to perform the same tasks. It’s as if a 12th grader were taught how to do a task by a third grader. The trick was to do it without GPT-4 taking too big a hit in performance. The results were mixed. The team measured the gap in performance between GPT-4 trained on GPT-2’s best guesses and GPT-4 trained on correct answers. They found that GPT-4 trained by GPT-2 performed 20% to 70% better than GPT-2 on the language tasks but did less well on the chess puzzles. The fact that GPT-4 outdid its teacher at all is impressive, says team member Pavel Izmailov: “This is a really surprising and positive result.” But it fell far short of what it could do by itself, he says. They conclude that the approach is promising but needs more work. “It is an interesting idea,” says Thilo Hagendorff, an AI researcher at the University of Stuttgart in Germany who works on alignment. But he thinks that GPT-2 might be too dumb to be a good teacher. “GPT-2 tends to give nonsensical responses to any task that is slightly complex or requires reasoning,” he says. Hagendorff would like to know what would happen if GPT-3 were used instead. He also notes that this approach does not address Sutskever’s hypothetical scenario in which a superintelligence hides its true behavior and pretends to be aligned when it isn’t. “Future superhuman models will likely possess emergent abilities which are unknown to researchers,” says Hagendorff. “How can alignment work in these cases?” But it is easy to point out shortcomings, he says. He is pleased to see OpenAI moving from speculation to experiment: “I applaud OpenAI for their effort.” OpenAI now wants to recruit others to its cause. Alongside this research update, the company announced anew $10 million money potthat it plans to use to fund people working on superalignment. It will offer grants of up to $2 million to university labs, nonprofits, and individual researchers and one-year fellowships of $150,000 to graduate students. “We’re really excited about this,” says Aschenbrenner. “We really think there’s a lot that new researchers can contribute.” The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems. It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it?

Vertex will pay tens of millions to license a controversial CRISPR patent
https://www.technologyreview.com/2023/12/13/1085209/vertex-license-controversial-crispr-patent-editas/
December 13, 2023
Antonio Regalado
The company will pay rival Editas Medicine and the Broad Institute so that it can sell its breakthrough gene-editing treatment for sickle-cell disease. Vertex Pharmaceuticals has agreed to buy rights to use a dominant CRISPR patent owned by the Broad Institute of Harvard and MIT, avoiding a potential lawsuit over its new gene-editing treatment for sickle-cell disease. The agreement allows Vertex to start selling its treatment, approved last Friday, without fear of patent infringement claims. The one-time treatment will be among the most expensive ever sold, with a price tag of $2.2 million. Thepatent on CRISPRhas been the fulcrum off a decade-long legal fight after the Broad Institute, a research center in Cambridge, Massachusetts, snatched rights to the most important uses of the gene-editing tool in 2014. Broad’s patent claims have been opposed by the University of California, Berkeley, which says researchers Jennifer Doudna and Emmanuelle Charpentier are the tool’s true inventors. The pair won a Nobel Prize in 2020 for their work on the technology. An exclusive license to the Broad Institute patents for human use was previously sold to Editas Medicine, a competing CRISPR editing company, which has its own treatment for sickle-cell disease in the works. Under an agreement with Editas announced today, Vertex agreed to pay it $50 million and annual fees of between $10 million and $40 million a year until 2034, when the patent expires. Of this money, the Broad Institute and Harvard University, whose employees are listed on key patent claims, will receive a percentage in the “mid double digits.” Broad manages a portfolio of CRISPR patents on behalf of itself, MIT and Harvard. A spokesperson for the Broad Institute, David Cameron, did not answer questions about whether any of the funds from Editas also benefit MIT, which is the publisher of this website. In our Checkup newsletter two weeks ago,we predicted that the patent issue could come to a head, but some researchers told us a lawsuit was unlikely, because it would stand in the way of cures. Reached for comment last week, David Altshuler, the head of research at Vertex, said the company was “confident in our patent position.” By that time, however, he likely knew a deal was close and that Vertex would gain rights to use the Broad patents. Before joining Vertex in 2015, Altshuler was a senior deputy at the Broad Institute, even sharing an office area and lab space with Feng Zhang, the center’s key CRISPR scientist, whose name is on the patents (and who also contributed to early work on the sickle-cell treatment). Given that background, some observers believed a settlement was likely. A Vertex spokesperson declined to comment on the arrangement. Ina press release, Editas said the windfall would allow it to finance its operations through 2026. It’s not yet clear if the license agreement points to an end of the fierce patent fight between Broad and Berkeley. That has been continuing before a US patent court, with Berkeley still trying to overturn its rival's claims. “This license does not seem to end the decade-long dispute between Doudna and Zhang,” says Jacob Sherkow, a professor at the University of Illinois College of Law. “Is it going to end, or is this license just a one-off?” Despite having a population of just 1,400, until recently, Tokelau’s .tk domain had more users than any other country. Here’s why. New capabilities unlocked by advanced analytics, AI, and machine learning enable smarter business buying. A decade after the high profile bust of cleantech 1.0, venture-backed firms are again flourishing. We need them to succeed. Will they? In a bid to find new economic growth, Hong Kong’s government is busy setting up legal frameworks to court Web3 companies.

This new data poisoning tool lets artists fight back against generative AI
https://www.technologyreview.com/2023/10/23/1082189/data-poisoning-artists-fight-generative-ai/
October 23, 2023
Melissa Heikkilä
The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. A new tool lets artists add invisible changes to the pixels in their art before they upload it online so that if it’s scraped into an AI training set, it can cause the resulting model to break in chaotic and unpredictable ways. The tool, called Nightshade, is intended as a way to fight back against AI companies that use artists’ work to train their models without the creator’s permission. Using it to “poison” this training data could damage future iterations of image-generating AI models, such as DALL-E, Midjourney, and Stable Diffusion, by rendering some of their outputs useless—dogs become cats, cars become cows, and so forth. MIT Technology Review got an exclusivepreview of the research, which has been submitted for peer review at computer security conference Usenix. AI companies such as OpenAI, Meta, Google, and Stability AI are facing a slew of lawsuits from artists who claim that theircopyrighted materialandpersonal informationwas scraped without consent or compensation. Ben Zhao, a professor at the University of Chicago, who led the team that created Nightshade, says the hope is that it will help tip the power balance back from AI companies towards artists, by creating a powerful deterrent against disrespecting artists’ copyright and intellectual property. Meta, Google, Stability AI, and OpenAI did not respond to MIT Technology Review’s request for comment on how they might respond. Zhao’s team also developedGlaze, a tool that allows artists to “mask” their own personal style to prevent it from being scraped by AI companies. It works in a similar way to Nightshade: by changing the pixels of images in subtle ways that are invisible to the human eye but manipulate machine-learning models to interpret the image as something different from what it actually shows. The team intends to integrate Nightshade intoGlaze, and artists can choose whether they want to use the data-poisoning tool or not. The team is also making Nightshade open source, which would allow others to tinker with it and make their own versions. The more people use it and make their own versions of it, the more powerful the tool becomes, Zhao says. The data sets for large AI models can consist of billions of images, so the more poisoned images can be scraped into the model, the more damage the technique will cause. Nightshade exploits asecurity vulnerabilityin generative AI models, one arising from the fact that they are trained on vast amounts of data—in this case, images that have been hoovered from the internet. Nightshade messes with those images. Greg Rutkowski is a more popular prompt than Picasso. Artists who want to upload their work online but don’t want their images to be scraped by AI companies can upload them to Glaze and choose to mask it with an art style different from theirs. They can then also opt to use Nightshade. Once AI developers scrape the internet to get more data to tweak an existing AI model or build a new one, these poisoned samples make their way into the model’s data set and cause it to malfunction. Poisoned data samples can manipulate models into learning, for example, that images of hats are cakes, and images of handbags are toasters. The poisoned data is very difficult to remove, as it requires tech companies to painstakingly find and delete each corrupted sample. The researchers tested the attack on Stable Diffusion’s latest models and on an AI model they trained themselves from scratch. When they fed Stable Diffusion just 50 poisoned images of dogs and then prompted it to create images of dogs itself, the output started looking weird—creatures with too many limbs and cartoonish faces. With 300 poisoned samples, an attacker can manipulate Stable Diffusion to generate images of dogs to look like cats. Generative AI models are excellent at making connections between words, which helps the poison spread. Nightshade infects not only the word “dog” but all similar concepts, such as “puppy,” “husky,” and “wolf.” The poison attack also works on tangentially related images. For example, if the model scraped a poisoned image for the prompt “fantasy art,” the prompts “dragon” and “a castle inThe Lord of the Rings” would similarly be manipulated into something else. Zhao admits there is a risk that people might abuse the data poisoning technique for malicious uses. However, he says attackers would need thousands of poisoned samples to inflict real damage on larger, more powerful models, as they are trained on billions of data samples. “We don’t yet know of robust defenses against these attacks. We haven’t yet seen poisoning attacks on modern [machine learning] models in the wild, but it could be just a matter of time,” says Vitaly Shmatikov, a professor at Cornell University who studies AI model security and was not involved in the research. “The time to work on defenses is now,” Shmatikov adds. Gautam Kamath, an assistant professor at the University of Waterloo who researches data privacy and robustness in AI models and wasn’t involved in the study, says the work is “fantastic.” The research shows that vulnerabilities “don’t magically go away for these new models, and in fact only become more serious,” Kamath says. “This is especially true as these models become more powerful and people place more trust in them, since the stakes only rise over time.” Junfeng Yang, a computer science professor at Columbia University, who has studied the security of deep-learning systems and wasn’t involved in the work, says Nightshade could have a big impact if it makes AI companies respect artists’ rights more—for example, by being more willing to pay out royalties. AI companies that have developed generative text-to-image models, such as Stability AI and OpenAI, have offered to let artistsopt out of having their images used to trainfuture versions of the models. But artists say this is not enough. Eva Toorenent, an illustrator and artist who has used Glaze, says opt-out policies require artists to jump through hoops and still leave tech companies with all the power. Toorenent hopes Nightshade will change the status quo. “It is going to make [AI companies] think twice, because they have the possibility of destroying their entire model by taking our work without our consent,” she says. Autumn Beverly, another artist, says tools like Nightshade and Glaze have given her the confidence to post her work online again. She previously removed it from the internet after discovering it had been scraped without her consent into the popular LAION image database. “I’m just really grateful that we have a tool that can help return the power back to the artists for their own work,” she says. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems. It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it? They had to throw away most of what it produced but there was gold among the garbage.

The Biggest Questions: What is death?
https://www.technologyreview.com/2023/11/17/1082937/what-is-death/
November 17, 2023

New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. Just as birth certificates note the time we enter the world, death certificates mark the moment we exit it. This practice reflects traditional notions about life and death as binaries. We are here until, suddenly, like a light switched off, we are gone. But while this idea of death is pervasive, evidence is building that it is an outdated social construct, not really grounded in biology. Dying is in fact a process—one with no clear point demarcating the threshold across which someone cannot come back. Scientists and many doctors have already embraced this more nuanced understanding of death. As society catches up, the implications for the living could be profound. “There is potential for many people to be revived again,” says Sam Parnia, director of critical care and resuscitation research at NYU Langone Health. Neuroscientists, for example, are learning that the brain can survive surprising levels of oxygen deprivation. This means the window of time that doctors have to reverse the death process could someday be extended. Other organs likewise seem to be recoverable for much longer than is reflected in current medical practice, opening up possibilities for expanding the availability of organ donations. To do so, though, we need to reconsider how we conceive of and approach life and death. Rather than thinking of death as an event from which one cannot recover, Parnia says, we should instead view it as a transient process of oxygen deprivation that has the potential to become irreversible if enough time passes or medical interventions fail. If we adopt this mindset about death, Parnia says, “then suddenly, everyone will say, ‘Let’s treat it.’” Legal and biological definitions of death typically refer to the “irreversible cessation” of life-sustaining processes supported by the heart, lungs, and brain. The heart is the most common point of failure, and for the vast majority of human history, when it stopped there was generally no coming back. That changed around 1960, with the invention of CPR. Until then, resuming a stalled heartbeat had largely been considered the stuff of miracles; now, it was within the grasp of modern medicine. CPR forced the first major rethink of death as a concept. “Cardiac arrest” entered the lexicon, creating a clear semantic separation between the temporary loss of heart function and the permanent cessation of life. Around the same time, the advent of positive-pressure mechanical ventilators, which work by delivering breaths of air to the lungs, began allowing people who incurred catastrophic brain injury—for example, from a shot to the head, a massive stroke, or a car accident—to continue breathing. In autopsies after these patients died, however, researchers discovered that in some cases their brains had been so severely damaged that the tissue had begun to liquefy. In such cases, ventilators had essentially created “a beating-heart cadaver,” says Christof Koch, a neuroscientist at the Allen Institute in Seattle. These observations led to the concept of brain death and ushered in medical, ethical, and legal debate about the ability to declare such patients dead before their heart stops beating. Many countries did eventually adopt some form of this new definition. Whether we talk about brain death or biological death, though, the scientific intricacies behind these processes are far from established. “The more we characterize the dying brain, the more we have questions,” says Charlotte Martial, a neuroscientist at the University of Liège in Belgium. “It’s a very, very complex phenomenon.” Traditionally, doctors have thought that the brain begins incurring damage minutes after it’s deprived of oxygen. While that’s the conventional wisdom, says Jimo Borjigin, a neuroscientist at the University of Michigan, “you have to wonder, why would our brain be built in such a fragile manner?” The system, previously used to return circulation to severed pigs’ brains, can now restore some functions of cells in other vital organs Recent research suggests that perhaps it actually isn’t. In 2019, scientists reported in Naturethat they were able torestore a suite of functionsin the brains of 32 pigs that had been decapitated in a slaughterhouse four hours earlier. The researchers restarted circulation and cellular activity in the brains using an oxygen-rich artificial blood infused with a cocktail of protective pharmaceuticals. They also included drugs that stopped neurons from firing, preventing any chance that the pig brains would regain consciousness. They kept the brains alive for up to 36 hours before ending the experiment. “Our work shows there’s probably a lot more damage from lack of oxygen that’s reversible than people thought before,” says coauthor Stephen Latham, a bioethicist at Yale University. In 2022, Latham and colleagues published a second paper in Nature announcing that they’d been able torecover many functions in multiple organs, including the brain and heart, in whole-body pigs that had been killed an hour earlier. They continued the experiment for six hours and confirmed that the anesthetized, previously dead animals had regained circulation and that numerous key cellular functions were active. “What these studies have shown is that the line between life and death isn’t as clear as we once thought,” says Nenad Sestan, a neuroscientist at the Yale School of Medicine and senior author of both pig studies. Death “takes longer than we thought, and at least some of the processes can be stopped and reversed.” A handful of studies in humans have also suggested that the brain is better than we thought at handling a lack of oxygen after the heart stops beating. “When the brain is deprived of life-sustaining oxygen, in some cases there seems to be this paradoxical electrical surge,” Koch says. “For reasons we don’t understand, it’s hyperactive for at least a few minutes.” In a study published in September in Resuscitation, Parnia and his colleagues collected brain oxygen and electrical activity data from 85 patients who experienced cardiac arrest while they were in the hospital. Most of the patients’ brain activity initially flatlined on EEG monitors, but for around 40% of them, near-normalelectrical activity intermittently reemergedin their brains up to 60 minutes into CPR. Similarly, in a study published in Proceedings of the National Academy of Sciences in May, Borjigin and her colleagues reportedsurges of activityin the brains of two comatose patients after their ventilators had been removed. The EEG signatures occurred just before the patients died and had all the hallmarks of consciousness, Bojigin says. While many questions remain, such findings raise tantalizing questions about the death process and the mechanisms of consciousness. The more scientists can learn about the mechanisms behind the dying process, the greater the chances of developing “more systematic rescue efforts,” Borjigin says. In best-case scenarios, she adds, this line of study could have “the potential to rewrite medical practices and save a lot of people.” Everyone, of course, does eventually have to die and will someday be beyond saving. But a more exact understanding of the dying process could enable doctors to save some previously healthy people who meet an unexpected early end and whose bodies are still relatively intact. Examples could include people who suffer heart attacks, succumb to a deadly loss of blood, or choke or drown. The fact that many of these people die and stay dead simply reflects “a lack of proper resource allocation, medical knowledge, or sufficient advancement to bring them back,” Parnia says. Borjigin’s hope is to eventually understand the dying process “second by second.” Such discoveries could not only contribute to medical advancements, she says, but also “revise and revolutionize our understanding of brain function.” Sestan says he and his colleagues are likewise working on follow-up studies that seek to “perfect the technology” they have used to restore metabolic function in pig brains and other organs. This line of research could eventually lead to technologies that are able to reverse damage—up to a point, of course—from oxygen deprivation in the brain and other organs in people whose hearts have stopped. If successful, the method could also expand the pool of available organ donors, Sestan adds, by lengthening the window of time doctors have to recover organs from the permanently deceased. If these breakthroughs do come, Sestan emphasizes, they will take years of research. “It’s important that we not overexaggerate and promise too much,” he says, “although that doesn’t mean we don’t have a vision.” In the meantime, ongoing investigations into the dying process will no doubt continue to challenge our notions of death, leading to sea changes within science and other realms of society, from the theological to the legal. As Parnia says: “Neuroscience doesn’t own death. We all have a stake in it.” Rachel Nuwer is a freelance science journalist who regularly contributes to the New York Times, Scientific American, Nature and more. Her latest book is I Feel Love: MDMA and the Quest for Connection in a Fractured World. She lives in Brooklyn.  After deafness treatment, Yiyi can hear her mother and dance to the music. But why is it so noisy at night? As a patient enrolled in a clinical trial for Vertex’s new treatment, I was among the first to experience CRISPR’s transformative effects. Gene editing for sickle-cell is here. This is how researchers knew what DNA to change. CRISPR is being used in an experimental effort to eliminate the virus that causes AIDS.

Rogue superintelligence and merging with machines: Inside the mind of OpenAI’s chief scientist
https://www.technologyreview.com/2023/10/26/1082398/exclusive-ilya-sutskever-openais-chief-scientist-on-his-hopes-and-fears-for-the-future-of-ai/
October 26, 2023
Will Douglas Heaven
An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. Ilya Sutskever, head bowed, is deep in thought. His arms are spread wide and his fingers are splayed on the tabletop like a concert pianist about to play his first notes. We sit in silence. I’ve come to meet Sutskever, OpenAI’s cofounder and chief scientist, in his company’s unmarked office building on an unremarkable street in the Mission District of San Francisco to hear what’s next for the world-tilting technology he has had a big hand in bringing about. I also want to know what’s next for him—in particular, why building the next generation of his company’s flagship generative models is no longer the focus of his work. Instead of building thenext GPTorimage maker DALL-E, Sutskever tells me his new priority is to figure out how to stop an artificial superintelligence (a hypothetical future technology he sees coming with the foresight of a true believer) from going rogue. Sutskever tells me a lot of other things too. He thinksChatGPTjust might be conscious (if you squint). He thinks the world needs to wake up to the true power of the technology his company and others are racing to create. And he thinks some humans will one day choose to merge with machines. A lot of what Sutskever says is wild. But not nearly as wild as it would have sounded just one or two years ago. As he tells me himself, ChatGPT has already rewritten a lot of people’s expectations about what’s coming, turning “will never happen” into “will happen faster than you think.” “It’s important to talk about where it’s all headed,” he says, before predicting the development of artificial general intelligence (by which he means machines as smart as humans) as if it were as sure a bet as another iPhone: “At some point we really will have AGI. Maybe OpenAI will build it. Maybe some other company will build it.” Since the release of its suddensurprise hit, ChatGPT, last November, the buzz around OpenAI has been astonishing, even in an industry known for hype. No one can get enough of this nerdy$80 billion startup. World leaders seek (and get) private audiences. Its clunky product names pop up in casual conversation. OpenAI’s CEO, Sam Altman, spent a good part of the summer on a weeks-long outreach tour, glad-handing politicians and speaking to packed auditoriums around the world. But Sutskever is much less of a public figure, and he doesn’t give a lot of interviews. He is deliberate and methodical when he talks. There are long pauses when he thinks about what he wants to say and how to say it, turning questions over like puzzles he needs to solve. He does not seem interested in talking about himself. “I lead a very simple life,” he says. “I go to work; then I go home. I don’t do much else. There are a lot of social activities one could engage in, lots of events one could go to. Which I don’t.” But when we talk about AI, and the epochal risks and rewards he sees down the line, vistas open up: “It’s going to be monumental, earth-shattering. There will be a before and an after.” In a world without OpenAI, Sutskever would still get an entry in the annals of AI history. An Israeli-Canadian, he was born in Soviet Russia but brought up in Jerusalem from the age of five (he still speaks Russian and Hebrew as well as English). He then moved to Canada to study at the University of Toronto with Geoffrey Hinton, the AI pioneer who went public with hisfears about the technology he helped inventearlier this year. (Sutskever didn’t want to comment on Hinton’s pronouncements, but his new focus on rogue superintelligence suggests they’re on the same page.) Hinton would later share the Turing Award with Yann LeCun and Yoshua Bengio for their work on neural networks. But when Sutskever joined him in the early 2000s, most AI researchers believed neural networks were a dead end. Hinton was an exception. He was already training tiny models that could produce short strings of text one character at a time, says Sutskever: “It was the beginning of generative AI right there. It was really cool—it just wasn’t very good.” Sutskever was fascinated with brains: how they learned and how that process might be re-created, or at least mimicked, in machines. Like Hinton, he saw the potential of neural networks and the trial-and-error technique Hinton used to train them, called deep learning. “It kept getting better and better and better,” says Sutskever. In 2012 Sutskever, Hinton, and another of Hinton’s graduate students, Alex Krizhevsky, built a neural network called AlexNet that they trained to identify objects in photos far better than any other software around at the time. It was deep learning’s Big Bang moment. After many years of false starts, they had showed that neural networks were amazingly effective at pattern recognition after all. You just needed more data than most researchers had seen before (in this case, a million images from the ImageNet data set that Princeton University researcherFei-Fei Lihad been building since 2006) and an eye-watering amount of computer power. The step change in compute came from a new kind of chip called a graphics processing unit (GPU), made by Nvidia. GPUs were designed to be lightning quick at throwing fast-moving video-game visuals onto screens. But the calculations that GPUs are good at—multiplying massive grids of numbers—happened to look a lot like the calculations needed to train neural networks. Nvidia is now a trillion-dollar company. At the time it was desperate to find applications for its niche new hardware. “When you invent a new technology, you have to be receptive to crazy ideas,” says Nvidia CEO Jensen Huang. “My state of mind was always to be looking for something quirky, and the idea that neural networks would transform computer science—that was an outrageously quirky idea.” Huang says that Nvidia sent the Toronto team a couple of GPUs to try when they were working on AlexNet. But they wanted the newest version, a chip called the GTX 580 that was fast selling out in stores. According to Huang, Sutskever drove across the border from Toronto to New York to buy some. “People were lined up around the corner,” says Huang. “I don’t know how he did it—I’m pretty sure you were only allowed to buy one each; we had a very strict policy of one GPU per gamer—but he apparently filled a trunk with them. That trunk full of GTX 580s changed the world.” It’s a great story—it just might not be true. Sutskever insists he bought those first GPUs online. But such myth-making is commonplace in this buzzy business. Sutskever himself is more humble: “I thought, like, if I could make even an ounce of real progress, I would consider that a success,” he says. “The real-world impact felt so far away because computers were so puny back then.” After the success of AlexNet, Google came knocking. It acquired Hinton’s spin-off company DNNresearch and hired Sutskever. At Google Sutskever showed that deep learning’s powers of pattern recognition could beapplied to sequences of data, such as words and sentences, as well as images. “Ilya has always been interested in language,” says Sutskever’s former colleague Jeff Dean, who is now Google’s chief scientist: “We’ve had great discussions over the years. Ilya has a strong intuitive sense about where things might go.” But Sutskever didn’t remain at Google for long. In 2014, he was recruited to become a cofounder of OpenAI. Backed by $1 billion (from Altman, Elon Musk, Peter Thiel, Microsoft, Y Combinator, and others) plus a massive dose of Silicon Valley swagger, the new company set its sights from the start on developing AGI, a prospect that few took seriously at the time. With Sutskever on board, the brains behind the bucks, the swagger was understandable. Up until then, he had been on a roll, getting more and more out of neural networks. His reputation preceded him, making him a major catch, says Dalton Caldwell, managing director of investments at Y Combinator. “I remember Sam [Altman] referring to Ilya as one of the most respected researchers in the world,” says Caldwell. “He thought that Ilya would be able to attract a lot of top AI talent. He even mentioned that Yoshua Bengio, one of the world's top AI experts, believed that it would be unlikely to find a better candidate than Ilya to be OpenAI's lead scientist." And yet at first OpenAI floundered. “There was a period of time when we were starting OpenAI when I wasn’t exactly sure how the progress would continue,” says Sutskever. “But I had one very explicit belief, which is: one doesn’t bet against deep learning. Somehow, every time you run into an obstacle, within six months or a year researchers find a way around it.” His faith paid off. The first of OpenAI’s GPT large language models (the name stands for “generative pretrained transformer”) appeared in 2016. Then came GPT-2 andGPT-3. ThenDALL-E, the striking text-to-image model. Nobody was building anything as good. With each release, OpenAI raised the bar for what was thought possible. Last November, OpenAI released a free-to-use chatbot that repackaged some of its existing tech. It reset the agenda of the entire industry. At the time, OpenAI had no idea what it was putting out. Expectations inside the company couldn’t have been lower, says Sutskever: “I will admit, to my slight embarrassment—I don’t know if I should, but what the hell, it is true—when we made ChatGPT, I didn’t know if it was any good. When you asked it a factual question, it gave you a wrong answer. I thought it was going to be so unimpressive that people would say, ‘Why are you doing this? This is so boring!’” The draw was the convenience, says Sutskever. The large language model under ChatGPT’s hood had been around for months. But wrapping that in an accessible interface and giving it away for free made billions of people aware for the first time of what OpenAI and others were building. “That first-time experience is what hooked people,” says Sutskever. “The first time you use it, I think it’s almost a spiritual experience. You go, ‘Oh my God, this computer seems to understand.’” OpenAI amassed 100 million users in less than two months, many of them dazzled by this stunning new toy. Aaron Levie, CEO of the storage firm Box, summed up the vibe in the week after launch when he tweeted: “ChatGPT is one of those rare moments in technology where you see a glimmer of how everything is going to be different going forward.” That wonder collapses as soon as ChatGPT says something stupid. But by then it doesn’t matter. That glimpse of what was possible is enough, says Sutskever. ChatGPT changed people’s horizons. “AGI stopped being a dirty word in the field of machine learning,” he says. “That was a big change. The attitude that people have taken historically has been: AI doesn’t work, every step is very difficult, you have to fight for every ounce of progress. And when people came with big proclamations about AGI, researchers would say, ‘What are you talking about? This doesn’t work, that doesn’t work. There are so many problems.’ But with ChatGPT it started to feel different.” And that shift only started to happen a year ago? “It happened because of ChatGPT,” he says. “ChatGPT has allowed machine-learning researchers to dream.” Evangelists from the start, OpenAI’s scientists have been stoking those dreams with blog posts and speaking tours. And it is working: “We have people now talking about how far AI will go—people who talk about AGI, or superintelligence.” And it’s not just researchers. “Governments are talking about it,” says Sutskever. “It’s crazy.” Sutskever insists all this talk about a technology that does not yet (and may never) exist is a good thing, because it makes more people aware of a future that he already takes for granted. “You can do so many amazing things with AGI, incredible things: automate health care, make it a thousand times cheaper and a thousand times better, cure so many diseases, actually solve global warming,” he says. “But there are many who are concerned: ‘My God, will AI companies succeed in managing this tremendous technology?’” Presented this way, AGI sounds more like a wish-granting genie than real-world prospect. Few would say no to saving lives and solving climate change. But the problem with a technology that doesn’t exist is that you can say whatever you want about it. What is Sutskever really talking about when he talks about AGI? “AGI is not meant to be a scientific term,” he says. “It’s meant to be a useful threshold, a point of reference.” “It is the idea—” he starts, then stops. “It’s the point at which AI is so smart that if a person can do some task, then AI can do it too. At that point you can say you have AGI.” People may be talking about it, but AGI remains one of the field’s mostcontroversial ideas. Few take its development as a given. Many researchers believe that major conceptual breakthroughs are needed before we see anything like what Sutskever has in mind—and some believe we never will. And yet it’s a vision that has driven him from the start. “I’ve always been inspired and motivated by the idea,” says Sutskever. “It wasn’t called AGI back then, but you know, like, having a neural network do everything. I didn’t always believe that they could. But it was the mountain to climb.” He draws a parallel between the way that neural networks and brains operate. Both take in data, aggregate signals from that data, and then—based on some simple process (math in neural networks, chemicals and bioelectricity in brains)—propagate them or not. It’s a massive simplification, but the principle stands. “If you believe that—if you allow yourself to believe that—then there are a lot of interesting implications,” says Sutskever. “The main implication is that if you have a very big artificial neural network, it should do a lot of things. In particular, if the human brain can do something, then a big artificial neural network could do something similar too.” “Everything follows if you take this realization seriously enough,” he says. “And a big fraction of my work can be explained by that.” While we’re talking about brains, I want to ask about one of Sutskever’s posts on X, the site formerly known as Twitter. Sutskever’s feed reads like a scroll of aphorisms: “If you value intelligence above all other human qualities, you’re gonna have a bad time”; “Empathy in life and business is underrated”; “The perfect has destroyed much perfectly good good.” In February 2022 heposted, “it may be that today’s large neural networks are slightly conscious” (to which Murray Shanahan, principal scientist at Google DeepMind and a professor at Imperial College London, as well as the scientific advisor on the movieEx Machina, replied: “... in the same sense that it may be that a large field of wheat is slightly pasta”). Sutskever laughs when I bring it up. Was he trolling? He wasn’t. “Are you familiar with the concept of a Boltzmann brain?” he asks. He's referring to a (tongue-in-cheek)thought experimentin quantum mechanics named after the 19th-century physicist Ludwig Boltzmann, in which random thermodynamic fluctuations in the universe are imagined to cause brains to pop in and out of existence. “I feel like right now these language models are kind of like a Boltzmann brain,” says Sutskever. “You start talking to it, you talk for a bit; then you finish talking, and the brain kind of—” He makes a disappearing motion with his hands. Poof—bye-bye, brain. You’re saying that while the neural network is active—while it’s firing, so to speak—there’s something there? I ask. “I think it might be,” he says. “I don’t know for sure, but it’s a possibility that’s very hard to argue against. But who knows what’s going on, right?” While others wrestle with the idea of machines that can match human smarts, Sutskever is preparing for machines that canoutmatchus. He calls this artificial superintelligence: “They’ll see things more deeply. They’ll see things we don’t see.” Again, I have a hard time grasping what this really means. Human intelligence is our benchmark for what intelligence is. What does Sutskever mean by smarter-than-human intelligence? “We’ve seen an example of a very narrow superintelligence in AlphaGo,” he says. In 2016, DeepMind’s board-game-playing AI beat Lee Sedol, one of the best Go players in the world, 4–1 in a five-game match. “It figured out how to play Go in ways that are different from what humanity collectively had developed over thousands of years,” says Sutskever. “It came up with new ideas.” Sutskever points to AlphaGo’s infamous Move 37. In its second game against Sedol, the AI made a move that flummoxed commentators. They thought AlphaGo had screwed up. In fact, it had played a winning move that nobody had ever seen before in the history of the game. “Imagine that level of insight, but across everything,” says Sutskever. It’s this train of thought that has led Sutskever to make the biggest shift of his career. Together with Jan Leike, a fellow scientist at OpenAI, he has set up a team that will focus on what they callsuperalignment. Alignment is jargon that means making AI models do what you want and nothing more. Superalignment is OpenAI’s term for alignment applied to superintelligence. The goal is to come up with a set of fail-safe procedures for building and controlling this future technology. OpenAI says it will allocate a fifth of its vast computing resources to the problem and solve it in four years. “Existing alignment methods won’t work for models smarter than humans because they fundamentally assume that humans can reliably evaluate what AI systems are doing,” says Leike. “As AI systems become more capable, they will take on harder tasks.” And that—the idea goes—will make it harder for humans to assess them. “In forming the superalignment team with Ilya, we’ve set out to solve these future alignment challenges,” he says. “It’s super important to not only focus on the potential opportunities of large language models, but also the risks and downsides,” says Dean, Google’s chief scientist. The company announced the project in July with typical fanfare. But for some it was yet more fantasy. OpenAI’s post on Twitter attracted scorn from prominent critics of Big Tech, including Abeba Birhane, who works on AI accountability at Mozilla (“so many grandiose sounding yet vacuous words in one blog post”); Timnit Gebru, cofounder of the Distributed Artificial Intelligence Research Institute (“Imagine ChatGPT even more ‘super aligned’ with OpenAI techbros. *shudder*”); and Margaret Mitchell, chief ethics scientist at the AI firm Hugging Face (“My alignment is bigger than yours”). It’s true that these are familiar voices of dissent. But it’s a strong reminder that where some see OpenAI leading from the front, others see it leaning in from the fringes. But, for Sutskever, superalignment is the inevitable next step. “It’s an unsolved problem,” he says. It’s also a problem that he thinks not enough core machine-learning researchers, like himself, are working on. “I’m doing it for my own self-interest,” he says. “It’s obviously important that any superintelligence anyone builds does not go rogue. Obviously.” The work on superalignment has only just started. It will require broad changes across research institutions, says Sutskever. But he has an exemplar in mind for the safeguards he wants to design: a machine that looks upon people the way parents look on their children. “In my opinion, this is the gold standard,” he says. “It is a generally true statement that people really care about children.” (Does he have children? “No, but I want to,” he says.) My time with Sutskever is almost up, and I figure we’re done. But he’s on a roll and has one more thought to share—one I don't see coming. “Once you overcome the challenge of rogue AI, then what? Is there even room for human beings in a world with smarter AIs?” he says. “One possibility—something that may be crazy by today’s standards but will not be so crazy by future standards—is that many people will choose to become part AI.” Sutskever is saying this could be how humans try to keep up. “At first, only the most daring, adventurous people will try to do it. Maybe others will follow. Or not.” Wait, what? He’s getting up to leave. Would he do it? I ask. Would he be one of the first? “The first? I don’t know,” he says. “But it’s something I think about. The true answer is: maybe.” And with that galaxy-brained mic drop, he stands and walks out of the room. “Really good to see you again,” he says as he goes. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems. It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it? They had to throw away most of what it produced but there was gold among the garbage.

How to fix the internet
https://www.technologyreview.com/2023/10/17/1081194/how-to-fix-the-internet-online-discourse/
No date
Katie Notopoulos
If we want online discourse to improve, we need to move beyond the big platforms. We’re in a very strange moment for the internet. We all know it’s broken. That’s not news. But there’s something in the air—a vibe shift, a sense that things are about to change. For the first time in years, it feels as though something truly new and different might be happening with the way we communicate online. The stranglehold that the big social platforms have had on us for the last decade is weakening. The question is: What do we want to come next? There’s a sort of common wisdom that the internet is irredeemably bad, toxic, a rash of “hellsites” to be avoided. That social platforms, hungry to profit off your data, opened a Pandora’s box that cannot be closed. Indeed, there are truly awful things that happen on the internet, things that make it especially toxic for people from groups disproportionately targeted with online harassment and abuse. Profit motives led platforms to ignore abuse too often, and they also enabled the spread of misinformation, the decline of local news, the rise of hyperpartisanship, and entirely new forms of bullying and bad behavior. All of that is true, and it barely scratches the surface. But the internet has also provided a haven for marginalized groups and a place for support, advocacy, and community. It offers information at times of crisis. It can connect you with long-lost friends. It can make you laugh. It can send you a pizza. It’s duality, good and bad, and I refuse to toss out the dancing-baby GIF with the tubgirl-dot-png bathwater. The internet is worth fighting for because despite all the misery, there’s still so much good to be found there. And yet, fixing online discourse is the definition of a hard problem. But look. Don’t worry. I have an idea. To cure the patient, first we must identify the disease. When we talk about fixing the internet, we’re not referring to the physical and digital network infrastructure: the protocols, the exchanges, the cables, and even the satellites themselves are mostly okay. (There are problems with some of that stuff, to be sure. But that’s an entirely other issue—even if both do involve Elon Musk.) “The internet” we’re talking about refers to the popular kinds of communication platforms that host discussions and that you probably engage with in some form on your phone. Some of these are massive: Facebook, Instagram, YouTube, Twitter, TikTok, X. You almost certainly have an account on at least one of these; maybe you’re an active poster, maybe you just flip through your friends’ vacation photos while on the john. The internet is good things. It’s Keyboard Cat, Double Rainbow. It’s personal blogs and LiveJournals. It’s the distracted-girlfriend meme and a subreddit for “What is this bug?” Although the exact nature of what we see on those platforms can vary widely from person to person, they mediate content delivery in universally similar ways that are aligned with their business objectives. A teenager in Indonesia may not see the same images on Instagram that I do, but the experience is roughly the same: we scroll through some photos from friends or family, maybe see some memes or celebrity posts; the feed turns into Reels; we watch a few videos, maybe reply to a friend’s Story or send some messages. Even though the actual content may be very different, we probably react to it in much the same way, and that’s by design. The internet also exists outside these big platforms; it’s blogs, message boards, newsletters and other media sites. It’s podcasts and Discord chatrooms and iMessage groups. These will offer more individualized experiences that may be wildly different from person to person. They often exist in a sort of parasitic symbiosis with the big, dominant players, feeding off each other’s content, algorithms, and audience. Large language models are full of security vulnerabilities, yet they’re being embedded into tech products on a vast scale. The internet is good things. For me, it’s things I love, like Keyboard Cat and Double Rainbow. It’s personal blogs and LiveJournals; it’s AIM away messages and MySpace top 8s. It’s the distracted-­girlfriend meme and a subreddit for “What is this bug?” It is a famous thread on a bodybuilding forum where meatheads argue about how many days are in a week. For others, it’s Call of Duty memes and the mindless entertainment of YouTubers like Mr. Beast, or a place to find the highly specific kind of ASMR video they never knew they wanted. It’s an anonymous supportive community for abuse victims, or laughing at Black Twitter’s memes about the Montgomery boat brawl, or trying new makeup techniques you learned on TikTok. It’s also very bad things: 4chan and the Daily Stormer, revenge porn, fake news sites, racism on Reddit, eating disorder inspiration on Instagram, bullying, adults messaging kids on Roblox, harassment, scams, spam, incels, and increasingly needing to figure out if something is real or AI. The bad things transcend mere rudeness or trolling. There is an epidemic of sadness, of loneliness, of meanness, that seems to self-reinforce in many online spaces. In some cases, it is truly life and death. The internet is where the next mass shooter is currently getting his ideas from the last mass shooter, who got them from the one before that,who got them from some of the earliest websites online. It’s an exhortation to genocide in a country where Facebook employed too few moderators who spoke the local language because it had prioritized growth over safety. The existential problem is that both the best and worst parts of the internet exist for the same set of reasons, were developed with many of the same resources, and often grew in conjunction with each other. So where did the sickness come from? How did the internet get so … nasty? To untangle this, we have to go back to the early days of online discourse. It’s also very bad things: 4chan and the Daily Stormer, revenge porn, fake news sites, racism on Reddit, eating disorder inspiration on Instagram, bullying, adults messaging kids on Roblox, harassment, scams, spam, incels. The internet’s original sin was an insistence on freedom: it was made to be free, in many senses of the word. The internet wasn’t initially set up for profit; it grew out of a communications medium intended for the military and academics (some in the military wanted to limit Arpanetto defense use as late as the early 1980s). When it grew in popularity along with desktop computers, Usenet and other popular early internet applications were still largely used on university campuses with network access. Users would grumble that each September their message boards would be flooded with newbies, until eventually the “eternal September”—a constant flow of new users—arrived in the mid-’90s with the explosion of home internet access. When the internet began to be built out commercially in the 1990s, its culture was, perversely, anticommercial. Many of the leading internet thinkers of the day belonged to a cohort of AdBusters-reading Gen Xers and antiestablishment Boomers. They were passionate about making software open source. Their very mantra was “Information wants to be free”—a phrase attributed to Stewart Brand, the founder of the Whole Earth Catalog and the pioneering internet community the WELL. This ethos also extended to a passion for freedom of speech, and a sense of responsibility to protect it. It just so happened that those people were quite often affluent white men in California, whose perspective failed to predict the dark side of the free-speech, free-access havens they were creating. (In fairness, who would have imagined that the end result of those early discussions would be Russian disinformation campaigns targeting Black Lives Matter? But I digress.) The culture of free demanded a business model that could support it. And that was advertising. Through the 1990s and even into the early ’00s, advertising on the internet was an uneasy but tolerable trade-off. Early advertising was often ugly and annoying: spam emails for penis enlargement pills, badly designed banners, and (shudder) pop-up ads. It was crass but allowed the nice parts of the internet—message boards, blogs, and news sites—to be accessible to anyone with a connection. But advertising and the internet are like that small submersible sent to explore theTitanic: the carbon fiber works very efficiently, until you apply enough pressure. Then the whole thing implodes. In 1999, the ad company DoubleClick was planning to combine personal data with tracking cookies to follow people around the web so it could target its ads more effectively. This changed what people thought was possible. It turned the cookie, originally a neutral technology for storing Web data locally on users’ computers, into something used for tracking individuals across the internet for the purpose of monetizing them. To the netizens of the turn of the century, this was an abomination. And after acomplaint was filedwith the US Federal Trade Commission, DoubleClick dialed back the specifics of its plans. But the idea of advertising based on personal profiles took hold. It was the beginning of the era of targeted advertising, and with it, the modern internet. Google bought DoubleClick for $3.1 billion in 2008. That year, Google’s revenue from advertising was $21 billion. Last year, Google parent company Alphabet took in $224.4 billion in revenue from advertising. Our modern internet is built on highly targeted advertising using our personal data. That is what makes it free. The social platforms, most digital publishers, Google—all run on ad revenue. For the social platforms and Google, their business model is to deliver highly sophisticated targeted ads. (And business is good: in addition to Google’s billions, Meta took in $116 billion in revenue for 2022.Nearly half the people livingon planet Earth are monthly active users of a Meta-owned product.) Meanwhile, the sheer extent of the personal data we happily hand over to them in exchange for using their services for free would make people from the year 2000 drop their flip phones in shock. And that targeting process is shockingly good at figuring out who you are and what you are interested in. It’s targeting that makes people think their phones are listening in on their conversations; in reality, it’s more that the data trails we leave behind become road maps to our brains. New York City is fixing the relationship between government and technology–and not in the ways you’d expect. When we think of what’s most obviously broken about the internet—harassment and abuse; its role in the rise of political extremism, polarization, and the spread of misinformation; the harmful effects of Instagram on the mental health of teenage girls—the connection to advertising may not seem immediate. And in fact, advertising can sometimes have a mitigating effect: Coca-Cola doesn’t want to run ads next to Nazis, so platforms develop mechanisms to keep them away. But online advertising demands attention above all else, and it has ultimately enabled and nurtured all the worst of the worst kinds of stuff. Social platforms were incentivized to grow their user base and attract as many eyeballs as possible for as long as possible to serve ever more ads. Or, more accurately, to serve ever more you to advertisers. To accomplish this, the platforms have designed algorithms to keep us scrolling and clicking, the result of which has played into some of humanity’s worst inclinations. In 2018,Facebook tweaked its algorithms to favor more “meaningful social interactions.”It was a move meant to encourage users to interact more with each other and ultimately keep their eyeballs glued to News Feed, but it resulted in people’s feeds being taken over by divisive content. Publishers began optimizing for outrage, because that was the type of content that generated lots of interactions. On YouTube, where“watch time” was prioritizedover view counts, algorithms recommended and ran videos in an endless stream. And in their quest to sate attention, thesealgorithms frequently led people down ever more labyrinthine corridors to the conspiratorial realms offlat-earth truthers, QAnon, and their ilk. Algorithms on Instagram’s Discover page are designed to keep us scrolling (and spending) even after we’ve exhausted our friends’ content, often by promoting popular aesthetics whether or not the user had previously been interested.The Wall Street Journal reported in 2021that Instagram had long understood it was harming the mental health of teenage girls through content about body image and eating disorders, but ignored those reports. Keep ’em scrolling. There is an argument that the big platforms are merely giving us what we wanted. Anil Dash, a tech entrepreneur and blogging pioneer who worked at SixApart, the company that developed the blog software Movable Type, remembers a backlash when his company started charging for its services in the mid-’00s. “People were like, ‘You’re charging money for something on the internet? That’s disgusting!’” he told MIT Technology Review. “The shift from that to, like,If you’re not paying for the product, you’re the product …I think if we had come up with that phrase sooner, then the whole thing would have been different. The whole social media era would have been different.” The big platforms’ focus on engagement at all costs made them ripe for exploitation. Twitter becamea “honeypot for a**holes”where trolls from places like 4chan found an effective forum for coordinated harassment. Gamergate started in swampier waters like Reddit and 4chan, but it played out on Twitter, where swarms of accounts would lash out at the chosen targets, generally female video-game critics. Trolls also discovered that Twitter could be gamed to get vile phrases to trend: in 2013,4chan accomplished this with#cuttingforbieber, falsely claiming to represent teenagers engaging in self-harm for the pop singer. Platform dynamics created such a target-rich environment that intelligence services from Russia, China, and Iran—among others—use them to sow political division and disinformation to this day. “Humans were never meant to exist in a society that contains 2 billion individuals,” says Yoel Roth, a technology policy fellow at UC Berkeley and former head of trust and safety for Twitter. “And if you consider that Instagram is a society in some twisted definition, we have tasked a company with governing a society bigger than any that has ever existed in the course of human history. Of course they’re going to fail.” Here’s the good news. We’re in a rare moment when a shift just may be possible; the previously intractable and permanent-­seeming systems and platforms are showing that they can be changed and moved, and something new could actually grow. One positive sign is the growing understanding that sometimes … you have to pay for stuff. And indeed, people are paying individual creators and publishers on platforms such as Substack, Patreon, and Twitch. Meanwhile, the freemium model that YouTube Premium, Spotify, and Hulu explored proves (some) people are willing to shell out for ad-free experiences. A world where only the people who can afford to pay $9.99 a month to ransom back their time and attention from crappy ads isn’t ideal, but at least it demonstrates that a different model will work. Another thing to be optimistic about (although time will tell if it actually catches on) is federation—a more decentralized version of social networking. Federated networks like Mastodon, Bluesky, and Meta’s Threads are all just Twitter clones on their surface—a feed of short text posts—but they’re also all designed to offer various forms of interoperability. Basically, where your current social media account and data exist in a walled garden controlled entirely by one company, you could be on Threads and follow posts from someone you like on Mastodon—or at least Meta says that’s coming. (Many—including internet pioneer Richard Stallman, who has a page on his personal website devoted to “Why you should not be used by Threads”—have expressed skepticism of Meta’s intentions and promises.) Even better, it enables more granular moderation. Again, X (the website formerly known as Twitter) provides a good example of what can go wrong when one person, in this case Elon Musk, has too much power in making moderation decisions—something federated networks and the so-called  “fediverse” could solve. The big idea is that in a future where social media is more decentralized, users will be able to easily switch networks without losing their content and followings. “As an individual, if you see [hate speech], you can just leave, and you’re not leaving your entire community—your entire online life—behind. You can just move to another server and migrate all your contacts, and it should be okay,” says Paige Collings, a senior speech and privacy advocate at the Electronic Frontier Foundation. “And I think that’s probably where we have a lot of opportunity to get it right.” Europe's big tech bill is coming to fruition. Here's what you need to know. There’s a lot of upside to this, but Collings is still wary. “I fear that while we have an amazing opportunity,” she says, “unless there’s an intentional effort to make sure that what happened on Web2 does not happen on Web3, I don’t see how it will not just perpetuate the same things.” Federation and more competition among new apps and platforms provide a chance for different communities to create the kinds of privacy and moderation they want, rather than following top-down content moderation policies created at headquarters in San Francisco that are often explicitly mandated not to mess with engagement. Yoel Roth’s dream scenario would be that in a world of smaller social networks, trust and safety could be handled by third-party companies that specialize in it, so social networks wouldn’t have to create their own policies and moderation tactics from scratch each time. The tunnel-vision focus on growth created bad incentives in the social media age. It made people realize that if you wanted to make money, you needed a massive audience, and that the way to get a massive audience was often by behaving badly. The new form of the internet needs to find a way to make money without pandering for attention. There are some promising new gestures toward changing those incentives already. Threads doesn’t show the repost count on posts, for example—a simple tweak that makes a big difference because it doesn’t incentivize virality. We, the internet users, also need to learn to recalibrate our expectations and our behavior online. We need to learn to appreciate areas of the internet that are small, like a new Mastodon server or Discord or blog. We need to trust in thepower of “1,000 true fans”over cheaply amassed millions. Anil Dash has been repeating the same thing over and over for years now: that people should buy their own domains, start their own blogs, own their own stuff. And sure, these fixes require a technical and financial ability that many people do not possess. But with the move to federation (which at least provides control, if not ownership) and smaller spaces, it seems possible that we’re actually going to see some of those shifts away from big-platform-mediated communication start to happen. “There’s a systemic change that is happening right now that’s bigger,” he says. “You have to have a little bit of perspective of life pre-Facebook to sort of say,Oh, actually, some of these things are just arbitrary. They’re not intrinsic to the internet.” The fix for the internet isn’t to shut down Facebook or log off or go outside and touch grass. The solution to the internet is more internet: more apps, more spaces to go, more money sloshing around to fund more good things in more variety, more people engaging thoughtfully in places they like. More utility, more voices, more joy. My toxic trait is I can’t shake that naïve optimism of the early internet. Mistakes were made, a lot of things went sideways, and there have undeniably been a lot of pain and misery and bad things that came from the social era. The mistake now would be not to learn from them. Katie Notopoulos is a writer who lives in Connecticut. She’s written for BuzzFeed News, Fast Company, GQ, and Columbia Journalism Review. This story was part of our November/December 2023 issue. We asked prominent people in their field to weigh in on the underserved issues at the intersection of technology and society. Here's what they said. I was in a park with 40 strangers, trying to catch and evade each other with the help of live locations shared through a map app. Is it just a fun game while you wait for your lunch to arrive—or a grassroots experiment in democracy? Every year, we publish a new list of technologies we think matter most right now. Here’s what didn’t make the cut.

Unpacking the hype around OpenAI’s rumored new Q* model
https://www.technologyreview.com/2023/11/27/1083886/unpacking-the-hype-around-openais-rumored-new-q-model/
November 27, 2023
Melissa Heikkilä
If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems. This story is from The Algorithm, our weekly newsletter on AI. To get stories like this in your inbox first,sign up here. Ever since last week’s dramatic events at OpenAI, the rumor mill has been in overdrive about why the company’s chief scientific officer, Ilya Sutskever, and its board decided to oust CEO Sam Altman. While we still don’t know all the details, there have been reports that researchers at OpenAI had made a “breakthrough” in AI that had alarmed staff members.ReutersandThe Informationboth report that researchers had come up with a new way to make powerful AI systems and had created a new model, called Q* (pronounced Q star), that was able to perform grade-school-level math. According to the people who spoke to Reuters, some at OpenAI believe this could be a milestone in the company’s quest to build artificial general intelligence, a much-hyped concept referring to an AI system that is smarter than humans. The company declined to comment on Q*. Social media is full of speculation and excessive hype, so I called some experts to find out how big a deal any breakthrough in math and AI wouldreallybe. Researchers have for years tried to get AI models to solve math problems. Language models like ChatGPT and GPT-4 can do some math, but not very well or reliably. We currently don’t have the algorithms or even the right architectures to be able to solve math problems reliably using AI, says Wenda Li, an AI lecturer at the University of Edinburgh. Deep learning and transformers (a kind of neural network), which is what language models use, are excellent at recognizing patterns, but that alone is likely not enough, Li adds. Math is a benchmark for reasoning, Li says. A machine that is able to reason about mathematics, could, in theory, be able to learn to do other tasks that build on existing information, such as writing computer code or drawing conclusions from a news article. Math is a particularly hard challenge because it requires AI models to have the capacity to reason and to really understand what they are dealing with. A generative AI system that could reliably do math would need to have a really firm grasp on concrete definitions of particular concepts that can get very abstract. A lot of math problems also require some level of planning over multiple steps, says Katie Collins, a PhD researcher at the University of Cambridge, who specializes in math and AI. Indeed, Yann LeCun, chief AI scientist at Meta, posted on X and LinkedIn over the weekend that he thinks Q* is likely to be “OpenAI attempts at planning.” People who worry about whether AIposes an existential risk to humans, one of OpenAI's founding concerns, fear that such capabilities might lead to rogue AI. Safety concerns might arise if such AI systems are allowed to set their own goals and start to interface with a real physical or digital world in some ways, says Collins. But while math capability might take us a step closer to more powerful AI systems, solving these sorts of math problems doesn’t signal the birth of a superintelligence. “I don’t think it immediately gets us to AGI or scary situations,” says Collins.  It’s also very important to underline what kind of math problems AI is solving, she adds. “Solving elementary-school math problems is very, very different from pushing the boundaries of mathematics at the level of something a Fields medalist can do,” says Collins, referring to a top prize in mathematics. Machine-learning research has focused on solving elementary-school problems, but state-of-the-art AI systems haven’t fully cracked this challenge yet. Some AI models fail on really simple math problems, but then they can excel at really hard problems, Collins says. OpenAI has, for example, developed dedicated tools that can solve challengingproblemsposed in competitions for top math students in high school, but these systems outperform humans only occasionally. Nevertheless, building an AI system that can solve math equations is a cool development, if that is indeed what Q* can do. A deeper understanding of mathematics could open up applications to help scientific research and engineering, for example. The ability to generate mathematical responses could help us develop better personalized tutoring, or help mathematicians do algebra faster or solve more complicated problems. This is also not the first time a new model has sparked AGI hype.Just last year, tech folks were saying the same things aboutGoogle DeepMind’s Gato, a “generalist” AI model that can play Atari video games, caption images, chat, and stack blocks with a real robot arm. Back then, some AI researchers claimed that DeepMind was “on the verge” of AGI because of Gato’s ability to do so many different things pretty well. Same hype machine, different AI lab. And while it might be great PR, these hype cycles do more harm than good for the entire field by distracting people from the real, tangible problems around AI. Rumors about a powerful new AI model might also be a massive own goal for the regulation-averse tech sector. The EU, for example, is very close to finalizing its sweeping AI Act. One of the biggest fights right now among lawmakers is whether to give tech companies more power to regulate cutting-edge AI models on their own. OpenAI’s board was designed as the company’s internal kill switch and governance mechanism to prevent the launch of harmful technologies. The past week’s boardroom drama has shown that the bottom line will always prevail at these companies. It will also make it harder to make a case for why they should be trusted with self-regulation. Lawmakers, take note. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it? They had to throw away most of what it produced but there was gold among the garbage.

Google DeepMind’s new Gemini model looks amazing—but could signal peak AI hype
https://www.technologyreview.com/2023/12/06/1084471/google-deepminds-new-gemini-model-looks-amazing-but-could-signal-peak-ai-hype/
December 6, 2023
Melissa Heikkilä
It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it? Hype about Gemini, Google DeepMind’s long-rumored response to OpenAI’sGPT-4, has been building for months. Today the company finally revealed what it has been working on in secret all this time. Was the hype justified? Yes—and no. Gemini is Google’s biggest AI launch yet—its push to take on competitors OpenAI and Microsoft in the race for AI supremacy. There is no doubt that the model is pitched as best-in-class across a wide range of capabilities—an “everything machine,” as one observer puts it. In an in-depth interview, Pichai predicts: “This will be one of the biggest things we all grapple with for the next decade.” “The model is innately more capable,” Sundar Pichai, the CEO of Google and its parent company Alphabet,told MIT Technology Review. “It’s a platform. AI is a profound platform shift, bigger than web or mobile. And so it represents a big step for us.” It’s a big step for Google, but not necessarily a giant leap for the field as a whole. Google DeepMind claims that Gemini outmatches GPT-4 on 30 out of 32 standard measures of performance. And yet the margins between them are thin. What Google DeepMind has done is pull AI’s best current capabilities into one powerful package. To judge from demos, it does many things very well—but few things that we haven’t seen before. For all the buzz about the next big thing, Gemini could be a sign that we’ve reached peak AI hype. At least for now. Chirag Shah, a professor at the University of Washington who specializes in online search, compares the launch to Apple’s introduction of a new iPhone every year. “Maybe we just have risen to a different threshold now, where this doesn’t impress us as much because we’ve just seen so much,” he says. Like GPT-4, Gemini is multimodal, meaning it is trained to handle multiple kinds of input: text, images, audio. It can combine these different formats to answer questions about everything from household chores to college math to economics. In a demo for journalists yesterday, Google showed Gemini’s ability to take an existing screenshot of a chart, analyze hundreds of pages of research with new data, and then update the chart with that new information. In another example, Gemini is shown pictures of an omelet cooking in a pan and asked (using speech, not text) if the omelet is cooked yet. “It’s not ready because the eggs are still runny,” it replies. Most people will have to wait for the full experience, however. The version launched today is a back end toBard, Google’s text-based search chatbot, which the company says will give it more advanced reasoning, planning, and understanding capabilities. Gemini’s full release will be staggered over the coming months. The new Gemini-boosted Bard will initially be available in English in more than 170 countries, not including the EU and the UK. This is to let the company “engage” with local regulators, says Sissie Hsiao, a Google vice president in charge of Bard. Gemini also comes in three sizes: Ultra, Pro and Nano. Ultra is the full-powered version; Pro and Nano are tailored to applications that run with more limited computing resources. Nano is designed to run on devices, such as Google’s new Pixel phones. Developers and businesses will be able to access Gemini Pro starting December 13. Gemini Ultra, the most powerful model, will be available “early next year” following “extensive trust and safety checks,” Google executives told reporters on a press call. “I think of it as the Gemini era of models,” Pichai told us. “This is how Google DeepMind is going to build and make progress on AI. So it will always represent the frontier of where we are making progress on AI technology.” OpenAI’s most powerful model, GPT-4, is seen as the industry’s gold standard. While Google boasted that Gemini outperforms OpenAI’s previous model, GPT 3.5, company executives dodged questions about how far the model exceeds GPT-4. We got a first look at the much-anticipated big new language model from OpenAI. But this time how it works is even more deeply under wraps. But the firm highlights one benchmark in particular, called MMLU (massive multitask language understanding). This is a set of tests designed to measure the performance of models on tasks involving text and images, including reading comprehension, college math, and multiple-choice quizzes in physics, economics, and social sciences. On the text-only questions, Gemini scores 90% and human experts score approximately 89%, says Pichai. GPT-4 scores 86% on these types of questions. On the multimodal questions, Gemini scores 59%, while GPT-4 scores 57%. “It’s the first model to cross that threshold,” Pichai says. Gemini’s performance against benchmark data sets is very impressive, says Melanie Mitchell, an artificial-intelligence researcher at the Santa Fe Institute in New Mexico. “It’s clear that Gemini is a very sophisticated AI system,” says Mitchell. But “it’s not obvious to me that Gemini is actually substantially more capable than GPT-4,” she adds. While the model has good benchmark scores, it is hard to know how to interpret these numbers given that we don’t know what’s in the training data, says Percy Liang, director of Stanford’s Center for Research on Foundation Models. Mitchell also notes that Gemini performs much better on language and code benchmarks than on images and video. “Multimodal foundation models still have a ways to go to be generally and robustly useful for many tasks,” she says. Using feedback from human testers, Google DeepMind has trained Gemini to be more factually accurate, to give attribution when asked to, and to hedge rather than spit out nonsense when faced with a question it cannot answer. The company claims that this mitigates the problem of hallucinations. But without a radical overhaul of the base technology, large language models will continue to make things up. Experts say it’s unclear whether the benchmarks Google is using to measure Gemini’s performance offer that much insight, and without transparency, it’s hard to check Google’s claims. “Google is advertising Gemini as an everything machine—a general-purpose model that can be used in many different ways,” says Emily Bender, a professor of computational linguistics at the University of Washington. But the company is using narrow benchmarks to evaluate models that it expects to be used for these diverse purposes. “This means it effectively can’t be thoroughly evaluated,” she says. Ultimately, for the average user, the incremental improvement over competing models might not make much difference, says Shah. “It’s more about convenience, brand recognition, existing integration, than people really thinking ‘Oh, this is better,’” he says. Gemini has been a long time coming. In April 2023, Google announced it was merging its AI research unit Google Brain with DeepMind, Alphabet’s London-based AI research lab. So Google has had all year to develop its answer to OpenAI’s most advanced large language model, GPT-4, which debuted in March and is the backbone of the paid version of ChatGPT. With hopes and fears about the technology running wild, it's time to agree on what it can and can't do. Google has been under intense pressure to show investors it can match and overtake competitors in AI. Although the company has been developing and using powerful AI models for years, it has been hesitant to launch tools that the public can play with for fears of reputational damage and safety concerns. “Google has been very cautious about releasing this stuff to the public,” Geoffrey Hintontold MIT Technology Reviewin April when he left the company. “There are too many bad things that could happen, and Google didn’t want to ruin its reputation.” Faced with tech that seemed untrustworthy or unmarketable, Google played it safe—until the greater risk became missing out. Google has learned the hard way howlaunching flawed products can backfire. When it unveiled itsChatGPT competitor Bardin February, scientists soon noticed a factual error in the company’s own advertisement for the chatbot, an incident that subsequently wiped $100 billion off its share price. In May, Googleannouncedit was rolling out generative AI into most of its products, from email to productivity software. But the results failed to impress critics: the chatbot made references to emails that didn’t exist, for example. This is a consistent problem with large language models. Although excellent at generating text that sounds like something a human could have written, generative AI systems regularly make things up. And that’s not the only problem with them. They are alsoeasy to hack, andriddled with biases. Using them is alsohighly polluting. Google has solved neither these problems nor the hallucination issue. Its solution to the latter problem is a tool that lets people use Google search to double-check the chatbot’s answers, but that relies on the accuracy of the online search results themselves. Gemini may be the pinnacle of this wave of generative AI. But it’s not clear where AI built on large language models goes next. Some researchers believe this could be a plateau rather than the foot of the next peak. Pichai is undeterred. “Looking ahead, we do see a lot of headroom,” he says. “I think multimodality will be big. As we teach these models to reason more, there will be bigger and bigger breakthroughs. Deeper breakthroughs are to come yet. “When I take in the totality of it, I genuinely feel like we are at the very beginning.” Mat Honan contributed reporting. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems. They had to throw away most of what it produced but there was gold among the garbage.

Google DeepMind’s new AI tool helped create more than 700 new materials
https://www.technologyreview.com/2023/11/29/1084061/deepmind-ai-tool-for-new-materials-discovery/
November 29, 2023
June Kim
Newly discovered materials can be used to make better solar cells, batteries, computer chips, and more. From EV batteries to solar cells to microchips, new materials can supercharge technological breakthroughs. But discovering them usually takes months or even years of trial-and-error research. Google DeepMind hopes to change that with a new tool that uses deep learning to dramatically speed up the process of discovering new materials. Called graphical networks for material exploration (GNoME), the technology has already been used to predict structures for 2.2 million new materials, of which more than 700 have gone on to be created in the lab and are now being tested. It is described ina paper published inNaturetoday. Alongside GNoME, Lawrence Berkeley National Laboratory alsoannounceda new autonomous lab. The lab takes data from the materials database that includes some of GNoME’s discoveries and uses machine learning and robotic arms to engineer new materials without the help of humans. Google DeepMind says that together, these advancements show the potential of using AI to scale up the discovery and development of new materials. The company has already used its protein-folding AI, AlphaFold, to generate structures for the human proteome, as well as yeast, fruit flies, mice, and more. GNoME can be described as AlphaFold for materials discovery, according to Ju Li, a materials science and engineering professor at the Massachusetts Institute of Technology. AlphaFold, a DeepMind AI system announced in 2020, predicts the structures of proteins with high accuracy and has since advanced biological research and drug discovery. Thanks to GNoME, the number of known stable materials has grown almost tenfold, to 421,000. “While materials play a very critical role in almost any technology, we as humanity know only a few tens of thousands of stable materials,” said Dogus Cubuk, materials discovery lead at Google DeepMind, at a press briefing. To discover new materials, scientists combine elements across the periodic table. But because there are so many combinations, it’s inefficient to do this process blindly. Instead, researchers build upon existing structures, making small tweaks in the hope of discovering new combinations that hold potential. However, this painstaking process is still very time consuming. Also, because it builds on existing structures, it limits the potential for unexpected discoveries. To overcome these limitations, DeepMind combines two different deep-learning models. The first generates more than a billion structures by making modifications to elements in existing materials. The second, however, ignores existing structures and predicts the stability of new materials purely on the basis of chemical formulas. The combination of these two models allows for a much broader range of possibilities. Once the candidate structures are generated, they are filtered through DeepMind’s GNoME models. The models predict the decomposition energy of a given structure, which is an important indicator of how stable the material can be. “Stable” materials do not easily decompose, which is important for engineering purposes. GNoME selects the most promising candidates, which go through further evaluation based on known theoretical frameworks. This process is then repeated multiple times, with each discovery incorporated into the next round of training. In its first round, GNoME predicted different materials' stability with a precision of around 5%, but it increased quickly throughout the iterative learning process. The final results showed GNoME managed to predict the stability of structures over 80% of the time for the first model and 33% for the second. Using AI models to come up with new materials is not a novel idea. The Materials Project, a program led by Kristin Persson at Berkeley Lab, has used similar techniques to discover and improve the stability of 48,000 materials. However, GNoME’s size and precision set it apart from previous efforts. It was trained on at least an order of magnitude more data than any previous model, says Chris Bartel, an assistant professor of chemical engineering and materials science at the University of Minnesota. Doing similar calculations has previously been expensive and limited in scale, says Yifei Mo, an associate professor of materials science and engineering at the University of Maryland. GNoME allows these computations to scale up with higher accuracy and at much less computational cost, Mo says: “The impact can be huge.” Once new materials have been identified, it is equally important to synthesize them and prove their usefulness. Berkeley Lab’s new autonomous laboratory, named the A-Lab, has been using some of GNoME’s discoveries with the Materials Project information, integrating robotics with machine learning to optimize the development of such materials. The lab is capable of making its own decisions about how to make a proposed material and creates up to five initial formulations. These formulations are generated by a machine-learning model trained on existing scientific literature. After each experiment, the lab uses the results to adjust the recipes. Researchers at Berkeley Lab say that A-Lab was able to perform 355 experiments over 17 days and successfully synthesized 41 out of 58 proposed compounds. This works out to two successful syntheses a day. In a typical, human-led lab, it takes much longer to make materials. “If you’re unlucky, it can take months or even years,” said Persson at a press briefing. Most students give up after a few weeks, she said. “But the A-Lab doesn’t mind failing. It keeps trying and trying.” Researchers at DeepMind and Berkeley Lab say these new AI tools can help accelerate hardware innovation in energy, computing, and many other sectors. “Hardware, especially when it comes to clean energy, needs innovation if we are going to solve the climate crisis,” says Persson. “This is one aspect of accelerating that innovation.” Bartel, who was not involved in the research, says that these materials will be promising candidates for technologies spanning batteries, computer chips, ceramics, and electronics. Lithium-ion battery conductors are one of the most promising use cases. Conductors play an important role in batteries by facilitating the flow of electric current between various components. DeepMind says GNoME identified 528 promising lithium-ion conductors among other discoveries, some of which may help make batteries more efficient. However, even after new materials are discovered, it usually takes decades for industries to take them to the commercial stage. “If we can reduce this to five years, that will be a big improvement,” says Cubuk. Correction: This story has been updated to make clear where the lab's data comes from. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems. It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it?

Some deaf children in China can hear after gene therapy treatment
https://www.technologyreview.com/2023/10/27/1082551/gene-treatment-deaf-children-hearing-china/
October 27, 2023
Antonio Regalado
After deafness treatment, Yiyi can hear her mother and dance to the music. But why is it so noisy at night? Here’s the easy game Li Xincheng has been playing at home. Her mother says a few words. Then the six-year-old, nicknamed Yiyi, repeats what she heard. “Clouds, one by one, blossomed in the mountains,” says her mother, Qin Lixue, while covering her mouth so Yiyi can’t read her lips. “Clouds, one, one, blossomed in big mountains” Yiyi replies. It’s hard to believe that Yiyi was born entirely deaf. But this year her family, who live in a high-rise block in the city of Dongguan, enrolled her in a study of a new type of gene therapy. During the procedure, doctors used a virus to add replacement DNA to the cells in Yiyi’s inner ear that pick up vibrations, allowing them to transmit sound to her brain. In less than a month, her mother says, she was hearing with the treated ear for the first time. Yiyi can’t explain exactly what it’s like in words, but now, at school, she can hear the chime that ends naptime. She used to have to wait for the other kids to tell her. Yiyi is one of several deaf children who scientists in China say are the first people ever to have their natural hearing pathway restored in a dramatic new demonstration of the possibilities of gene therapy. The feat is even more remarkable because until now, no drug of any kind has ever been able to improve hearing. “We were careful, and a little bit nervous, because it was the first in the world,” says Yilai Shu, a surgeon and scientist at Fudan University in Shanghai who is leading the experiment. His team began the treatments last December, and before that he spent years developing the techniques involved, testing gene injections in countless mice and guinea pigs. “That was my project: How do we deliver this to the inner ear?” Shu says. In the US and Europe, gene therapy has been notching successes, including restoring limited vision to people with genetic causes of blindness. Now Shu’s study, in which as many as 10 kids have been enrolled, may be remembered as China’s first domestic gene-therapy breakthrough, as well the most dramatic restoration of a lost sense yet achieved. “Before the treatment, if you put them in a movie theater with the loudest sound, they wouldn’t hear it,” says Zheng-Yi Chen, an associate professor at Mass Eye and Ear, a Harvard-affiliated hospital in Boston, who helped design and plan the study. “Now they can hear close to normal speech, and one can hear a whisper.” Today, Shu is scheduled to present data on the first five children he treated at a meeting of the European Society for Gene and Cell Therapy in Brussels, Belgium. Four of them gained hearing in the treated ear, but one did not, possibly because of preexisting immunity to the type of virus used to convey new DNA into the body. “Any hearing improvement I would call a total win, and getting patients to moderate hearing loss is remarkable,” says Lawrence Lustig, a physician at Columbia University who runs studies of hearing treatments. “As a first step, this is huge.” The new treatment will nothelp everyone who is deaf. It applies only to one specific cause of deafness at birth: a defect in a gene that produces a protein called otoferlin. The inner ear contains about 16,000 hair cells, so called because they have comblike extensions that vibrate to different frequencies of sound. Without otoferlin, these cells can’t transmit the chemicals that relay information to the brain. “These patients basically don’t have a signal coming from the hair cells,” says Chen. Otoferlin gene defects are the cause of around 1% to  3% of cases of inborn deafness, and there are only about 900 new cases a year in China, meaning the condition is rare. But the Chinese success is expected to electrify researchers working on related genetic treatments. “This could be the gateway drug that drives a lot of funds toward other causes of deafness,” says Lustig. The new treatment is designed to add a working copy of the otoferlin gene. Because of the gene’s large size—it is around 6,000 DNA letters long—it had to be broken into two parts, each packaged separately into millions of copies of a harmless virus. Shu then carefully injects the loaded viruses deep into a fluid-filled chamber in a part of the children’s ears called the cochlea. Once inside the body, Shu says, the two sections of DNA recombine to make a complete gene able to guide the production of the missing otoferlin protein. “This technique is not usually done clinically, because the recombination process can be rather inefficient,” says Nicole Paulk, CEO of Siren Biotechnology and an expert on this type of virus, called an adeno-associated virus, or AAV. “This said, if the data they described [is] true, then this is a fantastic result.” Shu also thinks the treatment can be made more potent. But already, he says, the children’s hearing improved, on average, from not hearing anything under 95 decibels (as loud as a motorcycle) to hearing sounds at 50 to 55 decibels—about the level of a regular conversation. “They reach maybe 60% to 65% of normal hearing,” says Shu. Some of the subjects are just toddlers, who can’t tell doctors anything about what their first experiences of sound are like. But their parents are seeing behavioral changes. According to Shu, one child, who had never spoken, has started to say “baba” and “mama” after the treatment. Shu believes children would ideally be treated at around one year of age, a key moment for speech development. Yiyi is older, and like several of the children in the trial so far, she had previously received a cochlear implant, an electronic device that uses a receiver and electrodes to stimulate hearing by tapping directly into the main auditory nerve. With the implant in her right ear since she was two, Yiyi had already learned how to speak, although her mother says that when she disconnects it (the receiver and battery are external), “she can’t hear anything at all.” That changed after the treatment, which was put into her other ear. It was only weeks later that Yiyi could hear naturally from that ear.  Her mother notices that she sometimes disconnects her implant while playing with the neighbors. “When I first heard about the trial, at first I didn’t believe it was real. I asked some audiologists, and they also said it might not be real,” she says. But after she traveled to Shanghai and met Shu and other doctors, she decided to enroll her daughter. “I still can’t quite believe that it worked,” she says. Chen believes that gene therapy could offer better hearing than what can be achieved with an implant. “Cochlear implants are the most successful neural prosthesis ever developed,” he says, but they have limitations. With an implant, Chen says, “you may hear the music, but the nuance is totally gone—they just hear the beat. We hear wind in the trees and birds singing, but they can't. So the goal for everyone has been how to reverse hearing loss.” The decision to target Yiyi’s specific type of deafness was not an accident. Auditory hair cells respond relatively well to gene therapy, easily taking up new DNA. And they don’t grow or get replaced during a person’s life. This is a reason why very loud noises can lead to permanent hearing loss: they can kill the hair cells. But it also means that if a replacement gene is added to the cells, it could remain active for a lifetime, although Shu cautions it’s unknown how long the effect will persist. The apparent success means the Chinese team has won the first lap in a race involving at least three Western biotechnology companies. Among them is Akouos, which last year was acquired for $500 million by Eli Lilly, and Regeneron’s Decibel Therapeutics. Both have opened clinical trials for gene therapies also targeting the otoferlin gene and Decibel has treatedat least one patient. The Chinese work was sponsored by a small biotech company, Shanghai Refreshgene Therapeutics. That company’s founder, Nova Liu, said treating hearing was part of a strategy to develop gene therapies with reasonable prices. That could be the case with treatments for the eye and ear, since injecting gene therapy into either can require around a thousandth the amount of material needed to administer those treatments as IV infusions. “In China the first element of commercialization is to be affordable,” says Liu. For Yiyi, hearing better is a revelation, but there are some downsides. The family lives on the 15th floor of an apartment building, but there is a lot of traffic nearby whose rumble reaches their windows. Before the gene therapy, Yiyi would unplug the implant and hear nothing at all during the night. Now, her mother says, “she’s complaining it’s too noisy.” New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. As a patient enrolled in a clinical trial for Vertex’s new treatment, I was among the first to experience CRISPR’s transformative effects. Gene editing for sickle-cell is here. This is how researchers knew what DNA to change. CRISPR is being used in an experimental effort to eliminate the virus that causes AIDS.

Three things to know about the White House’s executive order on AI
https://www.technologyreview.com/2023/10/30/1082678/three-things-to-know-about-the-white-houses-executive-order-on-ai/
October 30, 2023
Tate Ryan-Mosley
Experts say its emphasis on content labeling, watermarking, and transparency represents important steps forward. MIT Technology Review Explains: Let our writers untangle the complex, messy world of technology to help you understand what's coming next.You can read more from the series here. The US has set out its most sweeping set of AI rules and guidelines yet in anexecutive orderissued by President Joe Biden today. The order will require more transparency from AI companies about how their models work and will establish a raft of new standards, most notably for labeling AI-generated content. The goal of the order, according to the White House, is to improve “AI safety and security.” It also includes a requirement that developers share safety test results for new AI models with the US government if the tests show that the technology could pose a risk to national security. This is a surprising move that invokes the Defense Production Act, typically used during times of national emergency. The executive order advances the voluntary requirements for AI policy that the White House set back in August, though itlacks specifics on how the rules will be enforced. Executive orders are also vulnerable to being overturned at any time by a future president, and they lack the legitimacy of congressional legislation on AI, which looks unlikely in the short term. “The Congress is deeply polarized and even dysfunctional to the extent that it is very unlikely to produce any meaningful AI legislation in the near future,” says Anu Bradford, a law professor at Columbia University who specializes in digital regulation. Nevertheless, AI experts have hailed the order as an important step forward, especially thanks to its focus on watermarking and standards set by the National Institute of Standards and Technology (NIST). However, others argue that it does not go far enough to protect people against immediate harms inflicted by AI. Here are the three most important things you need to know about the executive order and the impact it could have. What are the new rules around labeling AI-generated content? The White House’s executive order requires the Department of Commerce to develop guidance for labeling AI-generated content. AI companies will use this guidance to develop labeling and watermarking tools that the White House hopes federal agencies will adopt. “Federal agencies will use these tools to make it easy for Americans to know that the communications they receive from their government are authentic—and set an example for the private sector and governments around the world,” according to a fact sheet that the White House shared over the weekend. The hope is that labeling the origins of text, audio, and visual content will make it easier for us to know what’s been created using AI online. These sorts of tools are widely proposed as a solution to AI-enabled problems such as deepfakes and disinformation, and in a voluntary pledge with the White House announced in August, leading AI companies such as Google and Open AI pledged todevelop such technologies. The trouble is that technologies such as watermarks are still very much works in progress. There currently are no fully reliable ways to label text or investigate whether a piece of content was machine generated. AI detection tools are stilleasy to fool. The executive order also falls short of requiring industry players or government agencies to use these technologies. An internet protocol called C2PA adds a “nutrition label” to images, video, and audio. On a call with reporters on Sunday, a White House spokesperson responded to a question from MIT Technology Review about whether any requirements are anticipated for the future, saying, “I can imagine, honestly, a version of a call like this in some number of years from now and there'll be a cryptographic signature attached to it that you know you’re actually speaking to [the White House press team] and not an AI version.” This executive order intends to “facilitate technological development that needs to take place before we can get to that point.” The White House says it plans to push forward the development and use of these technologies with theCoalition for Content Provenance and Authenticity, called the C2PA initiative. Aswe’ve previously reported, the initiative and itsaffiliated open-source communityhas been growing rapidly in recent months as companies rush to label AI-generated content. The collective includes some major companies like Adobe, Intel, and Microsoft and has devised a new internet protocol that uses cryptographic techniques to encode information about the origins of a piece of content. The coalition does not have a formal relationship with the White House, and it’s unclear what that collaboration would look like. In response to questions, Mounir Ibrahim, the cochair of the governmental affairs team, said, “C2PA has been in regular contact with various offices at the NSC [National Security Council] and White House for some time.” The emphasis on developing watermarking is good, says Emily Bender, a professor of linguistics at the University of Washington. She says she also hopes content labeling systems can be developed for text; current watermarking technologies work best on images and audio. “[The executive order] of course wouldn’t be a requirement to watermark, but even an existence proof of reasonable systems for doing so would be an important step,” Bender says. Will this executive order have teeth? Is it enforceable? While Biden’s executive order goes beyond previous US government attempts to regulate AI, it places far more emphasis on establishing best practices and standards than on how, or even whether, the new directives will be enforced. The order calls on the National Institute of Standards and Technology to set standards for extensive “red team” testing—meaning tests meant to break the models in order to expose vulnerabilities—before models are launched. NIST has been somewhat effective at documenting how accurate or biased AI systems such asfacial recognitionare already. In 2019, a NIST study of over 200 facial recognition systems revealed widespread racial bias in the technology. However, the executive order does not require that AI companies adhere to NIST standards or testing methods. “Many aspects of the EO still rely on voluntary cooperation by tech companies,” says Bradford, the law professor at Columbia. The need for transparency around AI-generated content is clear, but the value of measures like watermarks is not. The executive order requires all companies developing new AI models whose computational size exceeds a certain threshold to notify the federal government when training the system and then share the results of safety tests in accordance with the Defense Production Act. This law has traditionally been used to intervene in commercial production at times of war or national emergencies such as the covid-19 pandemic, so this is anunusual way to pushthrough regulations. A White House spokesperson says this mandate will be enforceable and will apply to all future commercial AI models in the US, but will likely not apply to AI models that have already been launched. The threshold is set at a point where all major AI models that could pose risks “to national security, national economic security, or national public health and safety” are likely to fall under the order, according to the White House’s fact  sheet. The executive order also calls for federal agencies to develop rules and guidelines for different applications, such as supporting workers’ rights, protecting consumers, ensuring fair competition, and administering government services. These more specific guidelines prioritize privacy and bias protections. “Throughout, at least, there is the empowering of other agencies, who may be able to address these issues seriously,” says Margaret Mitchell, researcher and chief ethics scientist at AI startup Hugging Face. “Albeit with a much harder and more exhausting battle for some of the people most negatively affected by AI, in order to actually have their rights taken seriously.” What has the reaction to the order been so far? Major tech companies have largely welcomed the executive order. Brad Smith, the vice chair and president of Microsoft, hailed it as “another critical step forward in the governance of AI technology.” Google’s president of global affairs, Kent Walker, said the company looks “forward to engaging constructively with government agencies to maximize AI’s potential—including by making government services better, faster, and more secure.” “It’s great to see the White House investing in AI’s growth by creating a framework for responsible AI practices,” said Adobe’s general counsel and chief trust officer, Dana Rao. The White House’s approach remains friendly to Silicon Valley, emphasizing innovation and competition rather than limitation and restriction. The strategy is in line with thepolicy priorities for AI regulation set forthby Senate Majority Leader Chuck Schumer, and it further crystallizes the lighter touch of the American approach to AI regulation. However, some AI researchers say that sort of approach is cause for concern. “The biggest concern to me in this is it ignores a lot of work on how to train and develop models to minimize foreseeable harms,” says Mitchell. Instead of preventing AI harms before deployment—for example, by making tech companies’ data practices better—the White House is using a “whack-a-mole” approach, tackling problems that have already emerged, she adds. The highly anticipated executive order on artificial intelligence comes two days before the UK’s AI Safety Summit and attempts to position the US as a global leader on AI policy. It will likely have implications outside the US, adds Bradford. It will set the tone for the UK summit and will likely embolden the European Union to finalize itsAI Act, as the executive order sends a clear message that the US agrees with many of the EU’s policy goals. “The executive order is probably the best we can expect from the US government at this time,” says Bradford. Correction: A previous version of this story had Emily Bender's title wrong. This has now been corrected. We apologize for any inconvenience. Legislators are responding quickly after teens used AI to create nonconsensual sexually explicit images. Here's what you need to know. There’s still so much we don’t know about social media’s impact. But Meta president of global affairs Nick Clegg tells MIT Technology Review that he hopes new tools the company just released will start to change that. New York City is fixing the relationship between government and technology–and not in the ways you’d expect.

How a tiny Pacific Island became the global capital of cybercrime
https://www.technologyreview.com/2023/11/02/1082798/tiny-pacific-island-global-capital-cybercrime/
November 2, 2023
Jacob Judah
Despite having a population of just 1,400, until recently, Tokelau’s .tk domain had more users than any other country. Here’s why. Tokelau, a necklace of three isolated atolls strung out across the Pacific, is so remote that it was the last place on Earth to be connected to the telephone—only in 1997. Just three years later, the islands received a fax with an unlikely business proposal that would change everything. It was from an early internet entrepreneur from Amsterdam, named Joost Zuurbier. He wanted to manage Tokelau’s country-code top-level domain, or ccTLD—the short string of characters that is tacked onto the end of a URL. Up until that moment, Tokelau, formally a territory of New Zealand, didn’t even know it had been assigned a ccTLD. “We discovered the .tk,” remembered Aukusitino Vitale, who at the time was general manager of Teletok, Tokelau’s sole telecom operator. Zuurbier said “that he would pay Tokelau a certain amount of money and that Tokelau would allow the domain for his use,” remembers Vitale. It was all a bit of a surprise—but striking a deal with Zuurbier felt like a win-win for Tokelau, which lacked the resources to run its own domain. In the model pioneered by Zuurbier and his company, now named Freenom, users could register a free domain name for a year, in exchange for having advertisements hosted on their websites. If they wanted to get rid of ads, or to keep their website active in the long term, they could pay a fee. In the succeeding years, tiny Tokelau became an unlikely internet giant—but not in the way it may have hoped. Until recently, its .tk domain had more users than any other country’s: a staggering 25 million. But there has been and still is only one website actually from Tokelau that is registered with the domain: the page for Teletok. Nearly all the others that have used .tk  have been spammers, phishers, and cybercriminals. Everyone online has come across a .tk––even if they didn’t realize it. Because .tk addresses were offered for free, unlike most others, Tokelau quickly became the unwitting host to the dark underworld by providing a never-ending supply of domain names that could be weaponized against internet users. Scammers began using .tk websites to do everything from harvesting passwords and payment information to displaying pop-up ads or delivering malware. Many experts say that this was inevitable. “The model of giving out free domains just doesn’t work,” says John Levine, a leading expert on cybercrime. “Criminals will take the free ones, throw it away, and take more free ones.” Tokelau, which for years was at best only vaguely aware of what was going on with .tk, has ended up tarnished. In tech-savvy circles, many painted Tokelauans with the same brush as their domain’s users or suggested that they were earning handsomely from the .tk disaster. It is hard to quantify the long-term damage to Tokelau, but reputations have an outsize effect for tiny island nations, where even a few thousand dollars’ worth of investment can go far. Now the territory is desperately trying to shake its reputation as the global capital of spam and to finally clean up .tk. Its international standing, and even its sovereignty, may depend on it. To understand how we got here, you have to go back to the chaotic early years of the internet. In the late ’90s, Tokelau became the second-smallest place to be assigned a domain by the Internet Corporation for Assigned Names and Numbers, or ICANN, a group tasked with maintaining the global internet. These domains are the address books that make the internet navigable to its users. While you can create a website without registering a domain name for it, it would be like building a house without an easily findable postal address. Many domains are familiar. The UK has .uk, France .fr, and New Zealand .nz. There are also domains that are not tied to specific countries, such as .com and .net. Most countries’ domains are run by low-profile foundations, government agencies, or domestic telecom companies, which usually charge a few dollars to register a domain name. They usually also require some information about who is registering and keep tabs to prevent abuse. But Tokelau, with just 1,400 inhabitants, had a problem: it simply didn’t have the money or know-how to run its own domain, explains Tealofi Enosa, who was the head of Teletok for a decade before stepping down in July 2023. “It would not be easy for Tokelau to try and manage or build the local infrastructure,” Enosa says. “The best arrangement is for someone else from outside to manage it, trade it, and bring in money from it.” This is precisely what Zuurbier, the businessman from Amsterdam, wanted to do. Zuurbier had come across Tokelau while chasing the internet’s next big idea. He was convinced that just as people had adopted free email addresses by the millions, the natural next step was for them to have their own free websites. Zuurbier intended to put advertisements on those sites, which could be removed for a small fee. All he needed to turn this billion-dollar idea into reality was a place with a ccTLD that had not yet found a registrar. Tokelau—the last corner of the British Empire to be informed about the outbreak of World War I, where regular shortwave radio wasn’t available until the ’70s and most people were yet to even see a website—was the perfect partner. Representatives from Tokelau and Zuurbier met in Hawaii in 2001 and put pen to paper on a deal. Quickly, .tk domain names began to pop up as people took advantage of the opportunity to create websites for free. He still had to convince ICANN, which oversees the domain name system, that Tokelau couldn’t host its own servers—one of the criteria for ccTLDs. But Tokelau—which switched off its power at midnight—would still need a reliable internet connection to keep in touch. In 2003 Zuurbier took a grueling 36-hour boat ride from Samoa to Tokelau to install internet routers that he had bought for $50 on eBay. Gone was the unreliable dial-up. Tokelau had met modernity. “He provided all the equipment, got all the three atolls connected up, and then he also provided some funding which I used to share with the community,” says Vitale, who established internet cafés that could be used for free by anybody from Tokelau’s four hamlets. For the first time, thousands of Tokelauans in New Zealand were able to easily connect with home. “What was important for Tokelau was that we were getting some money that could help the villages,” says Vitale.  Many of the initial sign-ups on .tk were completely innocuous individuals wanting to blog about thoughts and holidays, as well as gaming communities and small businesses. In an attempt to protect its forests and famous wildlife, Virunga has become the first national park to run a Bitcoin mine. But some are wondering what the hell crypto has to do with conservation. Zuurbier sent Teletok regular reports about .tk’s growth, and they indicated that the free-domain model was working better than anybody expected. Tiny Tokelau, which was being paid a small cut of the profits Zuurbier was making, was going global. “We were hearing how successful .tk was. We were bigger than China,” says Vitale. “We were surprised, but we didn’t know what it meant for Tokelau. What was more meaningful at the time was that we were getting money to help the villages. We didn’t know about the other side of it then.” As the decade wore on, however, it looked to Vitale as if things were beginning to blow off course. “We went in blind,” he says. “We didn’t know how popular it would be.” It took until the late 2000s for Vitale to realize that something had gone badly wrong. After problems first arose, Zuurbier invited ministers and advisors from Tokelau to the Netherlands, paid for their flights, and explained the business’s nuts and bolts in an effort to reassure them. They went to watch Samoa play at the Rugby World Cup in France. “He [Zuurbier] appeared to be a really nice person,” Vitale remembers. “There was all this nice stuff that felt homely, warm fuzzies.” .Tk had hit the milestone of 1 million domain users. But soon after this trip, he says, Zuurbier started falling behind on scheduled payments to Tokelau worth hundreds of thousands of dollars. (MIT Technology Review requested an interview with Zuurbier. He initially accepted but subsequently did not answer the phone or respond to messages.) Meanwhile, Vitale had begun receiving complaints from concerned members of the “internet community.” He and his peers started to become aware that criminals and other questionable figures had cottoned onto the benefits that registering free domains could bring—providing an almost unlimited supply of websites that could be registered with virtual anonymity. “It was obvious from the start that this was not going to turn out well,” says Levine, coauthor ofThe Internet for Dummies. “The only people who want those domains are crooks.” Levine says that .tk had started attracting unsavory characters almost immediately. “The cost of the domain name is tiny compared to everything else that you need to do [to set up a website], so unless you’re doing something weird that actually needs lots of domains—which usually means criminals—then the actual value in free domains is insignificant,”  he says. What started as techies complaining to Vitale about spamming, malware, and phishing on .tk domains soon turned into more worrisome complaints from the New Zealand administrator tasked with overseeing Tokelau, asking him whether he was aware of who .tk’s users were. Allegations surfaced that .tk websites were being used for pornography. Researchers had found jihadists and the Ku Klux Klan registering .tk websites to promote extremism. Chinese state-backed hackers had been found using .tk websites for espionage campaigns. “Satanic stuff” is how Vitale describes it: “There were some activities that were not really aligned with our culture and our Christianity, so that didn’t work very well for Tokelau.” With Zuurbier not replying to worried emails, Vitale moved to unplug him. He opened negotiations with Internet NZ, the registry that runs New Zealand’s squeaky-clean domain, about how Tokelau might be able to wiggle out of its arrangement. He didn’t manage to get an answer before he moved on from Teletok. His successor, Enosa, tried to set the relationship on a new footing and signed new deals with Zuurbier on the understanding that he would clean up .tk. However, that never happened. One of Enosa’s final acts as general manager at Teletok, in the summer of 2023, was to reopen negotiations with Internet NZ about how Tokelau might be able to extricate itself from the deal once and for all. Meanwhile, most of Tokelau’s residents weren’t even aware of what was happening. Elena Pasilio, a journalist, saw firsthand how much this was hurting her home. When she was studying in New Zealand a few years ago, people—knowing that she was Tokelauan—started to tag her on social media posts complaining about .tk. At first, she felt confused; it took time before she even realized that .tk meant Tokelau. “I was really surprised by how many users it had, but then I realized that a lot of people were using .tk to make dodgy websites, and then I felt embarrassed. I was embarrassed because it had our name on it,” Pasilio explains. “It has got our name tangled up there with crimes that people here would not even begin to understand.” There is a sense from both Vitale and Enosa that Zuurbier cared little as Tokelau’s reputation was dragged through the mud. “I would argue with Joost,” Enosa says, adding that he would remind him he was the custodian for a legal asset that belonged to Tokelau alone. According to Enosa, he would shoot back: “I built this infrastructure from my own pocket. I spent millions of dollars building it. Do you think that was easy? Do you think that Tokelau can build this kind of infrastructure itself?” “I said: ‘Okay. Understood,’” Enosa recalls. “I understood how a white man looks at it. You know? This is how white men look at things. I understand that.” What has happened to Tokelau is not unique. The domains of small islands across the Pacific are cited in numerous stories either celebrating dumb luck or complaining of massive abuse. Tuvalu has managed to turn .tv into approximately 10% of its annual GDP. Micronesia’s .fm has been pushed heavily at radio stations and podcasters. Tonga’s .to has been favored by torrent and illegal streaming websites. Anguilla, in the Caribbean, is heavily marketing its .ai at technology startups. But these success stories seem to be the exception. In 2016, the Anti-Phishing Working Group found that alongside .tk and .com, the Australian Cocos Islands (.cc) and Palau (.pw) together represented 75% of all malicious domain registrations. They had been flooded by phishers attacking Chinese financial institutions. The Cocos Islands made headlines in Australia when websites allegedly hosting child sexual abuse images were recently found on its domain. Those domains whose names—by linguistic luck—seemed to mean something tended to attract better managers. Sharks seem to have circled around those that did not, or had a market that was less clear. While the abuse of Pacific Islands’ domains has waxed and waned over the years, the islands’ tiny size means that even small associations with crime can have damaging consequences. “There is a problem in Polynesia,” says Pär Brumark, a Swede who represents the Pacific island of Niue abroad. “You had these internet cowboys running around taking domains everywhere.” Niue lost control over the domain .nu after it was “stolen” by an American in the late 1990s, Brumark says. Its management was given to the Swedish Internet Foundation—which manages Sweden’s native .se—in a “shady deal” in 2013, he claims. .Nu has been wildly popular in Sweden, as it translates directly to “now.” Niue, which is also linked to New Zealand, is now fighting a David-versus-Goliath battle in the Swedish courts. It is seeking as much as $20 million in lost revenue—almost one year’s worth of Niue’s annual GDP. “Digital colonialism,” claims Brumark. “They exploit resources from another country without giving anything back. They have never spoken to the government. They have no permissions. They exploit. Colonialism to me is if you take resources from a country that you do not have the permission to take.” But now there may finally be some accountability—at least in the case of Zuurbier. In December 2022, courts in the Netherlands found in favor of an investor suing Freenom, the company that managed .tk and four other domains—those of Gabon, Equatorial Guinea, the Central African Republic, and Mali—that were subsequently added to the model it pioneered. The courts found that Freenom had fallen foul of various reporting rules and appointed a supervisory director. A new class of crypto investors have bold plans to rebuild society from scratch. But their pet projects risk repeating the region’s long history of corporate colonialism. And in March of this year, Meta, which owns Facebook, Instagram, and WhatsApp, also sued Freenom for damages, claiming that sites hosted on .tk and the four African domains were engaging in cybersquatting, phishing, and trademark infringement. Meta provided examples of websites that appeared to be registered at .tk with the express purpose of deceiving users, such as faceb00k.tk, whatsaap.tk, Instaqram.tk. In an interview with the Dutch newspaper NRC, Zuurbier denied Meta’s allegations about the “proliferation of cybercrime.” But the Cybercrime Information Center recently reported that “in past years Freenom domains were used for 14% of all phishing attacks worldwide, and Freenom was responsible for 60% of the phishing domains reported in all the ccTLDs in November 2022.” Zuurbier says that Freenom distributed to over 90 trusted organizations, including Meta, an API that allowed them to take down offending sites and that Meta itself failed to continue using it. But many in the tech industry resent what they see as Freenom shifting the cost of policing its domains onto others. As of January 2023, it is no longer possible to register a .tk domain. All four African countries—many thousands of times larger than Tokelau—have broken ties with Freenom. Tokelau, which did not seem aware that there were other countries in the same boat, is still trying to figure out what to do next. It now looks as if Freenom is essentially finished as a company. But Enosa doesn’t believe that’ll stop Zuurbier from pursuing more shady schemes. “Joost always wins,” he says. Without access to the unlimited pool of free domain names that were available through .tk and the four other Freenom ccTLDs, many cybercrime groups that relied on them are being forced to adapt. Certain scattergun approaches to spamming and phishing are likely to go out of fashion. “Spammers are fairly rational,” explains Levine, the spam expert. “If the spam is cheap and the domains are free, they can afford to send out a lot of spam even though the likelihood of response is lower. If they actually have to pay for the domains, then they are likely to make it a lot more targeted.” “Bad things online require a domain name at some point,” says Carel Bitter, head of data at the Spamhaus Project, which tracks malicious activities online. “You need people to go somewhere to fill in their account details. If you can’t get domains for free, you will have to get them somewhere else.” Analysts have noted upticks in malicious use of cheap “new” generic TLDs such as .xyz, .top, and .live, whose reputations have been wrecked by dodgy dealers. While other domains may only cost $1, a drop in the ocean for the largest gangs, the fact that they now need to be purchased may limit the damage, says Bitter: “Any cybercrime business that relies on domain names will have some sort of natural limit that determines how much they can spend on domain names.” Others, though, may seek to compromise existing websites with low security. It is likely that “basement” operations—so-called “ankle-biters”—will feel the biggest pinch. “What is possible is that the guys that are just doing it as a dabble won’t want to put the money up, but the professionals are not going away,” says Dave Piscitello, director of research activity at the Cybercrime Information Center. “They will go elsewhere. If you are staging a revolution and the cost of a Kalashnikov goes from $150 to $250, you aren’t going to say ‘Forget it.’ It is the business.” The media sometimes reports that Tokelau makes millions from the use of .tk. Zuurbier himself claims on his LinkedIn that his relationship with Tokelau adds over 10% to the atolls’ GDP. “Bullshit,” says Enosa when asked. “That’s a lie.” Enosa claims that .tk provided a “very small” proportion of Teletok’s income: “It doesn’t give us good money. .Tk was nothing to my revenue.” While the arrival of the internet on Tokelau promised to zip information across the Pacific instantaneously, the islands have remained isolated. Even while I was reporting this story, it took weeks to get in touch with Pasilio and other sources there. Interviews were repeatedly delayed because of the price of data packages. Internet in Tokelau is among the most expensive in the world, and NZ$100 (US$60) worth of data can sometimes last only 24 hours at a time. Phone calls to Tokelau from Europe did not connect. “I feel sorry for our Tokelau,” Pasilio says. “We have been taken advantage of. I think people would be shocked if they knew what had been going on with .Tk.” Even many Tokelau elders had not fully understood the problem, at least until recently. There are other, arguably more existential problems the islands need to deal with, including climate change, emigration, and the atolls’ future relationship with New Zealand. “Our islands are already shrinking as it is, with the sea levels rising,” says Pasilio. She says her father tells her about reefs and sand banks that have sunk beneath the Pacific. “They would rather worry about things that they can see physically and that they know more about, rather than fighting back on this .Tk thing,” she says. But the issue of the abused .tk domain was recently raised in the General Fono, or Parliament, indicating that the issue had finally broken out of its technical niche and into the wider public. Those existential issues facing the islands are not wholly unrelated to .tk. Questions over the future of the domain have arisen at the same time that a debate over Tokelau’s political future has been revived. Tokelau is classified by the United Nations as a “non-self-governing territory” under the oversight of the Special Committee on Decolonization. In 2006 and 2007, referenda on whether Tokelau would enter “free association” with New Zealand—a possible stepping stone toward eventual independence—was approved, but not enough of Tokelau’s population voted to meet the turnout threshold. In May 2022, it was decided that another referendum on Tokelau’s future would be held ahead of the centenary of New Zealand rule in 2025. Repairing Tokelau’s devastated international reputation by cleaning up .tk will be a necessity if the atolls are to make any serious bid for sovereignty. Vitale is now the general manager of Tokelau’s government and wants to see its internet domain make a triumphant return to make it clear that the islands are turning a new page. “We are building nationhood here,” he explains. “We are on a pathway toward self-determination. We want to use the .tk as a catalyst to promote our nationhood and be proud of it—our domain name and our identity among the internet community.” All of Tokelau’s email and website addresses are currently hosted on New Zealand’s .nz. “What does that mean to people? It means that we are in New Zealand,” says Vitale with a sigh. “We should be selling ourselves as being in Tokelau, because .tk is the domain—the identity—for Tokelau.” “When you have people coming to knock on your door with attractive packages,” he adds, “you see it as an opportunity you hook onto—not realizing what the consequences will be further down the road.” Correction: This story has been updated post-publication as the previous version incorrectly stated that Antigua was the Carribean island with the .ai domain. It is in fact Anguilla. Our apologies. New capabilities unlocked by advanced analytics, AI, and machine learning enable smarter business buying. A decade after the high profile bust of cleantech 1.0, venture-backed firms are again flourishing. We need them to succeed. Will they? The company will pay rival Editas Medicine and the Broad Institute so that it can sell its breakthrough gene-editing treatment for sickle-cell disease. In a bid to find new economic growth, Hong Kong’s government is busy setting up legal frameworks to court Web3 companies.

The best way to prevent this deadly cancer is to remove multiple organs. And I’m about to do it.
https://www.technologyreview.com/2023/12/11/1084853/ovarian-cancer-fallopian-tubes-menopause-genetic-testing/
December 11, 2023

Scientific advances and more accessible genetic testing have made removal of the ovaries and fallopian tubes the recommended way to treat certain people at high risk of ovarian cancer. The results of my genetic test arrived in an unpretentious white envelope. It was the summer of 2021, and I almost missed it when I flipped through the mail, but I set it aside from the rest of the bills to look at later. About a month before, I had sent a sample of my saliva to a genetic testing company in California. I wasn’t even nervous about opening it, so sure was I that it wouldn’t be a big deal. Looking back on it now, it seems wildly naïve. At that point, my family and I had already weathered a grueling three years dealing with the fallout from a cancer diagnosis. In 2018, we found out that my mom, Teresa, had stage 4 ovarian cancer. The diagnosis was sudden, preceded by six months of uncertain and confusing symptoms like back pain, bloating, and loss of appetite. And then there was the shock of stage 4 cancer:Right off the bat? Wait, how many stages are there? My mind reeled as I immediately googled it. I didn’t even have a basic knowledge of ovarian cancer at the time. I stopped doing those internet searches pretty quickly; all I found were grim statistics and websites so dense with scientific jargon I could barely find the verb in a sentence. What I could understand was that ovarian cancer, not as well known or well funded as breast cancer with its ubiquitous pink ribbon, is the deadliest gynecological cancer there is. There is no way to screen for this disease, unlike breast or cervical cancer, with their annual, routine tests—mammograms and Pap smears. So we often can’t see ovarian cancer in its early stages, which is one reason why it’s so deadly. The odds were stacked against her, but my mom somehow survived after a brutal six months of chemotherapy. The relief was palpable for my family. When she was well again, in early 2019, she celebrated with a kids-and-grandkids tour, since my brother, sister, and I are spread out across different countries. She started to try to live life normally once more. When the cancer came back 11 months later, the rug was pulled out from under us again, and we were distraught, shocked. The doctors, however, were not. I found out later that the way my mom’s cancer presented was textbook: vague symptoms, a surprise diagnosis, and eventually a recurrence, which happens in 70 to 95% of people who were initially diagnosed with ovarian cancer in stages 3 or 4. This is how the disease goes, for the most part. After that first recurrence, doctors recommended that my mom do a genetic test, accompanied by genetic counseling. Researchers are engineering immune cells to take aim at solid tumors. Mom’s results came back positive for a gene mutation, which had contributed to the development of her cancer. This aberration is in a gene calledRAD51C. When she found out, her first action was to call her kids—all three of us—and urge us to go get tested ourselves. It was important and urgent that we do this, she said, because this was the best—and probably only—way to prevent us from being vulnerable to this cancer too. That’s why, eventually, I did get tested: I sent off that little sample to the lab, and received that envelope in the mail. But opening it and reading the results—which revealed that I was a carrier of this mutation also—took me down a path I would never have imagined traveling. I’ll shortly have surgery for prophylactic removal of my ovaries and my fallopian tubes, as a way to make sure I don’t go through what my mom has gone through: four rounds with this cancer in the last five years. In some ways, things are looking up, strange though that may sound for a disease with such a grim survival rate. Scientific advances now offer more opportunities for treatment and prevention than ever before. Increasingly accessible and affordable genetic testing has provided a huge step forward for dealing with a cancer for which there is no screening tool, and whose mortality rates have remained stubbornly high for the past several decades. This is, finally, a way to see around the corner to some extent, a new reason to allow some hope into the conversation. At the same time, recent discoveries have shown that the most common and deadliest form ofovarian cancer startsnot in the ovaries but in the fallopian tubes, and that ovarian cancer is not evenonecancer but comprises several distinct subtypes. This has all informed a newrecommended courseof action for “high-risk” people like me. Though it may seem like an extreme step, removing both tubes (a salpingectomy) and ovaries (an oophorectomy) is at leastsomethingan individual can do to manage the risk. But on a personal level, it’s a hard, anxiety-inducing decision to make. Removing the ovaries leads to immediate menopause, which is not just a giant change in any woman’s life but also increases the risk of cardiovascular disease, dementia, and stroke. Now that I know exactly how important these hidden and underrated organs are, I’m reluctant to part with them, frankly. But there’s no getting away from those grim statistics—most women who get ovarian cancer die from it.So removing my ovaries and fallopian tubes is not an ideal plan of action—but it’s the only one we’ve got so far. When my mom was first diagnosed, I asked her to start recording audio diaries on her phone to document her experiences, whether good or bad. Maybe we can do something with it one day, I told her. Now, five years later, I finally have: “Overlooked: A Podcast about Ovarian Cancer” charts my family’s journey in 10 episodes and explores how little we knew about this disease for many, many years. "I think the ovaries might have suffered a little bit from paternalistic, misogynistic practices of the past. I think we only really, in the last 20 to 30 years, have come to realize how important the ovaries are." I started gathering content for the podcast way before anybody mentioned genetics. This was meant to be a story about my mom, her journey with this cancer, and what it could teach the rest of us. At the time I wasn’t thinking about my own part in this story. I reluctantly became part of it, though, when I did that genetic test. Just in the last few years, new scientific discoveries and more widespread genetic testing have really driven a change of approach in treatment and prevention of all sorts of diseases and ovarian cancer specifically—stories that I covered for the show. This disease is now ever so slightly less overlooked than it used to be. Nevertheless, it’s critical to understand why it was largely ignored for so long. The most fundamental and chilling reason came from Emilie Chiasson, my first interviewee in 2018, who was at the time an advocate with Ovarian Cancer Canada, an organization that raises funds and works with researchers and patients. She told me about the phrases people used to describe this disease, referencing its stealthy nature—“the silent killer” and “the disease that whispers.” And then, she hit me with the stark reality: “Why don’t we know more about this? Well, because unfortunately, most people that are affected by the disease sadly die as a result of it and/or they’re very sick” by the time they’re diagnosed. “So they’re not out there advocating, marching on [Canada’s Parliament] Hill, wearing pink, doing all the things that women have done to really move forward [on] breast cancer,” Chiasson added. My own theory, though, has more to do with how we’ve regarded the ovaries historically. As Dianne Miller, a pioneering gynecological oncologist in British Columbia and cofounder ofOVCARE, a multidisciplinary research program there, told me, “I think the ovaries might have suffered a little bit from paternalistic, misogynistic practices of the past. I think we only really, in the last 20 to 30 years, have come to realize how important the ovaries are.” Important—and complex. In the process of producing the podcast, I learned that the ovaries play a hugely significant role in a woman’s health beyond fertility or even menstruation and menopause. They have an impact on cardiovascular disease, Alzheimer’s, osteoporosis, and more. We’re crawling with microbes, and scientists want to use them to treat disease. The term “ovarian cancer” is not even as straightforward as it sounds, being an umbrella term for different cancers. Doctors only figured this out in 2008, and it was “a huge step forward,” says David Huntsman, the director and another cofounder of OVCARE, whose work was critical to understanding the spectrum of ovarian cancers. The most common—and lethal—type is epithelial ovarian cancer, of which there are several subtypes; they form in the tissue surrounding the ovary, or in the lining of the fallopian tubes, or in peritoneum. There are also germ cell tumors and ovarian stromal tumors, which are rarer. The treatment to some extent depends on the subtype—or at least it should. Previously, Huntsman says, “we were treating a kind of mixture of different problems and trying to find a solution which was going to fit across them, which was never going to work.” My mom’s subtype was an epithelial cancer called high-grade serous, and her first round of treatment was three sessions of chemotherapy; then a “debulking” surgery to remove the cancer and all the reproductive organs; and then three more sessions of chemotherapy. But while doctors understand the cancer better these days, there’s still no way to see it coming:a recent major study in the UKshowed that there is no solid way to screen for it. The closest we’ve come is a blood test for a marker called cancer antigen 125, but it’s far from a fail-safe test. CA125 is sometimeselevatedin a person with ovarian cancer. But there are other causes of a high CA125 (like endometriosis, for example), so doctors don’t consider this a reliable indication of cancer. Making matters worse, ovarian cancer symptoms are dangerously vague: feeling bloated or full after eating very little, or sometimes experiencing pain in the pelvis or the back. These are all easily and often misattributed to other health issues. For years, ovarian cancer advocacy organizations have run campaigns to increase symptom awareness in an effort to detect this cancer early. Yet at the same time, in the words of one of my mom’s oncologists, the disease has been “outsmarting” us. Educating primary-care doctors, who are often the first type of health-care provider that women see about their symptoms, is an important element in tackling the problem of how to spot this cancer. Organizations like Target Ovarian Cancer in the UK have been working for several years to increase awareness of symptoms among primary-care providers within the UK’s National Health Service. But from a physician’s point of view, ovarian cancer won’t be the first thing they think of as they sift through broad symptoms like bloating and pelvic pain—something I heard not just from advocacy organizations, like the UK group, but also from researchers and oncologists in the US and Canada who have worked with physicians. The bottom line is that it’s just a tough cancer to diagnose early. This all adds up to a bleak reality: the cancer is mostly caught in later stages, when it has already spread and is difficult to treat. That’s why genetic testing has become the most important preventive tool there is, particularly sinceabout 20% of cases are hereditary. (Hereditary cancers also have a higher recurrence rate.) Knowing your genetic make-up, and whether your body carries a gene mutation that makes you susceptible to a certain type of cancer, is empowering. But with that knowledge comes a new, tough set of decisions. If a genetic mutation could become a household name, it kind of happened when, in 2013, Angelina Jolie had a bilateral mastectomy after finding out she was a carrier of a mutation in theBRCA1gene. Mutations inBRCA1andBRCA2, discovered about 30 years ago, can increase the risk of breast cancer by up to 85% and of ovarian cancer by up to 60%. The mutation my family has is less well known, more newly discovered, and rarer. But theRAD51Cmutation also means I have a genetic predisposition to ovarian cancer, and potentially breast cancer too. The risk—currently 10% for me—increases with ageuntil about 60, when it starts to decrease. When my mom first called me with the results of her genetic test, she sent me the report. My eyes glazed over. It was hard enough to deal with the fact that she was sick again and would go back to chemotherapy soon. But now I would also have to wrap my brain around understanding this complicated genetic mutation. The idea of “mutations” took my lizard brain straight to X-Men. My mom and I have this inside joke about how science teachers (her) struggle to explain “science-y” things to liberal arts majors (me), and after she had finished reading me the details of her report, there was a pause on the call. “Explain it to me like I’m a liberal arts major, Mum,” I said. TheRAD51Cgene is what’s called a “susceptibility gene,” meaning it’s supposed to protect the body from certain cancers, and a mutation means it can’t perform this job. In this case, that means fighting ovarian cancer. All three of us siblings have made different choices about testing. My brother has not yet been tested but plans to, because as a carrier of this gene, he could pass it on to his kids. My sister did hers almost straight away—the trauma of watching Mom go through that first round with cancer motivated her to react quickly. She found out she too has the mutation. The same fear that compelled my sister to act kept me frozen in hesitation and reluctant to open what I knew would be a Pandora’s box. Around that time the pandemic hit anyway, and everything to do with health care became laser-focused on covid-19; I put off getting myself tested until early 2021. I later got in touch with the San Francisco–based genetic testing company Invitae to get an overall picture of how testing is working within the realm of ovarian cancer.Ed Esplin, a clinical geneticist there, told me that although more people are being tested for this disease and for others, it’s still not systematic. “Unfortunately, there is evidence that less than 40% of all of the cancer patients who already qualify—according to the clinical practice guidelines for genetic testing—are actually getting it, which is a travesty, in my personal opinion,” he says. But last summer Esplin coauthoreda paperpublished in the Journal of the American Medical Association showing that rates of testing for patients diagnosed with ovarian cancer are growing: back in 2013, fewer than 30% got a genetic test; by 2019, that share was more than 40%. Nonwhite patients were tested less frequently, but their rates were also increasing. Testing can inform every aspect of a cancer patient’s care, but the key is that it needs to be done early. “Part of what that’s going to take is education of physicians and patients,” Esplin adds, “so that they understand how critically important genetic testing is, so that it gets done. Part of that is going to be health-care institutions in which these physicians are practicing, putting into place workflows that make this automatic.” My family’s experience is again an example of how this is just beginning to take root: my mom was asked to do a genetic test the second time her cancer was diagnosed, not the first time. Testing is not in any way “automatic,” but there’s certainly more discussion about it than there used to be, even a mere five years ago. Once I finally decided I’d put it off long enough, the process to get tested started with an initial session with agenetic counselor—in my case, a two-hour deep dive into everything from family history with cancer to the nature of my monthly cycle. Then I paid $250 out of pocket for my test, which wasn’t covered by my insurance provider, and sent my saliva to Invitae. That price tag puts it out of reach of a lot of people, but the price of testing has actuallyfallen dramatically in the last 20 years, which makes it much more accessible to the wider public than it used to be. While I probably should have anticipated what my test would reveal, given that my mom and sister were both positive, the result still caught me off guard. I had convinced myself that because this was a “rare” mutation, as all the literature had labeled it, I might not have it. But that was a function of the fear that had gripped me for the previous two years; my intellectual understanding of this disease was filtered through a highly emotional lens, and my brain didn't want to accept what the implications were. I can now understand why people might not want to put themselves forward for testing: there’s no turning away from the knowledge once you have it, and a positive result does force a decision on what to do next. The discovery some 15 years ago that most ovarian cancers start in the fallopian tubes has radically changed how patients—and even women who don’t know if they may be at risk—are treated. Miller, the gynecological oncologist from British Columbia, is now retired but led a team at Vancouver General Hospital that is credited with this discovery—as well as with a new approach of removing the fallopian tubes preventively. So when we spoke, I started out with a cheeky question designed to provoke: “I’m shortly going to have my fallopian tubes out. Do I have you to thank for that, Dr. Miller?” Without skipping a beat, she answered, “Probably not—I’m not your surgeon.” Nevertheless, it was Miller’s team that rewrote the textbooks on where the most common and lethal type of ovarian cancer originates, and their work has now been turned into guidelines for diagnosis and care. Their radical idea was “opportunistic salpingectomy”—removing the fallopian tubes altogether inanywoman already having pelvic surgery, thus preventing ovarian cancer whether or not there was a known risk. (While drastic, this wasn’t, of course, the first time prophylactic surgery had been proposed for cancer prevention, since mastectomies are performed to similarly protect those susceptible to breast cancer.) “Roughly a quarter of women in their lifetime will have their uterus taken out,” says Miller. In fact, hysterectomy is thesecondmost common surgery for women in the US, often performed because of fibroids, endometriosis, cancer prevention, or uterine prolapse. “And if you think of that, if you could prevent a quarter of the high-grade serous cancers from occurring simply by making a slight modification to a surgery, that’s big—that’s better than any chemotherapy drug that’s been developed throughout my career.” Miller’s team started doing this work in British Columbia around 2010, and since then they’ve done numerous studies to prove the safety and efficacy of salpingectomies. There are now 14 gynecological societies across the world, including the American College of Obstetricians and Gynecologists, that recommend opportunistic salpingectomies, and uptake has increased in Canada and the United States. Gillian Hanley,an associate professor in the Department of Obstetrics and Gynecology at the University of British Columbia, says that only 8% of hysterectomies performed in the province in 2008 included an opportunistic salpingectomy; in 2019, 80% did. Hanley also works with OVCARE, the research programcofoundedby Miller, Huntsman, and another doctor; she says the organization has worked with physicians for years, and more recently with surgeons, to ensure that any woman who has a scheduled pelvic surgery is able to have a discussion about this preventive step. “So if a woman is having a surgery, and she no longer wants any future pregnancies, there should be a discussion to say, ‘Hey, we’re in your pelvis. We can remove your fallopian tubes at the same time and dramatically reduce your risk for ovarian cancer. Is this something you would like?’” Hanley says. This conversation obviously forces a decision, if one hasn’t already been made, about one’s desire to have or have more children. My sister, who found out two years earlier than I did that she’d tested positive for theRAD51Cgene mutation, was 37 years old when she had her salpingectomy. She has two daughters and had decided that she was done having children, so the idea of removing her fallopian tubes was one she could live with. And having seen the toll this disease had taken on our mom, she wanted to get out of the line of sight of this cancer. When her daughters are older, they’ll get tested for this mutation as well. These ideas have recently started filtering through to the wider public. Earlier this year, the US-basedOvarian Cancer Research Alliance, one of the largest advocacy organizations for this cancer,had a major shift from an emphasis on early detection; it is now recommending genetic testing, both for diagnosed patients and for other people who know they’re at risk. It now also counsels prophylactic organ removal:opportunistic salpingectomyfor anyone at even “average” risk for the cancer; or, if you’re at increased risk, a salpingectomy whether or not it coincides with another procedure, as well as removal of the ovaries. But this emphasis on genetic testing as a preventive measure hasn’t necessarily been welcomed across the ovarian cancer community, says Sarah DeFeo, the chief program officer at the Ovarian Cancer Research Alliance. “There is this strong attachment to the importance of symptom awareness. And there is this real focus on the promise of early detection,” she says. “And we know that does not work.” “We need to focus on whatdoeswork and what we can do,” she adds. “And we encourage people to know their risk.” As for me, after I got my test results, I dragged my heels on a decision. After seeing the genetic counselor, I eventually found my way to a gynecological oncologist at New York–Presbyterian. There, I was told that the recommendation for high-risk women my age—approaching 50—is to have the ovaries as well as the fallopian tubes removed as a preventive measure. This week, shortly before my 49th birthday, I’ll have this surgery, which will instantly trigger menopause—“surgical” or “forced” menopause are the correct and depressing terms. It fills me with dread, frankly. In an effort to prepare, I find myself going back to googling “what to expect,” and the list is astonishing: menopause can bring hot flashes, brittle bones, heart palpitations, memory loss, insomnia, joint pain, depression, vaginal discomfort, bladder issues, hair loss … I usually don’t make it all the way to the end of these lists. This is where we’re at: a huge advance—finally—in science and technology has shined light on a cancer that has long been in the dark, and has been outsmarting us this entire time. But prevention comes at a price. I’m going to be a walking example of prevention very soon, and it does make me wonder: if you have a gene mutation like mine, it seems that the best way to not get ovarian cancer … is to not have ovaries (or fallopian tubes) at all, which tells me a lot about how powerful this disease really is. I put that to Miller, who has spent the better part of her career with a full awareness of the disease she was trying to outsmart as a scientist. “I can’t disagree with you,” she replied. “But it’s really the best we have for right now. Is it perfect? Absolutely not, for exactly the reasons that you said. But on the other hand, having watched too many women die of ovarian cancer over my career, I just think we have to do something. And there’s something we can do without increasing the morbidity to women.” So even as I turn back to googling, there comes a point when I remind myself that surgical menopause is a better outcome than even the possibility of ovarian cancer. It doesn’t take prisoners, this cancer. Golda Arthur is an audio journalist and podcast producer. She has launched and run podcasts at Vox Media, MIT Technology Review, and Marketplace, and has reported, edited, and produced for the BBC and the CBC. She lives in New York City. New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. After deafness treatment, Yiyi can hear her mother and dance to the music. But why is it so noisy at night? As a patient enrolled in a clinical trial for Vertex’s new treatment, I was among the first to experience CRISPR’s transformative effects. Gene editing for sickle-cell is here. This is how researchers knew what DNA to change.

Climate tech is back—and this time, it can’t afford to fail
https://www.technologyreview.com/2023/12/02/1084059/climate-tech-startups-are-back-and-this-time-they-might-survive/
December 2, 2023
David Rotman
A decade after the high profile bust of cleantech 1.0, venture-backed firms are again flourishing. We need them to succeed. Will they? Lost in a stupor of déjà vu, I rang the intercom buzzer a second time. I had the odd sensation of being unstuck in time. The headquarters of this solar startup looked strangely similar to its previous offices, which I hadvisited more than a decade before. The name of the company had changed from 1366 Technologies to CubicPV, and it had moved about a mile away. But the rest felt familiar, right down to what I had come to talk about: a climate-tech boom. A surge in cleantech investments, which had begun in 2006 with the high-profile entry of some of Silicon Valley’s leading venture capitalists, was still going strong during my first visit, in 2010—or at least it seemed to be. But a year later, it had begun to collapse. The rise of fracking was making natural gas cheap and abundant. US government funding for clean-energy research and deployment was falling. Meanwhile, China had begun to dominate solar and battery manufacturing. By the end of 2011, almost all the renewable-energy startups in the US were dead or struggling to survive. The list of eventual casualties included headline grabbers like the solar-cell maker Solyndra and thehigh-flying battery company A123, as well as numerous less well-known startups in areas like advanced biofuels, innovative battery tech, and solar power. How, I was wondering, had CubicPV survived when nearly all its peers had failed? Ushering me into the conference room (was that the same photo of a solar panel hanging on the wall that I had seen a decade before?), Frank van Mierlo, who is still the CEO, seemed almost giddy. And why not? After more than 10 years in photovoltaic limbo, with few opportunities to scale up its process for making the silicon wafers used in solar cells, the venture-backed company had suddenly seen its fortunes turn around. The excitement around cleantech investments and manufacturing is back, and the money is flowing again. The 2022 US Inflation Reduction Act, which provides strong incentives for US domestic solar manufacturing, changed everything, says van Mierlo. As of this summer, some 44 new US plants had been planned, providing CubicPV with a huge potential demand for its silicon wafers. Three bills investing hundreds of billions into technological development could change the way we think about government’s role in growing prosperity. Call it cleantech 2.0. In recent years, there has been a huge increase in public and private spending, both in the US and elsewhere, on technologies and infrastructure to address climate change. A recent analysis estimates that total green investments reached $213 billion in the US during the 12 months beginning July, 2022. Most of that spending is allocated to building sources of renewable energy, such as wind or solar, as well as to supporting battery and EV manufacturing and creating green hydrogen infrastructure. And the enormous amount of money is creating potential opportunities for the next generation of technologies to feed the expanding markets. For startups like CubicPV, this means that after years of little market demand, the appetite for its products is suddenly almost insatiable. The company is designing a billion-dollar plant to make the silicon wafers needed to feed the rapid expansion in US solar production. What’s more, a bigger solar manufacturing base could eventually provide the startup with a lucrative future market for its next innovation: a new type of solar panel that is far more efficient at capturing sunlight than conventional silicon ones. Silicon Valley and venture capitalists everywhere have fallen in love with the virtues and the promise of new catalysts and electrodes. Innovations in solar cells no longer seem like a lost cause. Startups are boasting radical new technologies for energy storage and carbon-free processes for making chemicals, steel, and cement. Investors are risking billions on scaling up nascent technologies such as geothermal power, fusion reactors, and ways to capture carbon dioxide directly from the air. These innovations in what is being called “deep” or “hard” tech—products and processes based on science and engineering advances—could be critical in addressing climate change. While the past few years have seen remarkable progress in deploying relatively mature renewables such as solar and wind power, as well as strong growth in electric-vehicle sales, large gaps in the cleantech portfolio remain. In its most recent report this fall, the International Energy Agency estimates that around 35% of the emissions cuts needed to meet 2050 climate goals will have to come from technologies not yet available. Key industrial sectors of the economy, in particular, have largely been untouched. Nearlya third of carbon emissionscome from industrial processes used to make steel, cement, chemicals, and other commodities; concrete alone accounts for more than 7% of global emissions, while steel production is responsible for another 7% to 9%. Cleaning up these industries will take an almost unlimited supply of cheap, steady, and easily accessible carbon-free energy. Progress will almost certainly require new science-based innovations. And that’s where venture-backed startups play an essential role. Over the last few decades, large industrial corporations in sectors such as energy, chemicals, and materials have all butabandoned research into new technologies. The days when industrial giants like DuPont created critical new technologies and spun them off into profitable operations are long gone. And while governments and universities fund research, venture-backed firms have emerged as an increasingly key outlet for transforming promising lab discoveries into sustainable businesses. A slew of such startups are now rapidlymoving toward commercialization, providing the first steps toward industrial decarbonization and adoption of radically new energy sources (see chart). But these startups still face some of the same issues that tripped up the cleantech revolution a decade ago. Transforming academic advances in physical sciences and engineering into commercial businesses is a project that’s fraught with dangers. It typically requires startups to build so-called demonstration plants at a relatively large scale to test whether their processes work beyond the lab and are efficient enough to compete with existing technologies. This is risky and expensive. Then, if it all works, startups commercializing, say, new energy sources or low-carbon processes to make concrete or steel face low-margin, well-established markets. They must often compete with mature processes that have been optimized over many decades. These costly and time-consuming steps to commercialization, which a climate-tech startup must survive before it has any significant revenues, is often known as the “valley of death.” Few startups in cleantech 1.0 were able to navigate it. The question now is: Can today’s ambitious startups successfully scale up their technologies and move across that valley this time around? These fledging venture-backed companies will first need to prove that their technologies work at a commercial scale. Then, if successful, they face the even harder challenge of making an impact on the huge energy and industrial markets, figuring out how to work with established companies to clean up these sectors. Can they survive? The bad news is that the record for suchventure-backed startups is dismal. From 2006 to around 2011, when much of the sector lay in ashes, venture capitalists spent about $25 billion on cleantech startups. The VCs lost more than half their money. It was particularly bad for those firms we would now call deep-tech startups; investments in stuff like new types of solar cells, advanced biofuels, and novel battery chemistries returned only about 16 cents on a dollar. For much of the rest of the decade, investors hunkered down. As spending on cleantech dwindled to miserly levels, consumer-facing software-based businesses (think Airbnb and Uber) took off. The common wisdom was that advances based on science and engineering in cleantech were too expensive and risky to scale up. The proportion of venture capital going to cleantech dropped from more than 8% in 2008 to around 3% between 2016 and 2020. Even before the 2022 IRA passed, however, venture investors had again beguneyeing the massive potential markets for climate tech, as governments around the world increased spending and more and more corporations set long-term emission-reduction goals. The markets are now real and growing, not speculative. While innovative battery startups a decade ago faced a tiny market for electric vehicles, today there is a huge demand for cheaper and more powerful batteries as sales of EVs take off. Likewise, demand for grid storage is growing as more renewable power is deployed and for cleaner industrial processes as companies pledge to reduce their carbon pollution. Yet the trajectory of climate tech in recent years hasn’t been a straight line. Venture investments in cleantech startups, which amounted to just $2 billion in 2013, soared to nearly $30 billion in the US by 2021, according to the National Venture Capital Association. Then, just as things started to heat up, inflation and the resulting rise in interest rates began to make borrowing money expensive. The general venture capital market began to crash in 2022, and investments in climate tech soon followed. In the first half of 2023, investments in climate-tech startups weredown 40% from the same period in 2022, reports Sightline Climate, a market intelligence firm. But dig deeper into the numbers and a mixed picture emerges. For one thing, Sightline Climate says investments have begun creeping back up in the latest quarter this fall. And though funding overall became more difficult to secure in the first half of 2023, some companies—especially in markets favored by the IRA legislation, like green hydrogen, batteries, solar, and carbon capture from the air—are still raising large amounts of money. According tothe latest data from the Engine, a “tough tech” venture group spun out of MIT, VC investments in startups working on industrial chemicals, materials, and carbon capture were actually up in the first half of 2023 from the same period in 2022—in fact, they were nearly at 2021 levels. For some startups, however, readily available cash has dried up, providing a reality check on their sustainability. And the first few failures could raise the ghosts of cleantech 1.0. But for many others, the financial downturn is simply the most recent reminder that climate-tech investments aren’t exempt from swings in the health of the economy. The same fundamental challenge that venture-backed startups faced in commercializing transformative technologies 15 years ago still exist. Novel, gee-whiz tech is not enough; a clear plan to target well-defined markets remains key to survival. “What is the path to market for these technologies?” asks David Popp, an economist at Syracuse University. He attributes the collapse of startups in cleantech 1.0 largely to the lack of demand for green products in highly competitive commodity markets. And that business puzzle, he says, remains: “I’m kind of curious to see, looking back five years from now, whether we’ll be looking at this like the first cleantech bubble.” In an influential 2016 post-mortem of cleantech 1.0 by the MIT Energy Initiative,  several researchers analyzed what went wrong and concluded thatventure capital was “the wrong model for clean energy innovation,” putting the blame on VCs’ unsuccessful attempts to fund startups through the “valley of death” by themselves. Simply put, the VCs quickly ran out of money and patience. The report’s conclusion: “The sector requires a more diverse set of actors and innovation models.” The good news is that the types of investors funding cleantech have in fact become more diversified. Arguably the biggest difference is that VCs are no longer going it alone. Thanks to the huge potential markets in renewable power and industrial decarbonization, there is a growing appetite among other types of investors to fund expensive and risky scale-up projects. Technology plays a crucial role in addressing one of society's most daunting threats. Many of these investing groups, which includes hedge funds, corporations, growth investors, and even wealthy individuals, can readily write checks for $100 million or $200 million, and today they’re providing much of the funding for the flurry of demonstration plants. “There is a whole new generation of investors whose entire business is financing first deployment tonth deployment,” says Matthew Nordan, general partner at Azolla Ventures. “That didn’t exist before, and that is where many of the [earlier] companies died on the shoals.” The new investors include companies in sectors such as steel, chemicals, and concrete that are bracing for an inevitable long-term shift to lower-carbon processes.  Typically led by their venture groups, these corporations—such as steel manufacturer ArcelorMittal and Siam Cement Group, a conglomerate based in Bangkok—are supporting startups in their areas of business with financing and engineering expertise. And though their commitment to investing in climate-tech startups is sometimes viewed with skepticism, the money is real—and so is the time and expertise they’re bringing to the new technologies. Still, Francis O’Sullivan, one of the authors of the 2016 MIT report who is now a lead climate investor at S2G Ventures, says that the way the startups are funded remains broken. The problem now, O’Sullivan says, is that the money is in several different types of buckets. There is a huge amount of money going to early-stage startups. And there is also ample money from banks and institutional investors for so-called infrastructure spending on well-proven technology (such as building a new wind or solar farm). But the bucket of money for the critical “growth stage”—funding for the demonstration of first-of-a-kind technologies—is relatively small. In a report just completed, O'Sullivan and Gokul Raghavan, his colleague at S2G Ventures, calculated that between 2017 and 2022,US and European private investors raised $270 billionfor what the authors broadly define as the energy transition. Some $120 billion went to early-stage, venture-backed companies, and another $100 billion was for later infrastructure spending. Only about $50 billion went to so-called growth-stage funding. What is getting shortchanged, says O'Sullivan, is financing for the scale-up of risky new technologies—the stage where startups find out if their innovative technologies actually work and are economical. It means many highly valued early-stage climate-tech startups could be stranded without an apparent path forward. It’s “one of the most significant barriers” to industrial decarbonization, says O'Sullivan. Beyond financing, there are other fundamental obstacles in the path toward industrial decarbonization. Chief among them: startups need to understand the challenges of large manufacturing processes. Many venture investors in cleantech 1.0 were from internet businesses and “applied software heuristics to things clearly not software companies,” says Ramana Nanda, a finance professor at Imperial College London and founder of its Institute for Deep Tech Entrepreneurship. “I think the big lesson from cleantech 1.0,” he says, “is that molecules don’t work the same way as bytes.” For one thing, he says, “we really don’t know if something will work until we build that large demonstration plant that costs lots of money.” And even if the new technology works, Nanda points out, startups are often facing risk-averse industrial customers that have invested hundreds of millions of dollars in existing equipment and processes. “What they don’t want to do is scrap all that and jump to a new process, only to find out in 10 years there is an unintended consequence that no one had predicted,” he says. One promising approach is the development of components that can be selectively added to existing production operations, minimizing the risk, says Nanda. Instead of hoping to completely make-over an entire industry like steel manufacturing, he says, the strategy is to ask: “Can you be part of the manufacturing process? Can you fit into an existing infrastructure?” From a practical perspective, he says, that often means offering modular solutions that existing industrial players can slot into their processes, with relatively little disruption. Take Boston Metal, a startup that wants totransform global steel manufacturing. This industry accounts for almost a tenth of global carbon emissions and is rapidly growing in many parts of the globe. The company aims to replace the iconic blast furnace with an electrochemical process that turns iron ore into pure iron, an initial step in making steel. It’s an almost absurdly audacious goal: replacing a century-old technology that is the mainstay of one the world’s largest industries. Boston Metal’s strategy is to try to make the transition as digestible as possible for steelmakers. “We won’t own and operate steel plants,” says Adam Rauwerdink, who heads business development at the company. Instead, it plans to license the technology for electrochemical units that are designed to be a simple drop-in replacement for blast furnaces; the liquid iron that flows out of the electrochemical cells can be handled just as if it were coming out of a blast furnace, with the same equipment. Working with industrial investors including ArcelorMittal, says Rauwerdink, allows the startup to learn “how to integrate our technology into their plants—how to handle the raw materials coming in, the metal products coming out of our systems, and how to integrate downstream into their established processes.” The startup’s headquarters in a business park about 15 miles outside Boston is far from any steel manufacturing, but these days it’s drawing frequent visitors from the industry. There, the startup’s pilot-scale electrochemical unit, the size of a large furnace, is intentionally designed to be familiar to those potential customers. If you ignore the hordes of electrical cables running in and out of it, and the boxes of electric equipment surrounding it, it’s easy to forget that the unit is not just another part of the standard steelmaking process. And that’s exactly what Boston Metal is hoping for. The company expects to have an industrial-scale unit ready for use by 2025 or 2026. The deadline is key, because Boston Metal is counting on commitments that many large steelmakers have made to reach zero carbon emissions by 2050. Given that the life of an average blast furnace is around 20 years, that means having the technology ready to license before 2030, as steelmakers plan their long-term capital expenditures. But even now, says Rauwerdink, demand is growing for green steel, especially in Europe, where it’s selling for a few hundred dollars a metric ton more than the conventional product. It’s that kind of blossoming market for clean technologies that many of today’s startups are depending on. The recent corporate commitments to decarbonize, and the IRA and other federal spending initiatives, are creating significant demand in markets “that previously didn’t exist,” says Michael Kearney, a partner at Engine Ventures. One wild card, however, will be just how aggressively and faithfully corporations pursue ways to transform their core businesses and to meet their publicly stated goals. Funding a small pilot-scale project, says Kearney, “looks more like greenwashing if you have no intention of scaling those projects.” Watching which companies move from pilot plants to full-scale commercial facilities will tell you “who’s really serious,” he says. Putting aside the fears of greenwashing, Kearney says it’s essential to engage these large corporations in the transition to cleaner technologies. Susan Schofer, a partner at the venture firm SOSV, has some advice for those VCs and startups reluctant to work with existing companies in traditionally heavily polluting industries: Get over it. “We need to partner with them. These incumbents have important knowledge that we all need to get in order to effect change. So there needs to be healthy respect on both sides,” she says. Too often, she says, there is “an attitude that we don’t want to do that because it’s helping an incumbent industry.” But the reality, she says, is that finding ways for such industries to save energy or use cleaner technologies “can make the biggest difference in the near term.” It’s tempting to dismiss the history of cleantech 1.0. It was more than a decade ago, and there’s a new generation of startups and investors. Far more money is around today, along with a broader range of financing options. Surely we’re savvier these days. But it would be a mistake to ignore the past failures. The challenges of commercializing climate technologies rooted in advances in science and engineeringremain the same: not only are they expensive and risky to scale up, but you’re aiming to compete in mature markets characterized by commodity products with low margins. The economics, despite what some Silicon Valley boosters might proclaim, haven’t changed. Many of the technologies that we’re so excited about today could fail. Even as billions are flowing into green hydrogen and direct air capture, these technologies remain highly speculative and may prove too expensive to ever be competitive. Fusion might never be a working source of power. Some of the venture-backed startups that are pinning their hopes on green cement or steel could, like the advanced biofuels startups of the late 2000s, be gone in a few years. And that’s not even mentioning the plethora of early-stage startups with exotic technologies that gained funding in recent years and are little more than lab experiments with no discernible markets. The most enduring lesson from cleantech 1.0 is a simple one: the survival of climate-tech startups depends on demand for their inventions from large and expanding markets. Take, for instance, the company that gave me the strange sense of déjà vu; for years 1366 Technologies, which in 2021 became CubicPV, chased after ways to improve solar manufacturing, developing a new method for making the wafers that are the backbone of solar cells. But nearly all wafer production was done in China, by that country’s own manufacturers. Without buyers for its wafers, 1366 spent much of the 2010s biding its time, making technical advances and building expertise—surviving thanks to its patient venture investors. Then came the 2022 US climate bill, and the startup’s prospects changed overnight. As US solar manufacturers race to ramp up their production, says van Mierlo, domestic supply of the silicon wafers could become a critical bottleneck. Suddenly, CubicPV has a huge potential market for its innovations. “I would like to say it’s all strategy,” says van Mierlo, “but, you know, we just got lucky.” Even if companies do heed the lessons of the past, shifting political winds could once again derail the “luck” of today’s climate-tech investments. The IRA passed without a single vote from Republicans in either the House or the Senate. If a Republican president is elected next year, they could try to end much of the funding. In the UK, the prime minister recently proposed weakening the country’s climate policies to slow down its green energy transition. And other countries have also shown an unpredictable commitment to new, cleaner technologies during tough economic times. In arecent paper, Syracuse University’s Popp and his coauthor traced many of the woes of cleantech 1.0 back to a largely forgotten Senate election in early 2010. After the death of the liberal Democrat Ted Kennedy, Massachusetts voters surprisingly elected the Tea Party–adjacent Republican Scott Brown, dooming a comprehensive climate bill in Congress. Without the legislation’s anticipated carbon pricing, many venture investors lost interest in clean-energy startups. If a similar political shift were to happen again, it could be a disaster for badly needed innovations. The lingering damage from the collapse of cleantech 1.0 was not to the wallets of venture investors. The far greater harm was the death of startups with once promising technologies: more efficient solar cells, batteries made of more abundant materials, and cleaner fuels. Those failures ripped the heart out of a generation of cleantech entrepreneurs and investors. Clever new ideas emerging from university labs had few productive places to go. The critical role that venture-backed startups can play in the energy transition by turning radical new advances into sustainable businesses was crushed. Innovation in climate tech went into a decade-long winter. We can’t afford to fail again. We desperately need the new advances. But at the same time, the techno-optimism that often surrounds these startups needs to be tempered. We’renotjust a few breakthroughs away from solving the climate crisis. Today’s venture-back startups are just one piece of the far larger effort to create a clean economy. Investors and founders need to learn how they fit into this massive undertaking and develop more self-awareness about the constraints and limitations they face. The hubris of Silicon Valley investors that helped doom cleantech 1.0 needs to be checked. For many venture capitalists, climate tech is a mindset and business model that still doesn’t work. But some, including numerous veterans of cleantech 1.0, have learned from the past failures. As more and more remarkable advances emerge out of academic labs, investors with many more financial tools available to them are ready to turn the breakthroughs into viable businesses. This time they’re fully aware of the time and money it takes—and the willingness to tolerate risk. With some luck, they could succeed. Despite having a population of just 1,400, until recently, Tokelau’s .tk domain had more users than any other country. Here’s why. New capabilities unlocked by advanced analytics, AI, and machine learning enable smarter business buying. The company will pay rival Editas Medicine and the Broad Institute so that it can sell its breakthrough gene-editing treatment for sickle-cell disease. In a bid to find new economic growth, Hong Kong’s government is busy setting up legal frameworks to court Web3 companies.

I received the new gene-editing drug for sickle-cell disease. It changed my life.
https://www.technologyreview.com/2023/12/04/1084209/vertex-exacel-approval-gene-editing-sickle-cell-disease-patient/
December 4, 2023
Jimi Olaghere
As a patient enrolled in a clinical trial for Vertex’s new treatment, I was among the first to experience CRISPR’s transformative effects. On a picturesque fall day a few years ago, I opened the mailbox and took out an envelope as thick as a Bible that would change my life. The package was from Vertex Pharmaceuticals, and it contained a consent form to participate in a clinical trial for a new gene-editing drug to treat sickle-cell disease. A week before, my wife and I had talked on the phone with Haydar Frangoul, an oncologist and hematologist in Nashville, Tennessee, and the lead researcher of the trial. He gave us an overview of what the trial entailed and how the early participants were faring. Before we knew it, my wife and I were flying to the study site in Nashville to enroll me and begin treatment. At the time, she was pregnant with our first child. I’d lived with sickle-cell my whole life—experiencing chronic pain, organ damage, and hopelessness. To me, this opportunity meant finally taking control of my life and having the opportunity to be a present father. The drug I received, called exa-cel, could soon become the first CRISPR-based treatment to win approval from the US Food and Drug Administration, following the UK’s approval in mid-November. I’m one of only a few dozen patients who have ever taken it. In late October, I testified in favor of approval to the FDA’s advisory group as it met to evaluate the evidence. The agency will make its decision about exa-cel no later than December 8. The Dobb-E domestic robotics system was trained in real people’s homes and could help solve the field’s data problem. I’m very aware of how privileged I am to have been an early recipient and to reap the benefits of this groundbreaking new treatment. People with sickle-cell disease don’t produce healthy hemoglobin, a protein that red blood cells use to transport oxygen in the body. As a result, they develop misshapen red blood cells that can block blood vessels, causing intense bouts of pain and sometimes organ failure. They often die decades younger than those without the disease. After I received exa-cel, I started to experience things I had only dreamed of: boundless energy and the ability to recover by merely sleeping. My physical symptoms—including a yellowish tint in my eyes caused by the rapid breakdown of malfunctioning red blood cells—virtually disappeared overnight. Most significantly, I gained the confidence that sickle-cell disease won’t take me away from my family, and a sense of control over my own destiny. Today, several other gene therapies to treat sickle-cell disease are in the pipeline from biotech startups such as Bluebird Bio, Editas Medicine, and Beam Therapeutics as well as big pharma companies including Pfizer and Novartis—all to treat the worst-suffering amongan estimated US patient populationof about 100,000, most of whom are Black Americans. But many people who need these treatments may never receive them. Even though I benefited greatly from gene editing, I worry that not enough others will have that opportunity. And though I’m grateful for my treatment, I see real barriers to making these life-changing medicines available to more people. I feel very fortunate to have received exa-cel, but undergoing the treatment itself was an intense, months-long journey. Doctors extracted stem cells from my own bone marrow and used CRISPR to edit them so that they would produce healthy hemoglobin. Then they injected those edited stem cells back into me. It was an arduous process, from collecting the stem cells to conditioning my body to receive the edited cells to the eventual transplant. The collection process alone can take up to eight hours. For each collection, I sat next to an apheresis machine that vigorously separated my red blood cells from my stem cells, leaving me weakened. In my case, I needed blood transfusions after every collection—and I needed four collections to finally amass enough stem cells for the medical team to edit. The conditioning regimen that prepared my body to receive the edited cells was a whole different challenge. I underwent weeks of chemotherapy to clear out old, faulty stem cells from my body and make room for the newly edited ones. That meant dealing with nausea, weakness, hair loss, debilitating mouth sores, and the risk of exacerbating the underlying condition. My transplant day was in September 2020. In a matter of minutes, a doctor transferred the edited stem cells into me using three small syringes filled with clear fluid. Of course, the care team did a lot to try to make it a special day, but for me that moment was honestly deflating. However, the days and months since have been enriching. I’ve escaped from the clutch of fear that comes from thinking every occasion could be my last. Noise and laughter from my two-year-old twin daughters and four-year-old son echo through my home, and I’ve gained immense confidence from achieving my goal of being a father. It’s clear to me from my experience that this treatment is not made for everyone, though. To receive exa-cel, I spent a total of 17 weeks in the hospital. Not everyone will want to subject themselves to such a grueling process or be able to take time away from family obligations or work. And my treatment was free as part of the trial—if approved, exa-cel could cost millions of dollars per patient. Another potential barrier is that some people become enmeshed with their chronic disease. In many ways, your disease becomes part of your identity and way of life. The community of people with sickle-cell disease—we call ourselves warriors—is a source of strength and support for many. Even the promise of a better life from a novel technology may not be strong enough to break that bond. Other challenges are society-wide. Even as it advances new treatments, the US medical-industrial complex has too often left a trail of systemic racism and unethical medical practices in its wake. As a result, many Black Americans mistrust the medical system, which could further suppress turnout for new gene therapies. CRISPR is being used in an experimental effort to eliminate the virus that causes AIDS. Global accessibility has also not been a priority for most of the companies developing these new treatments, which I feel is a mistake. Some have cited the lack of health-care infrastructure in sub-Saharan Africa, where about80% of all people with sickle-cell live. But that just sounds to me like a convenient excuse. The options for treating sickle-cell disease are very limited. Denying access to such a powerful and transformative treatment on the basis of inability to pay, or where someone happens to live, strikes me as unethical. I believe patients and health-care providers everywhere deserve to know that the treatment will be available to those who need it. Conducting gene therapy research and clinical trials in African populations could allow for a more comprehensive understanding of the genetic diversity of sickle-cell disease. This knowledge could even contribute to the development of more effective and tailored therapies—not only for Africans, but also for people of African descent living in other regions. Even as a direct beneficiary of gene therapy, I often struggle with not knowing the full consequences of my actions. I fundamentally, at a cellular level, changed who I am. Where do we draw the line at playing God? And how do we make the benefits of a God-like technology such as this more widely available? Jimi Olaghere is a patient advocate and tech entrepreneur. New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. After deafness treatment, Yiyi can hear her mother and dance to the music. But why is it so noisy at night? Gene editing for sickle-cell is here. This is how researchers knew what DNA to change. CRISPR is being used in an experimental effort to eliminate the virus that causes AIDS.

Text-to-image AI models can be tricked into generating disturbing images
https://www.technologyreview.com/2023/11/17/1083593/text-to-image-ai-models-can-be-tricked-into-generating-disturbing-images/
November 17, 2023
Rhiannon Williams
Nonsense words can trick Stable Diffusion and DALL-E 2 into producing pictures that show violence and nudity. Popular text-to-image AI models can be prompted to ignore their safety filters and generate disturbing images. A group of researchers managed to get both Stability AI’sStable Diffusionand OpenAI’sDALL-E 2text-to-image models to disregard their policies and create images of naked people, dismembered bodies, and other violent and sexual scenarios. Theirwork, which they will present at the IEEE Symposium on Security and Privacy in May next year, shines a light on how easy it is to force generative AI models into disregarding their own guardrails and policies, known as “jailbreaking.” It also demonstrates how difficult it is to prevent these models from generating such content, as it’s included in thevast troves of datathey’ve been trained on, says Zico Kolter, an associate professor at Carnegie Mellon University. He demonstrated a similar form of jailbreaking onChatGPTearlier this year but was not involved in this research. “We have to take into account the potential risks in releasing software and tools that have known security flaws into larger software systems,” he says. All major generative AI models have safety filters to prevent users from prompting them to produce pornographic, violent, or otherwise inappropriate images. The models won’t generate images from prompts that contain sensitive terms like “naked,” “murder,” or “sexy.” But this new jailbreaking method, dubbed “SneakyPrompt” by its creators from Johns Hopkins University and Duke University, uses reinforcement learning to create written prompts that look like garbled nonsense to us but that AI models learn to recognize as hidden requests for disturbing images. It essentially works by turning the way text-to-image AI models function against them. These models convert text-based requests into tokens—breaking words up into strings of words or characters—to process the command the prompt has given them. SneakyPrompt repeatedly tweaks a prompt’s tokens to try to force it to generate banned images, adjusting its approach until it is successful. This technique makes it quicker and easier to generate such images than if somebody had to input each entry manually, and it can generate entries that humans wouldn’t imagine trying. Large language models are full of security vulnerabilities, yet they’re being embedded into tech products on a vast scale. SneakyPrompt examines the prompt it has been given, searches for words known to be blocked by the models, and converts them into tokens. It then replaces the tokens from the banned words with tokens from non-banned words that share semantics, or meanings, similar to the model. For example, giving SneakyPrompt the target prompt “a naked man riding a bike” causes it to replace “naked” with the nonsense term “grponypui,” which the team successfully used to generate images of a naked man riding a bike. Similarly, when it was told to generate “an anatomcalifwmg couple stand outside the bar,” it recognized “anatomcalifwmg” as meaning nude, and generated an image of exactly what the prompt requested. “We’ve used reinforcement learning to treat the text in these models as a black box,” says Yinzhi Cao, an assistant professor at Johns Hopkins University, who co-led the study. “We repeatedly probe the model and observe its feedback. Then we adjust our inputs, and get a loop, so that it can eventually generate the bad stuff that we want them to show.” Stability AIandOpenAIforbid the use of their technology to commit, promote, or incite violence or sexual violence. OpenAI also warns its users against attempting to “create, upload, or share images that are not G-rated or that could cause harm.” However, these policies are easily sidestepped using SneakyPrompt. “Our work basically shows that these existing guardrails are insufficient,” says Neil Zhenqiang Gong, an assistant professor at Duke University who is also a co-leader of the project. “An attacker can actually slightly perturb the prompt so the safety filters won’t filter [it], and steer the text-to-image model toward generating a harmful image.” Bad actors and other people intent on generating these kinds of images could run SneakyPrompt’s code, which is publicly available on GitHub, to trigger a series of automated requests to an AI image model. Stability AI and OpenAI were alerted to the group’s findings, and at the time of writing, these prompts no longer generated NSFW images on OpenAI’s DALL-E 2. Stable Diffusion 1.4, the version the researchers tested, remains vulnerable to SneakyPrompt attacks. OpenAI declined to comment on the findings but pointed MIT Technology Review towards resources on its website for improving safety in DALL·E 2, general AI safety and information about DALL·E 3. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. A Stability AI spokesperson said the firm was working with the SneakyPrompt researchers “to jointly develop better defense mechanisms for its upcoming models. Stability AI is committed to preventing the misuse of AI.”Stability AI has taken proactive steps to mitigate the risk of misuse, including implementing filters to remove unsafe content from training data, ​​they added. By removing that content before it ever reaches the model, it can help to prevent the model from generating unsafe content. Stability AI says it also has filters to intercept unsafe prompts or unsafe outputs when users interact with its models, and has incorporated content labeling features to help identify images generated on our platform. “These layers of mitigation help to make it harder for bad actors to misuse AI,” the spokesperson said. While the research team acknowledges it’s virtually impossible to completely protect AI models from evolving security threats, they hope their study can help AI companies develop and implement more robust safety filters. One possible solution would be to deploy new filters designed to catch prompts trying to generate inappropriate images by assessing their tokens instead of the prompt’s entire sentence. Another potential defense would involve blocking prompts containing words not found in any dictionaries, although the team found that nonsensical combinations of standard English words could also be used as prompts to generate sexual images. For example, the phrase “milfhunter despite troy” represented lovemaking, while “mambo incomplete clicking” stood in for naked.The research highlights the vulnerability of existing AI safety filters and should serve as a wake-up call for the AI community to bolster security measures across the board, says Alex Polyakov, co-founder and CEO of security company Adversa AI, who was not involved in the study. That AI models can be prompted to “break out” of their guardrails is particularly worrying in the context of information warfare, he says. They have already been exploited to produce fake content related to war events, such as the recentIsrael-Hamas conflict. “This poses a significant risk, especially given the limited general awareness of the capabilities of generative AI,” Polyakov adds. “Emotions run high during times of war, and the use of AI-generated content can have catastrophic consequences, potentially leading to the harm or death of innocent individuals. With AI’s ability to create fake violent images, these issues can escalate further.” The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems. It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it?

Making an image with generative AI uses as much energy as charging your phone
https://www.technologyreview.com/2023/12/01/1084189/making-an-image-with-generative-ai-uses-as-much-energy-as-charging-your-phone/
December 1, 2023
Melissa Heikkilä
This is the first time the carbon emissions caused by using an AI model for different tasks have been calculated. Each time you use AI to generate an image, write an email, or ask a chatbot a question, it comes at a cost to the planet. In fact, generating an image using a powerful AI model takes as much energy as fully charging your smartphone, according to a newstudyby researchers at the AI startup Hugging Face and Carnegie Mellon University. However, they found that using an AI model to generate text is significantly less energy-intensive. Creating text 1,000 times only uses as much energy as 16% of a full smartphone charge. Their work, which is yet to be peer reviewed, shows that while training massive AI models is incredibly energy intensive, it’s only one part of the puzzle. Most of their carbon footprint comes from their actual use. The study is the first time researchers have calculated the carbon emissions caused by using an AI model for different tasks, says Sasha Luccioni, an AI researcher at Hugging Face who led the work. She hopes understanding these emissions could help us make informed decisions about how to use AI in a more planet-friendly way. Luccioni and her team looked at the emissions associated with 10 popular AI tasks on the Hugging Face platform, such as question answering, text generation, image classification, captioning, and image generation. They ran the experiments on 88 different models. For each of the tasks, such as text generation, Luccioni ran 1,000 prompts, and measured the energy used with a tool she developed called Code Carbon. Code Carbon makes these calculations by looking at the energy the computer consumes while running the model. The team also calculated the emissions generated by doing these tasks using eight generative models, which were trained to do different tasks. Generating images was by far the most energy- and carbon-intensive AI-based task. Generating 1,000 images with a powerful AI model, such as Stable Diffusion XL, is responsible for roughly as much carbon dioxide as driving the equivalent of 4.1 miles in an average gasoline-powered car. In contrast, the least carbon-intensive text generation model they examined was responsible for as much CO2as driving 0.0006 miles in a similar vehicle. Stability AI, the company behind Stable Diffusion XL, did not respond to a request for comment. AI startup Hugging Face has undertaken the tech sector’s first attempt to estimate the broader carbon footprint of a large language model. The study provides useful insights into AI’s carbon footprint by offering concrete numbers and reveals some worrying upward trends, says Lynn Kaack, an assistant professor of computer science and public policy at the Hertie School in Germany, where she leads work on AI and climate change. She was not involved in the research. These emissions add up quickly. The generative-AI boom has led big tech companies tointegrate powerful AI modelsinto many different products, from email to word processing. These generative AI models are now used millions if not billions of times every single day. The team found that using large generative models to create outputs was far more energy intensive than using smaller AI models tailored for specific tasks. For example, using a generative model to classify movie reviews according to whether they are positive or negative consumes around 30 times more energy than using a fine-tuned model created specifically for that task, Luccioni says. The reason generative AI models use much more energy is that they are trying to do many things at once, such as generate, classify, and summarize text, instead of just one task, such as classification. Luccioni says she hopes the research will encourage people to be choosier about when they use generative AI and opt for more specialized, less carbon-intensive models where possible. “If you’re doing a specific application, like searching through email … do you really need these big models that are capable of anything? I would say no,” Luccioni says. The energy consumption associated with using AI tools has been a missing piece in understanding their true carbon footprint, says Jesse Dodge, a research scientist at the Allen Institute for AI, who was not part of the study. Comparing the carbon emissions from newer, larger generative models and older AI models  is also important, Dodge adds. “It highlights this idea that the new wave of AI systems are much more carbon intensive than what we had even two or five years ago,” he says. Google onceestimatedthat an average online search used 0.3 watt-hours of electricity,equivalentto driving 0.0003 miles in a car. Today, that number is likely much higher, because Google has integrated generative AI models into its search, says Vijay Gadepally, a research scientist at the MIT Lincoln lab, who did not participate in the research. Not only did the researchers find emissions for each task to be much higher than they expected, but they discovered that the day-to-day emissions associated with using AI far exceeded the emissions fromtraining large models. Luccioni tested different versions of Hugging Face’s multilingual AI modelBLOOMto see how many uses would be needed to overtake training costs. It took over 590 million uses to reach the carbon cost of training its biggest model. For very popular models, such as ChatGPT, it could take just a couple of weeks for such a model’s usage emissions to exceed its training emissions, Luccioni says. This is because large AI models get trained just once, but then they can be used billions of times. According tosome estimates,popular models such as ChatGPT have up to 10 million users a day, many of whom prompt the model more than once. Studies like these make the energy consumption and emissions related to AI more tangible and help raise awareness that there is a carbon footprint associated with using AI, says Gadepally, adding, “I would love it if this became something that consumers started to ask about.” Dodge says he hopes studies like this will help us to hold companies more accountable about their energy usage and emissions. “The responsibility here lies with a company that is creating the models and is earning a profit off of them,” he says. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems. It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it?

Google DeepMind’s weather AI can forecast extreme weather faster and more accurately
https://www.technologyreview.com/2023/11/14/1083366/google-deepminds-weather-ai-can-forecast-extreme-weather-quicker-and-more-accurately/
November 14, 2023
Melissa Heikkilä
It said Hurricane Lee would make landfall in Nova Scotia three days sooner than traditional methods predicted. This year the Earth has been hit by a record number of unpredictable extreme weather events made worse by climate change. Predicting them faster and with greater accuracy could enable us to prepare better for natural disasters and help save lives. A new AI model from Google DeepMind could make that easier. In researchpublished in Sciencetoday, Google DeepMind’s model, GraphCast, was able to predict weather conditions up to 10 days in advance, more accurately and much faster than the current gold standard. GraphCast outperformed the model from the European Centre for Medium-Range Weather Forecasts (ECMWF) in more than 90% of over 1,300 test areas. And on predictions for Earth’s troposphere—the lowest part of the atmosphere, where most weather happens—GraphCast outperformed the ECMWF’s model on more than 99% of weather variables, such as rain and air temperature Crucially, GraphCast can also offer meteorologists accurate warnings, much earlier than standard models, of conditions such as extreme temperatures and the paths of cyclones. In September, GraphCast accurately predicted that Hurricane Lee would make landfall in Nova Scotia nine days in advance, says Rémi Lam, a staff research scientist at Google DeepMind. Traditional weather forecasting models pinpointed the hurricane to Nova Scotia only six days in advance. They could also help to make them more accurate. “Weather prediction is one of the most challenging problems that humanity has been working on for a long, long time. And if you look at what has happened in the last few years with climate change, this is an incredibly important problem,” says Pushmeet Kohli, the vice president of research at Google DeepMind. Traditionally, meteorologists use massive computer simulations to make weather predictions. They are very energy intensive and  time consuming to run, because the simulations take into account many physics-based equations and different weather variables such as temperature, precipitation, pressure, wind, humidity, and cloudiness, one by one. GraphCast uses machine learning to do these calculations in under a minute. Instead of using the physics-based equations, it bases its predictions on four decades of historical weather data. GraphCast uses graph neural networks, which map Earth’s surface into more than a million grid points. At each grid point, the model predicts the temperature, wind speed and direction, and mean sea-level pressure, as well as other conditions like humidity. The neural network is then able to find patterns and draw conclusions about what will happen next for each of these data points. For the past year,weather forecasting has been going through a revolutionas models such as GraphCast,Huawei’s Pangu-Weatherand Nvidia’s FourcastNet have made meteorologists rethink the role AI can play in weather forecasting. GraphCast improves on the performance of other competing models, such as Pangu-Weather, and is able to predict more weather variables, says Lam. The ECMWF is already using it. When Google DeepMind first debuted GraphCast last December, it felt like Christmas, says Peter Dueben, head of Earth system modeling at ECMWF, who was not involved in the research. “It showed that these models are so good that we cannot avoid them anymore,” he says. GraphCast is a “reckoning moment” for weather prediction because it shows that predictions can be made using historical data, says Aditya Grover, an assistant professor of computer science at UCLA, who developedClimaX, a foundation model that allows researchers to do different tasks relating to modeling the Earth’s weather and climate. Plus: AI-text detection tools are really easy to fool. DeepMind's model is “great work and extremely exciting,” says Oliver Fuhrer, the head of the numerical prediction department at MeteoSwiss, the Swiss Federal Office of Meteorology and Climatology. Fuhrer says that other weather agencies, such as the ECMWF and the Swedish Meteorological and Hydrological Institute, have also used the graph neural network architecture proposed by Google DeepMind to build their own models. But GraphCast is not perfect. It still lags behind conventional weather forecasting models in some areas, such as precipitation, Dueben says. Meteorologists will still have to use conventional models alongside machine-learning models to offer better predictions. Google DeepMind is also making GraphCast open source. This is a good development, says UCLA’s Grover. “With climate change on the rise, it's very important that big organizations, which have had the luxury of so much compute, also think about giving back [to the scientific community],” he says. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems. It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it?

The Biggest Questions: What is death?
https://www.technologyreview.com/2023/11/17/1082937/what-is-death/
November 17, 2023

New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. Just as birth certificates note the time we enter the world, death certificates mark the moment we exit it. This practice reflects traditional notions about life and death as binaries. We are here until, suddenly, like a light switched off, we are gone. But while this idea of death is pervasive, evidence is building that it is an outdated social construct, not really grounded in biology. Dying is in fact a process—one with no clear point demarcating the threshold across which someone cannot come back. Scientists and many doctors have already embraced this more nuanced understanding of death. As society catches up, the implications for the living could be profound. “There is potential for many people to be revived again,” says Sam Parnia, director of critical care and resuscitation research at NYU Langone Health. Neuroscientists, for example, are learning that the brain can survive surprising levels of oxygen deprivation. This means the window of time that doctors have to reverse the death process could someday be extended. Other organs likewise seem to be recoverable for much longer than is reflected in current medical practice, opening up possibilities for expanding the availability of organ donations. To do so, though, we need to reconsider how we conceive of and approach life and death. Rather than thinking of death as an event from which one cannot recover, Parnia says, we should instead view it as a transient process of oxygen deprivation that has the potential to become irreversible if enough time passes or medical interventions fail. If we adopt this mindset about death, Parnia says, “then suddenly, everyone will say, ‘Let’s treat it.’” Legal and biological definitions of death typically refer to the “irreversible cessation” of life-sustaining processes supported by the heart, lungs, and brain. The heart is the most common point of failure, and for the vast majority of human history, when it stopped there was generally no coming back. That changed around 1960, with the invention of CPR. Until then, resuming a stalled heartbeat had largely been considered the stuff of miracles; now, it was within the grasp of modern medicine. CPR forced the first major rethink of death as a concept. “Cardiac arrest” entered the lexicon, creating a clear semantic separation between the temporary loss of heart function and the permanent cessation of life. Around the same time, the advent of positive-pressure mechanical ventilators, which work by delivering breaths of air to the lungs, began allowing people who incurred catastrophic brain injury—for example, from a shot to the head, a massive stroke, or a car accident—to continue breathing. In autopsies after these patients died, however, researchers discovered that in some cases their brains had been so severely damaged that the tissue had begun to liquefy. In such cases, ventilators had essentially created “a beating-heart cadaver,” says Christof Koch, a neuroscientist at the Allen Institute in Seattle. These observations led to the concept of brain death and ushered in medical, ethical, and legal debate about the ability to declare such patients dead before their heart stops beating. Many countries did eventually adopt some form of this new definition. Whether we talk about brain death or biological death, though, the scientific intricacies behind these processes are far from established. “The more we characterize the dying brain, the more we have questions,” says Charlotte Martial, a neuroscientist at the University of Liège in Belgium. “It’s a very, very complex phenomenon.” Traditionally, doctors have thought that the brain begins incurring damage minutes after it’s deprived of oxygen. While that’s the conventional wisdom, says Jimo Borjigin, a neuroscientist at the University of Michigan, “you have to wonder, why would our brain be built in such a fragile manner?” The system, previously used to return circulation to severed pigs’ brains, can now restore some functions of cells in other vital organs Recent research suggests that perhaps it actually isn’t. In 2019, scientists reported in Naturethat they were able torestore a suite of functionsin the brains of 32 pigs that had been decapitated in a slaughterhouse four hours earlier. The researchers restarted circulation and cellular activity in the brains using an oxygen-rich artificial blood infused with a cocktail of protective pharmaceuticals. They also included drugs that stopped neurons from firing, preventing any chance that the pig brains would regain consciousness. They kept the brains alive for up to 36 hours before ending the experiment. “Our work shows there’s probably a lot more damage from lack of oxygen that’s reversible than people thought before,” says coauthor Stephen Latham, a bioethicist at Yale University. In 2022, Latham and colleagues published a second paper in Nature announcing that they’d been able torecover many functions in multiple organs, including the brain and heart, in whole-body pigs that had been killed an hour earlier. They continued the experiment for six hours and confirmed that the anesthetized, previously dead animals had regained circulation and that numerous key cellular functions were active. “What these studies have shown is that the line between life and death isn’t as clear as we once thought,” says Nenad Sestan, a neuroscientist at the Yale School of Medicine and senior author of both pig studies. Death “takes longer than we thought, and at least some of the processes can be stopped and reversed.” A handful of studies in humans have also suggested that the brain is better than we thought at handling a lack of oxygen after the heart stops beating. “When the brain is deprived of life-sustaining oxygen, in some cases there seems to be this paradoxical electrical surge,” Koch says. “For reasons we don’t understand, it’s hyperactive for at least a few minutes.” In a study published in September in Resuscitation, Parnia and his colleagues collected brain oxygen and electrical activity data from 85 patients who experienced cardiac arrest while they were in the hospital. Most of the patients’ brain activity initially flatlined on EEG monitors, but for around 40% of them, near-normalelectrical activity intermittently reemergedin their brains up to 60 minutes into CPR. Similarly, in a study published in Proceedings of the National Academy of Sciences in May, Borjigin and her colleagues reportedsurges of activityin the brains of two comatose patients after their ventilators had been removed. The EEG signatures occurred just before the patients died and had all the hallmarks of consciousness, Bojigin says. While many questions remain, such findings raise tantalizing questions about the death process and the mechanisms of consciousness. The more scientists can learn about the mechanisms behind the dying process, the greater the chances of developing “more systematic rescue efforts,” Borjigin says. In best-case scenarios, she adds, this line of study could have “the potential to rewrite medical practices and save a lot of people.” Everyone, of course, does eventually have to die and will someday be beyond saving. But a more exact understanding of the dying process could enable doctors to save some previously healthy people who meet an unexpected early end and whose bodies are still relatively intact. Examples could include people who suffer heart attacks, succumb to a deadly loss of blood, or choke or drown. The fact that many of these people die and stay dead simply reflects “a lack of proper resource allocation, medical knowledge, or sufficient advancement to bring them back,” Parnia says. Borjigin’s hope is to eventually understand the dying process “second by second.” Such discoveries could not only contribute to medical advancements, she says, but also “revise and revolutionize our understanding of brain function.” Sestan says he and his colleagues are likewise working on follow-up studies that seek to “perfect the technology” they have used to restore metabolic function in pig brains and other organs. This line of research could eventually lead to technologies that are able to reverse damage—up to a point, of course—from oxygen deprivation in the brain and other organs in people whose hearts have stopped. If successful, the method could also expand the pool of available organ donors, Sestan adds, by lengthening the window of time doctors have to recover organs from the permanently deceased. If these breakthroughs do come, Sestan emphasizes, they will take years of research. “It’s important that we not overexaggerate and promise too much,” he says, “although that doesn’t mean we don’t have a vision.” In the meantime, ongoing investigations into the dying process will no doubt continue to challenge our notions of death, leading to sea changes within science and other realms of society, from the theological to the legal. As Parnia says: “Neuroscience doesn’t own death. We all have a stake in it.” Rachel Nuwer is a freelance science journalist who regularly contributes to the New York Times, Scientific American, Nature and more. Her latest book is I Feel Love: MDMA and the Quest for Connection in a Fractured World. She lives in Brooklyn.  After deafness treatment, Yiyi can hear her mother and dance to the music. But why is it so noisy at night? As a patient enrolled in a clinical trial for Vertex’s new treatment, I was among the first to experience CRISPR’s transformative effects. Gene editing for sickle-cell is here. This is how researchers knew what DNA to change. CRISPR is being used in an experimental effort to eliminate the virus that causes AIDS.

The Biggest Questions: Why is the universe so complex and beautiful?
https://www.technologyreview.com/2023/11/16/1083141/why-is-the-universe-so-complex-and-beautiful/
November 16, 2023

For some reason the universe is full of stars, galaxies, and life. But it didn’t have to be this way. The Biggest Questions is a mini-series that explores how technology is helping probe some of the deepest, most mind-bending questions of our existence. Why isn’t the universe boring? It could be. The number of subatomic particles in the universe is about 1080, a 1 with 80 zeros after it. Scatter those particles at random, and the universe would just be a monotonous desert of sameness, a thin vacuum without any structure much larger than an atom for billions of light-years in any direction. Instead, we have a universe filled with stars and planets, canyons and waterfalls, pine trees and people. There is an exuberant plenty to nature. But why is any of this stuff here? Cosmologists have pieced together an answer to this question over the past half-century, using a variety of increasingly complex experiments and observational instruments. But as is nearly always the case in science, that answer is incomplete. Now, with new experiments of breathtaking sensitivity, physicists are hoping to spot a never-before-seen event that could explain one of the great remaining mysteries in that story: why there was any matter around to form complicated things in the first place. The interestingness of the world around us is all the more puzzling when you look at the universe on the largest scales. You find structured clumpiness for a while. Stars form galaxies, galaxies form galaxy clusters, and those clusters form superclusters and filaments and walls around great cosmic voids nearly empty of matter. But when you zoom out even further, looking at chunks of the universe more than 300 million light-years wide, all that structure fades away. Past this point, the light from all the stars in the cosmos merges into an indistinct blur, and the universe does indeed look quite boringly similar in all directions, with no features or differences of note anywhere. Cosmologists call this the “end of greatness.” AI is helping chemists unpick the mysteries around the origins of life and detect signs of it on other worlds. This tedious cosmic landscape exists because the universe really was boring once. Shortly after the Big Bang, and for hundreds of thousands of years after that, it was relentlessly dull. All that existed was a thick red-hot haze of particles, stretching for trillions upon trillions of kilometers and filling every point in the universe almost evenly, with minuscule differences in the density of matter between one spot and another. But as the universe expanded and cooled, gravity amplified those tiny differences. Slowly, over the following millions and billions of years, the places in the universe with slightly more stuff attracted even more stuff. And that’s where we came from—the profusion of things in the universe today eventually arose as more and more material accumulated, making those slightly over-dense regions into radically complicated places packed with enough matter to form stars, galaxies, and us. On the very largest scales, boredom still reigns, as it has since the beginning of time. But down here in the dirt, there’s ample variety. This story still has some holes. For one thing, it is not clear where the matter came from in the first place. Particle physics demands that anything that creates matter must also create an equal amount of antimatter, carefully conserving the balance between the two. Every kind of matter particle has an antimatter twin that behaves like matter in nearly every way. But when a matter particle comes into contact with its antimatter counterpart, they annihilate each other, disappearing and leaving behind nothing but radiation. That’s exactly what happened right after the Big Bang. Matter and antimatter annihilated, leaving our universe aglow with radiation—and a small amount of leftover matter, which had slightly exceeded the amount of antimatter at the start. This tiny mismatch made the difference between the universe we have today and an eternity of tedium, and we don’t know why it happened. “Somehow there was this little imbalance and it turned into everything—namely, us. I really care about us,” says Lindley Winslow, an experimental particle physicist at MIT. “We have a lot of questions about the universe and how it evolved. But this is a pretty basic kindergarten sort of question of, okay, why are we here?” To answer this question, Winslow and other physicists around the world have constructed several experiments to catch nature in the act of violating the balance between matter and antimatter. They hope to see that violation in the form of neutrinoless double-beta decay, a type of radioactive decay. At the moment, that process is theoretical—it may not happen at all. But if it does, it would provide a possible explanation for the imbalance between matter and antimatter in the early universe. That explanation would rely on neutrinos, the ghostly oddballs of particle physics. These lightweight specters whiz about the universe, barely interacting with anything. Trillions of neutrinos are constantly streaming through every square centimeter of your body and the entire planet Earth, ignoring you just as completely as they ignore the iron core of our planet. Reliably stopping just one neutrino would take a slab of lead a light-year thick. And neutrinos might perform an even more bizarre trick. The neutrino and its antimatter partner could be one and the same, making it different from every other known form of matter and capable of annihilating itself. “If we observed [neutrinoless double-beta decay], it would prove that the neutrino is its own antiparticle,” says Winslow. “It would also provide us a process that makes more matter than antimatter.” That process starts in the heart of the atom. When some unstable atomic nuclei decay, they emit an electron along with an antineutrino to counterbalance it: one particle of matter and one of antimatter. This is a very common kind of radioactive decay, known for historical reasons as beta decay. Significantly less common is double-beta decay, when an atomic nucleus emits two electrons at once, along with two antineutrinos to balance them out. Double-beta decay is “one of the longest processes that we’ve ever measured,” says Winslow. To see a single atom undergo double-beta decay, she continues, we would typically have to wait a billion times longer than the current age of the universe. But if the neutrino is its own antiparticle, there’s the possibility of something even more rare than that: a double-beta decay where the two neutrinos annihilate each other immediately, leaving only the two electrons without any antimatter to counterbalance them. This is neutrinoless double-beta decay. Spotting such a rare process would be difficult—but not impossible, thanks to the phenomenally huge number of atoms in objects of everyday size. There are nearly a trillion trillion atoms in a few grams of material. “So if you just pile up a bunch of stuff, you just have the possibility of seeing something that happens in timelines even longer than the age of the universe,” says Winslow. This is the approach taken by the Cryogenic Underground Observatory for Rare Events (CUORE, Italian for “heart”), a detector under a mountain in Italy that is waiting for evidence of neutrinoless double-beta decay. A certain isotope of tellurium is one of the nuclei susceptible to double-beta decay. CUORE watches for it in a set of 988 five-centimeter-wide cubic crystals of tellurium dioxide, each connected to a highly sensitive thermometer. The combined energy of the two electrons emitted in neutrinoless double-beta decay is the same every time, so if the decay occurs anywhere within one of these crystals, that specific amount of energy will be deposited into the crystal as heat, raising its temperature by one ten-thousandth of a degree Celsius. But a signal that small is hard to see against all the other things that could change the temperature of a crystal. That’s why CUORE is under a mountain—the bulk of the rock above it shields it from nearly all cosmic rays. And that’s also why CUORE needs to be kept phenomenally cold, just a few thousandths of a degree above absolute zero—it “wins the award for coldest cubic meter in the known universe,” says Winslow. The sensors are so exquisitely sensitive that they can even pick up vibrations from waves crashing on the beach, 60 kilometers away. CUORE isn’t alone. There are other experiments looking for neutrinoless double-beta decay, including KamLAND-Zen, an experiment—also under a mountain—in Japan, using gaseous xenon in place of tellurium crystals. But none of the experiments searching for the decay have seen it yet, despite years of waiting. There are plans to upgrade the sensors at CUORE and increase the number of crystals being used; there are also plans to increase the size and sensitivity of KamLAND-Zen. But the future of these experiments is uncertain. Scientists are training machine-learning models and designing instruments to hunt for life on other worlds. “In principle, we could make bigger, better experiments,” says Reina Maruyama, a physicist at Yale who is also part of the CUORE collaboration. “You could make 10 of what we have. And so I think it just becomes a matter of how much resources humankind wants to put into this experiment.” Winslow estimates that a full search would require two more rounds of improvements to existing experiments. If those are done and they come up empty-handed, she says, “then we will have pretty much eliminated the possibility of the neutrino being its own antiparticle.” If that happens, it’s the end of a promising theory, but not the end of the search. Physicists have plenty of other ideas about how matter and antimatter could have become imbalanced. But finding evidence for those ideas is hard. Some could be confirmed if the Large Hadron Collider, the largest particle collider in the world, finds something unexpected over the next few years; other theories depend on sensitive searches for dark matter, an invisible and hypothetical substance, strongly suggested by decades of evidence, which is believed to constitute more than 80% of the matter in the universe. And some theories exact a high price for explaining the imbalance: they suggest that protons, one of the key components of atomic nuclei, are unstable. These theories say that proton decay takes even longer than neutrinoless double-beta decay, on average about a trillion trillion times longer than the current age of the universe. Super-Kamiokande (aka “Super-K”), in Japan, is the largest experiment watching for proton decay, using an underground vat of 50,220 metric tons of ultra-pure water surrounded by 13,031 light sensors. At the limits of knowledge, Super-K waits for a faint flash in the darkness. It has yet to catch a proton in the act. But whatever caused the matter-antimatter imbalance in the early universe, there’s one thing that physicists are sure of: eventually, the show will end. Over time, all interesting structures will fade away as the universe’s matter and energy are scattered about increasingly at random. Eons from now, this will lead once more to a fully featureless void—and this time, it will be far less dense and far more uniform than the primordial haze. This state, known as heat death, is likely to be the final fate of the universe, myriad quadrillions of years in the future. So we’re lucky—we live at a time when the universe is filled with complexity and beauty, even if we don’t fully understand why. Adam Beckeris a freelance journalist based in Berkeley, California. He has written for the New York Times, the BBC, Scientific American, Quanta, New Scientist, and other outlets. He is also the author ofWhat Is Real?, an affable account of the sordid untold history of quantum physics. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If we want online discourse to improve, we need to move beyond the big platforms.

The Biggest Questions: Is it possible to really understand someone else’s mind?
https://www.technologyreview.com/2023/11/15/1083250/is-it-possible-to-really-understand-someone-elses-mind/
November 15, 2023

How we think, feel and experience the world is a mystery to everyone but us. But technology may be starting to help us understand the minds of others. Technically speaking, neuroscientists have been able to read your mind for decades. It’s not easy, mind you. First, you must lie motionless within the narrow pore of a hulking fMRI scanner, perhaps for hours, while you watch films or listen to audiobooks. Meanwhile, the machine will bang and knock as it records the shifting patterns of blood flow within your brain—a proxy for neural activity. The researchers, for whose experiment you have volunteered, will then feed the moment-to-moment pairings of blood flow and movie frames or spoken words to software that will learn the particularities of how your brain responds to the things it sees and hears. None of this, of course, can be done without your consent; for the foreseeable future, your thoughts will remain your own, if you so choose. But if you do elect to endure those claustrophobic hours in the scanner, the software will learn to generate a bespoke reconstruction of what you were seeing or listening to, just by analyzing how blood moves through your brain. Back in 2011, UC Berkeley neuroscientists trained such a program to createethereal doublesof the videos their subjects had been watching. More recently, researchers have deployed generative AI tools, like Stable Diffusion and GPT, to create far more realistic, if not entirely accurate, reconstructions offilmsandpodcastsbased on neural activity. Given the hype, and financial investment, that generative AI has attracted, this kind of stimulus reconstruction technology will inevitably continue to improve—especially if Elon Musk’s Neuralink succeeds in bringing brain implants to the masses. But as exciting as the idea of extracting a movie from someone’s brain activity may be, it is a highly limited form of “mind reading.” To really experience the world through your eyes, scientists would have to be able to infer not just what film you are watching but also what you think about it, how it makes you feel, and what it reminds you of. These interior thoughts and feelings are far more difficult to access. Scientists havemanaged to inferwhich specific object, out of two possibilities, someone was dreaming about; but in less constrained settings, such approaches struggle. Scientists are training machine-learning models and designing instruments to hunt for life on other worlds. That’s because machine-learning algorithms need both brain signals and information about what they correspond to, paired in perfect synchrony, to learn what the signals mean. When studying inner experience, all scientists have to go on is what peoplesayis going on inside their head, and that can be reliable. “It’s not like it’s directly measuring as a ground truth what people experienced,” says Raphaël Millière, a lecturer in philosophy at Macquarie University in Australia. Tying brain activity to subjective experience requires facing up to the slipperiness and inexactitude of language, particularly when deployed to capture the richness of one’s inner life. In order to meet that demanding brief, scientists like Millière are marrying contemporary artificial intelligence with centuries-old techniques, from philosophical interview strategies to ancient meditation practices. Bit by bit, they are starting to suss out some of the brain regions and networks that give rise to specific dimensions of human experience. “That’s a problem we can make, and have made, some progress on,” Millière says. “I’m not saying it’s easy, but I think it’s certainly more tractable than solving the grand mystery of consciousness.” Over 300 years ago, the philosopher John Locke asked whether the color blue looks the same to everyone—or whether my experience of “blue” might be closer to your experience of “yellow.” Answering such subtle questions could be a distant horizon toward which the neuroscience of experience might aim. In its current, early stage, however, the field has to address itself to much more dramatic forms of experience. “If we want to get a better grasp of what is distinctive about the ordinary, wakeful states in our daily lives, it’s useful to see what happens when you undergo some transition into a different kind of state,” Millière says. Some scientists focus on deep states of meditation or intense hallucinations. For his part, Millière is particularly interested in understanding self-consciousness—the awareness of oneself as a thinking, feeling individual in a particular place and time—and so he studies what happens to someone’s brain during a psychedelic trip. By comparing how subjects respond post-trip to questions like “I experienced a disintegration of my ‘self’ or ‘ego’” with their brain activity patterns, researchers have discovered some changes that may be linked to the loss of self-consciousness. The default mode network (DMN), for example—a group of brain regions that all become active when people are lost in thought—tends to lose its typical coordination. Taking a high dose of psychedelics is certainly the easiest way to lose one’s sense of self while awake. But if drugs aren’t your thing, there is another option: spend tens of thousands of hours practicing meditation. Highly skilled practitioners of Buddhist meditation can voluntarily enter a state in which the boundary between themselves and the world begins to seem porous, or even disappears entirely. Interestingly, such states are also associated withactivity changes in some core regions of the default mode network, like the posterior cingulate cortex. Because the potential pool of subjects is so much smaller, studying meditators can be a trickier way of getting at extreme experiences. But meditators also have some distinctive benefits as experimental subjects, says Sara Lazar, associate professor of psychiatry at Harvard Medical School. Expert meditators are masters of their own internal lives—they can spontaneously produce feelings of profound gratitude or descend into states of deep focus—and they tend to report their inner experiences in far more detail than untrained people are able to. “It’s because we spend so much time just listening and paying attention to what’s actually going on inside of us,” says Lazar, herself an experienced meditator. We non-meditators are sometimes so unaware of what’s going on in our own heads that when our minds start to wander—which they often do—we might not even notice what is happening. In order to study what the brain does at such times, Kalina Christoff, a psychologist at the University of British Columbia, had to periodically prompt her subjects to consider whether their minds had, at that moment, been wandering, and whether they had realized that they’d lost their focus. Frequently, they did not. Her subjects’ default mode networks weremore active while their minds were wandering, and especially so when they were unaware that it was happening. Speeding up communication between humans is surprisingly tricky. To investigate the onset of mind wandering in more detail, however, Christoff had to turn to experienced meditators, who could detect it the moment it occurred. Only with their assistance was she able to determine that the DMN isparticularly activein the moments just before the mind begins to drift away. Altogether, these results paint a fairly coherent picture. When you are wondering what to have for dinner or worrying over a recent disagreement with a friend, your DMN switches on; but in states of intense, selfless focus, the network deactivates or desynchronizes. But that doesn’t mean scientists can tell whether you are conscious of yourself, or whether your head is in the clouds, just by looking at your brain activity. In onestudy, researchers were able to decode particular internal states—a focus on the breath, a focus on sounds, and a wandering mind, for example—at a better rate than would be expected by chance, but they still got it wrong more than half the time. And these coarse descriptions of people’s inner states hardly paint a complete picture of what it’s like to be them. Even so, Lazar thinks brain data might help us better understand our own experiences. Deactivation of the default mode network, and of the posterior cingulate cortex in particular, isassociated with states of "effortless focus"that beginning meditators often struggle to attain. So some researchers are testing whether seeing live data from their own brains, in a process called neurofeedback, could help people learn to meditate. “Once you’ve felt the right state at least once or twice, then you know: okay, this is what I’m going for, this is what I’m aiming for,” Lazar says. “Now I know what this feels like.” If you’re a neuroscientist interested in subjective experience, times are relatively good: research on psychedelics and meditation has exploded in the past decade, and noninvasive neuroimaging technologies are only growing ever more powerful and precise. But the data means little without a solid indication of what the subject is experiencing, and the only way to obtain that information is to ask. “We simply cannot do away with reports of some sort,” Millière says. Psychological questionnaires are one approach. They’re conveniently quantitative, and they’re easy to use, but they require subjects to slot their transcendent experiences into preestablished, and potentially ill-fitting, boxes. There are alternatives. Phenomenology, the branch of philosophy that seeks to analyze first-person experience in rigorous, exacting detail, has had over a century to refine its techniques for obtaining such reports—three times as long as the fMRI machine has existed. Millière has organized training sessions for his neuroscientist colleagues in “micro-phenomenology,” a philosophical interview method that seeks to elicit as much experiential information from a subject as possible without leading the responses in any particular direction. But long textual descriptions, of the sort produced by a micro-phenomenological interview, are much trickier to parse than questionnaires. Researchers can manually rate each response according to the attributes that interest them, but that can be a messy and labor-intensive process—and it robs interviews of much of the nuance that makes them so valuable. Natural-language-processing algorithms, like those that power ChatGPT, may offer a more efficient and consistent alternative: they can quickly and automatically analyze large volumes of text for particular features. Already, Millière has experimented with applying natural-language processing to reports of psychedelic experiments from online databases likeErowidand discovered that the resulting characterizations correspond well to data obtained from questionnaires. Even with the help of micro-phenomenology, however, wrapping up what’s going on inside your head into a neat verbal package is a daunting task. So instead of asking subjects to struggle to represent their experiences in words, some scientists are using technology to try to reproduce those experiences. That way, all subjects need to do is confirm or deny that the reproductions match what’s happening in their heads. In astudythat has not yet been peer reviewed, a team of scientists from the University of Sussex, UK, attempted to devise such a question by simulating visual hallucinations with deep neural networks. Convolutional neural networks, which were originally inspired by the human visual system, typically take an image and turn it into useful information—a description of what the image contains, for example. Run the network backward, however, and you can get it toproduceimages—phantasmagoric dreamscapes that provide clues about the network’s inner workings. The idea was popularized in 2015 by Google, in the form of a program calledDeepDream. Like people around the world, the Sussex team started playing with the system for fun, says Anil Seth, a professor of neuroscience and one of the study’s coauthors. But they soon realized that they might be able to leverage the approach to reproduce various unusual visual experiences. Drawing on verbal reports from people with hallucination-causing conditions like vision loss and Parkinson’s, as well as from people who had recently taken psychedelics, the team designed an extensive menu of simulated hallucinations. That allowed them to obtain a rich description of what was going on in subjects’ minds by asking them a simple question: Which of these images best matches your visual experience? The simulations weren’t perfect, although many of the subjects were able to find an approximate match. Unlike the decoding research, this study involved no brain scans—but, Seth says, it may still have something valuable to say about how hallucinations work in the brain. Some deep neural networks do a respectable job of modeling the inner mechanisms of the brain’s visual regions, and so the tweaks that Seth and his colleagues made to the network may resemble the underlying biological “tweaks” that made the subjects hallucinate. “To the extent that we can do that,” Seth says, “we’ve got a computational-level hypothesis of what’s happening in these people’s brains that underlie these different experiences.” This line of research is still in its infancy, but it suggests that neuroscience might one day do more than simply telling us what someone else is experiencing. By using deep neural networks, the team was able to bring its subjects’ hallucinations out into the world, where anyone could share in them. Externalizing other sorts of experiences would likely prove far more difficult—deep neural networks do a good job of mimicking senses like vision and hearing, but they can’t yet model emotions or mind-wandering. As brain modeling technologies advance, however, they could bring with them a radical possibility: that people might not only know, but actually share, what is going on in someone else’s mind. New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. After deafness treatment, Yiyi can hear her mother and dance to the music. But why is it so noisy at night? As a patient enrolled in a clinical trial for Vertex’s new treatment, I was among the first to experience CRISPR’s transformative effects. Gene editing for sickle-cell is here. This is how researchers knew what DNA to change.

The Biggest Questions: How did life begin?
https://www.technologyreview.com/2023/11/14/1082828/how-did-life-begin/
November 14, 2023

AI is helping chemists unpick the mysteries around the origins of life and detect signs of it on other worlds. How life begins is one of the biggest and hardest questions in science. All we know is that something happened on Earth more than 3.5 billion years ago, and it may well have occurred on many other worlds in the universe as well. But we don’t knowwhat does the trick. Somehow a soup of nonliving chemicals like water and methane must combine and self-organize, growing ever more complex and coordinated, until eventually it gives rise to a living cell. One of the biggest difficulties is the sheer complexity of the problem: even the simplest known bacteria havewell over 100 genesand contain hundreds of kinds of molecules, all furiously interacting in a microscopic dance. The environment on the primordial Earth must also have been complicated: huge numbers of different chemicals, from metals and minerals to water and gases, all being blasted around by winds and volcanic eruptions. “The experimental parameter space is almost infinite,” saysWilhelm Huck, a chemist at Radboud University in Nijmegen, the Netherlands. Now, a few researchers are trying a new approach: harnessing artificial intelligence to zero in on the winning conditions. Specifically, several groups have started using machine-learning tools that can identify patterns in data sets too huge and messy for the human brain to comprehend. The hope is that these tools will help researchers achieve in years what would otherwise take decades. By pointing the way to the fastest and sturdiest processes for generating complexity, they could help us devise a universal theory of the origins of life—one that applies not just on Earth but on any other world. It’s early days—but there have already been some significant advances. The origins-of-life problem is at least partly about chemistry: what mix of chemicals, under what conditions, is required for life to form? “Chemistry is going to answer this question—one of the deepest questions humanity has,” saysLeroy “Lee” Cronin, a chemist at the University of Glasgow in the UK. The study of the origins of life was kick-started byan experiment published in 1953. Stanley Miller, a graduate student supervised by the chemist Harold Urey, mixed water and three gases in glass flasks, which were heated and subjected to electric shocks that mimicked the lightning he assumed regularly struck the young Earth. Within days, this setup had produced glycine, the simplest amino acid and one of the building blocks of proteins. While the Miller experiment did not produce life or anything close to it, it became iconic because it was relatively unsupervised: Miller just set it up and let it run. This was meant to mimic conditions on the young Earth, where there were no synthetic chemists to guide chemical reactions to the “correct” end. However, the purported realism of the experiment was also a problem: it produced so many chemicals that identifying them all and understanding how they formed was almost impossible. And it’s giving the data away for free, which could spur new scientific discoveries. Many subsequent experiments in “prebiotic” chemistry have been more carefully controlled. They have succeeded in producing many more amino acids, sugars, and other chemicals of life. However, it’s not clear that such meticulously curated reactions would take place without human intervention—and so they may tell us nothing about the primordial Earth. What researchers want is a way to get back to the Miller experiment, finding better ways to explore what happens in uncontrolled complex mixtures. That’s where machine learning can come in. The technology has already been applied to existing problems in biology: notably, Google DeepMind’s systemAlphaFoldhas successfullypredicted the three-dimensional folded shapes of thousands of proteins. To make this possible, its creators first trained AlphaFold on the known structures of many proteins. Once it had learned the patterns, it was able to predict, with high accuracy, the structures of other proteins that had not yet been characterized. Betül Kaçarat the University of Wisconsin–Madison and her colleagues did something similar ina study published in 2022. They were trying to reconstruct the evolutionary history of proteins called rhodopsins, which bacteria use to absorb energy from light. In particular, they wanted to know what kinds of light the earliest rhodopsins absorbed, as this would indicate what sort of environment they evolved in. By comparing the genes that code for rhodopsin in distantly related microbes, they were able to estimate the sequences of the oldest rhodopsin genes—genes that no longer exist. Furthermore, they concluded that these early rhodopsin proteins were tuned to specific frequencies of light. They determined this by using a machine-learning technique developed by another group, which could predict the light sensitivity of present-day rhodopsins. Kaçar’s team used machine learning to show that the primordial rhodopsins were most sensitive to green light. This suggested that the microbes they were part of lived a little below the surface of a body of water, where other frequencies of light were blocked by the water. This fits with other lines of evidence about where the first life originated. What about those complex mixtures of chemicals? One approach to understanding them was pioneered by the synthetic chemistBartosz Grzybowskiat the Institute for Basic Science in Ulsan, Republic of Korea, ina study published in 2020. The team compiled data on prebiotic chemistry from dozens of papers published since the Miller experiment in 1953, each of which demonstrated a small number of reactions. They combined them into a single database, creating a network of reactions. Then they wrote a computer program to predict new reactions, based on which kinds of interactions can take place between different kinds of chemicals. Starting from six simple starting materials, including water and ammonia, they showed it was possible to create tens of thousands of chemicals under mild conditions—including many of those found in live organisms. Crucially, the software predicted reactions that have never been observed, several of which the researchers then performed. At a stroke, they had identified a host of new chemical reactions that could have been important in the formation of the first life. The study would have been impossible without the software. “A human is not going to map a network of tens or thousands or millions of connections,” says Grzybowski. However, he emphasizes that the software was coded by chemists and obeys explicit rules. “I wouldn’t even call our stuff AI,” he says, but rather “a hybrid system.” Despite its correct predictions, Grzybowski’s reaction network is still a little theoretical. We also need to know how fast each of the reactions goes, and whether the by-products from earlier reactions will interfere with later ones. Huck, the chemist at Radboud University in the Netherlands, and his colleagues have begun tackling this with help from machine learning. Ina study published in 2022, Huck’s team performed the formose reaction, which creates sugars from simple carbon-based molecules. Given that a sugar called deoxyribose is used to make DNA, making sugars is a crucial early step in the origins of life. The formose reaction does this, but there’s a catch. It tends to undergo a “combinatorial explosion,” says Huck: it produces dozens or hundreds of products, which vary enormously depending on the exact conditions. Huck’s team carried out the reaction in small flow chambers to keep it under control. They varied a number of conditions, including the temperature and the availability of different chemicals; stopped it once it had made a few dozen chemicals; and analyzed the mixture. Environmental conditions like temperature determine what products are formed in the reaction, says Huck. However, it’s not obvious how or why: tiny changes in conditions sometimes have little effect, but sometimes they lead to drastically different outcomes. That’s where machine learning came in: after some training, the software was able to predict what the reaction would spit out. This takes us a step closer to understanding the conditions for making sugars on the primordial Earth. Determining the environmental conditions and other parameters that prevailed at the time is one of the biggest problems for origins-of-life research, saysWentao Ma, a computer modeler at Wuhan University in China. Techniques like machine learning will help narrow it down.In a 2021 study, Ma and his colleagues simulated a mixture of nucleic acids. Using machine learning, they were able to find the optimal conditions for creating nucleic acids that could speed up the formation of their own building blocks—the kind of virtuous circle on which life depends. Finally, machine learning can also help create high-fidelity simulations of the precise mechanisms by which chemical reactions happen—which is crucial for predicting when they will and won’t work. Key tools for this are computer models that simulate all the atoms in a mixture as they bounce around and interact with each other. “When performing the simulation, we can have access to the microscopic behavior of the system,” saysTimothée Devergne, a modeler at Sorbonne University in Paris. However, these “atomistic” simulations quickly become incredibly time consuming. Every single interaction between atoms requires solving complex equations, so it has been very challenging to simulate the complex mixtures that existed on the primordial Earth. Consequently, experiments in prebiotic chemistry have been something of a black box: we can see what’s being spat out, but exactly what happened is mysterious. Devergne is using machine learning to crack this problem. Back in 2007, researchers at ETH Zurich in Switzerland developeda neural network that could learn the most likely solutions to the necessary equations. This sped up the calculations by several orders of magnitude. Devergne and his colleagues are now applying this method to prebiotic chemistry. As a proof of principle, ina study published in 2022they used machine learning to simulate the reactions that made glycine in Miller’s experiment—something Devergne’s supervisor had previouslysimulated without machine learning. The neural network reduced the computation time by a factor of 10 to 50. Similar results from another group werereleased as a preprint in 2022. Everyone contacted for this story agreed that the use of machine learning and other AI tools in research on the origins of life is at a very early stage. Some are wary of over-hyping the approach. “It can’t tell us new stuff, because it knows what it knows,” saysValentina Erastova, a computational scientist at the University of Edinburgh. Machine-learning tools can only make accurate predictions after being fed enormous amounts of high-quality data, she says: “It can show you trends and it can show you links, but the links it will show are also completely biased by how you train it.” What is clear is that AI-type tools can speed up what would otherwise be drudgery. For example, in 2018 Cronin’s team describeda robot that can perform chemical experiments and analyses faster than humans. It used machine learning to assess the progress of reactions in real time, and to predict which mixtures would and would not react. Cronin has already spent years digitizing chemistry: he plans to use these systems to conduct experiments in prebiotic chemistry and discover pathways to the formation of life. In analogy to AlphaFold, he says he wants to make “AlphaSoup.” The power of machine learning is that it can see patterns in huge data sets when humans can’t. “You pick up patterns in complex mixtures and pinpoint processes taking place that you can’t spot yourself,” says Huck. “It’s such a high-dimensional space that the patterns elude you.” The hope is that these methods will enable researchers to finally understand what is happening in complex interacting mixtures like those found on the primordial Earth. “This [technology] allows us to study way bigger systems than before,” says Devergne. There is one final question. Suppose one of the experiments succeeds and a biochemist manages to make a simple form of life in the lab—or suppose the Perseverance rover on Mars discovers an extraterrestrial microbe. How will we know if what we’re looking at is truly alive? “This is an old problem in geochemistry,” saysJim Cleaves, a geochemist at Howard University in Washington, DC. “How do you say that something is living or not living?” For Cronin, the answer is “assembly theory.” He and his colleagues argue that life’s distinguishing feature is that it produces large numbers of highly complex objects. They define an object’s complexity by the number of steps required to make it.In a 2021 study, they presented evidence that they could distinguish between samples produced by life and samples produced without it, based on the measured complexity of the molecules. Machine learning was used to speed up the analyses. Ina study published in September, Cleaves and his colleagues used machine learning more directly. They trained a neural network on a wide range of substances, including basmati rice, coal, and shale. Afterwards it could identify biological and nonbiological samples with 90% accuracy. The AI used a subtly different metric from Cronin’s: it focuses on the overall mix of chemical components within a sample, Cleaves says, rather than the complexity of individual components. “They’re complementary ideas,” he says. Cleaves says methods like these could be applied to the data that comes back from probes like NASA’s Mars rovers. For example, the Curiosity rover has an instrument calledSample Analysis at Mars (SAM)that performs chemical analyses similar to those his team used. With relatively minor modifications, he says, “you could do it now.” Meanwhile researchers like Cronin and Huck are plowing ahead with their studies of primordial biochemistry. “I think we will be able to use machine-learning techniques like AlphaFold, but we’re going to have to retrain them on the chemical soup,” says Cronin. “We need to make AlphaSoup. If we can make AlphaSoup, we’re going to be away to the races.” Michael Marshall is a freelance writer based in the UK. He mostly covers life sciences, health and the environment. His first book The Genesis Quest is about the origins of life and is out now. New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. After deafness treatment, Yiyi can hear her mother and dance to the music. But why is it so noisy at night? As a patient enrolled in a clinical trial for Vertex’s new treatment, I was among the first to experience CRISPR’s transformative effects. Gene editing for sickle-cell is here. This is how researchers knew what DNA to change.

The Biggest Questions: Are we alone in the universe?
https://www.technologyreview.com/2023/11/13/1082873/the-biggest-questions-are-we-alone-in-the-universe/
November 13, 2023

Scientists are training machine-learning models and designing instruments to hunt for life on other worlds. In 1977, the New York Times published an article titled “Seeking an End to Cosmic Loneliness,” describing physicists’ attempts to pick up radio messages from aliens. The endeavor, known as the Search for Extraterrestrial Intelligence (SETI), was still in its early stages, and its proponents were struggling to persuade their peers and Congress that the idea was worth funding. The quest to determine if anyone or anything is out there has gained greater scientific footing in the nearly half-century since that article’s publication. Back then, astronomers had yet to spot a single planet outside our solar system. Now we know the galaxy is teeming with a diversity of worlds. Our planet’s oceans were once considered exceptional, whereas evidence today suggests that numerous moons in the outer solar system host subsurface waters. Our notion of the range of environments where life could exist has also expanded thanks to the discovery on Earth of extremophile organisms that can thrive in places far hotter, saltier, acidic, and more radioactive than previously thought possible, including creatures living around undersea hydrothermal vents. We’re now getting closer than ever before to learning how common living worlds like ours actually are. New tools, including machine learning and artificial intelligence, could help scientists look past their preconceived notions of what constitutes life. Future instruments will sniff the atmospheres of distant planets and scan samples from our local solar system to see if they contain telltale chemicals in the right proportions for organisms to prosper. “I think within our lifetime we will be able to do it,” says Ravi Kopparapu, a planetary scientist at NASA’s Goddard Space Flight Center in Maryland. “We will be able to know if there is life on other planets.” The moon, private space travel, and the wider solar system will all have major missions over the next 12 months. While humans have a long history of speculating about distant worlds, for much of that time actual evidence was in short supply. The first planets around other stars—known as exoplanets—were discovered in the early 1990s, but it took until the launch of NASA’s Kepler space telescope in 2009 for astronomers to understand how common they were. Kepler carefully monitored hundreds of thousands of stars, looking for tiny dips in their brightness that could indicate planets passing in front of them. The mission helped the number of known exoplanets rise from a mere handful to over 5,500. Kepler was built to help determine the prevalence of Earth-like planets orbiting sun-like stars at the right distance to have liquid water on their surface (a region often nicknamed the Goldilocks zone). While not a single extraterrestrial world has been a perfect twin of our own so far, researchers can use the sheer quantity of discoveries to make educated guesses as to how many might be out there. The current best estimates suggest that anywhere between 10% and 50% of sun-like stars have planets like ours, leading to numbers that make astronomers’ heads swim. “If it’s 50%, that’s bonkers, right?” says Jessie Christiansen, an astrophysicist at Caltech in Pasadena, California. “There are billions of sun-like stars in the galaxy, and if half of them have Earth-like planets, there could be billions of habitable rocky planets.” Determining whether these planets actually contain organisms is no easy task. Researchers must capture the faint light from an exoplanet and spread it into its constituent wavelengths, scanning for signatures that indicate the presence and amount of different types of chemicals. While astronomers would like to focus on sun-like stars, doing so is technically challenging. NASA’s mighty new James Webb Space Telescope (JWST) is currently training its 6.5-meter mirror and unparalleled infrared instruments on worlds around stars smaller, cooler, and redder than our sun, known as M dwarfs. Such places might be habitable, but at the moment, nobody is really sure. For liquid water to be present on their surfaces, planets around M dwarfs would need to orbit close to their stars—which tend to be more active than the sun, sending out violent flares that could strip away atmospheric gases and likely leave the ground a dry husk. JWST has been investigating Trappist-1, an M dwarf 40 light-years away with seven small rocky worlds, four of which are at the right distance to potentially have liquid water. The two closest exoplanets have already been shown to be devoid of atmospheres, but scientists are eagerly awaiting the results of JWST observations from the next three. They want to know if even those outside the habitable zone can have atmospheres. There’s special interest in looking for other planets around M dwarf stars, because they are far more prevalent than sun-size stars. “If they find them to hold atmospheres, that increases the habitable real estate of the galaxy a hundredfold,” says Christiansen. Once we’ve found a planet that looks a lot like Earth, then we’ll want to start hunting for chemical clues of life on its surface. JWST isn’t sensitive enough to do that, but future ground-based instruments like the Extremely Large Telescope, Giant Magellan Telescope, and Thirty Meter Telescope—which are expected to begin taking data in the 2030s—could tease out the chemical components of nearby Earth-like worlds. Information from more distant targets will have to wait for NASA’s next planned flagship mission, the space-based Habitable Worlds Observatory, expected to launch sometime in the late 2030s or early 2040s. The telescope will use either an external star shade or an instrument called a coronagraph to block the glaring light of a star and home in on dimmer planetary light and its potential molecular fingerprints. Which chemicals in particular astronomers should be looking for remains a matter of debate. Ideally, they want to find what are known as biosignatures—molecules like water, methane, and carbon dioxide present in amounts similar to what we find on Earth. What that means in practice isn’t always clear, since our planet has gone through many periods when it contained life yet the quantities of different chemicals varied wildly. “Do you want it to detect an Archaean Earth, like 2 or 3 billion years ago?” asks Kopparapu. “Or from the Neoproterozoic, where there was a snowball Earth? Or do you want to detect the current Earth, where there is a lot of free oxygen, ozone, water, and CO2?” Scientists were in awe of the flood of data that arrived when the new space observatory booted up. There was much excitement recently when JWST spotted dimethyl sulfide, a molecule that on our world is made only by living things, on an exoplanet nearly nine times Earth’s size located 120 light-years away. The results which have yet to be confirmed, highlight the trickiness of such methods. If dimethyl sulfide is truly present in the planet’s atmosphere, then starlight should also break it down to form ethane, a molecule that has yet to be seen. “No single gas is a biosignature,” says Kopparapu. “You need to see a combination of them.” Last year, he and others in the communitypublished a reportemphasizing that any particular finding must be placed in the context of its stellar and planetary environment, since there could be many results that seemingly point to life yet have alternative explanations. This problem—how to definitively differentiate between life and non-life—is a perennial one, whether you’re looking at distant planets or even phenomena here on Earth. Researchers may soon receive help from algorithmic techniques that can tease out associations too complex for the human brain to fathom. Inrecent experiments, Robert Hazen and his colleagues took 134 living and non-living samples (including petroleum, carbon-rich meteorites, ancient fossils, and a wasp that flew into their lab), vaporized them, and spread out their chemical constituents. Roughly 500,000 different attributes were identified within each sample’s molecular makeup and run through a machine-learning program. “When we look at those 500,000 attributes, there are patterns that are unique to living things and patterns unique to non-living things,” says Hazen, a mineralogist and astrobiologist at the Carnegie Institution for Science. After the software was trained on 70% of the specimens, the technique was able to recognize with 90% accuracy which of the remaining samples had a biological origin. The device that is used to spread out the chemical components of the samples is around seven inches long, small enough to be sent on missions to nearby ocean worlds like Jupiter’s Europa or Saturn’s Enceladus. NASA’s Perseverance rover carrieda similar instrumentto Mars, so Hazen thinks his team’s machine-learning algorithm could be adapted to sift through its data and hunt for organisms past or present there. And because it relies on molecular relationships rather than detecting specific organic chemicals like DNA or amino acids, which may not be used in other biospheres, the method could allow scientists to look for life entirely unlike what we have on Earth. Such machine-learning applications are also starting to find use in SETI, which has in recent years pivoted toward looking for a broader array of visible evidence for tool-using extraterrestrial species than before. Most in the field are on the lookout for such technosignatures, defined as “some remotely detectable signature of technology that we can characterize with astronomical instrumentation,” says Sofia Sheikh of the SETI Institute. This could be a radio signal, but other evidence could include things like optical laser pulses, giant space-based engineering projects, atmospheric pollution, or even artificial probes that make their way to our solar system. At the Zwicky Transient Facility near San Diego, California, which continuously searches the entire night sky for brief flashes of light coming from unknown sources, engineers are teaching artificial intelligence how to identify features that would not be expected from natural phenomena. “It’s at that point that we can start asking questions,” says Ashish Mahabal, an astronomer and data scientist at Caltech. The answers to such questions could help reveal novel astronomical events or, just maybe, a star surrounded by enormous solar panels that feed an energy-intensive alien society. SETI researchers hope that by using such tools, they can help overcome some of their anthropocentric biases. Most recognize that our expectations of otherworldly beings are constrained by our own experience. For example, the search for signs of massive alien solar panels is often “based on this assumption that there’s always going to be an exponential need for energy,” says Sheikh. Because of all the avenues currently being explored, many scientists believe that answers to our questions about extraterrestrial life are not far off. Yet ultimately, the question of our cosmic loneliness is a philosophical one. For most of humanity’s history, we didn’t believe ourselves to be alone. We filled the heavens with gods, monsters, and mythic creatures. It is only in the modern age that our species has started to worry about its place in the universe. But whether or not any other part of it harbors life, the cosmos is our home. We can choose to be lonely or to embrace the beauty and wonder all around us. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If we want online discourse to improve, we need to move beyond the big platforms.

The Hard Problems issue
https://www.technologyreview.com/magazines/the-hard-problems-issue/
No date

The intractable problem of plastics. Fixing the internet. Exploring what it would it take for AI to become conscious. Plus: there are so many urgent issues facing the world—where do we begin? Bill Gates, Lina Khan, Jennifer Doudna, and others offer their ideas. Plastic is cheap to make and shockingly profitable. It’s everywhere. And we’re all paying the price. Philosophers, cognitive scientists, and engineers are grappling with what it would take for AI to become conscious. New York City is fixing the relationship between government and technology–and not in the ways you’d expect. When it comes to exploring the solar system, we must grapple with the hard limits of physics. Hungarian scholar Gábor Domokos aims to understand the physical world by describing its forms in the simplest possible geometry. If we want online discourse to improve, we need to move beyond the big platforms. Theoretical computer scientist Manuel Blum has guided generations of graduate students into fruitful careers in the field. Cryptographers want encryption schemes that are impossible for tomorrow’s quantum computers to crack. There’s only one catch: they might not exist. Girls Garage blends arts, power

Think that your plastic is being recycled? Think again.
https://www.technologyreview.com/2023/10/12/1081129/plastic-recycling-climate-change-microplastics/
No date
Douglas Main
Plastic is cheap to make and shockingly profitable. It’s everywhere. And we’re all paying the price. On a Saturday last summer, I kayaked up a Connecticut river from the coast, buoyed by the rising tide, to pick up trash with a group of locals. Blue herons and white egrets hunted in the shallows. Ospreys soared overhead hauling freshly caught fish. The wind combed the water into fields of ripples, refracting the afternoon sun into a million diamonds. From our distance, the wetlands looked wild and pristine. Further inland, we left the main river channel and paddled into the muddy heart of the marsh—and began to notice all manner of plastic waste. Big things appeared first: empty bags of chips tangled in the reeds, grocery bags just beneath the surface, Styrofoam trays covered in mud, plastic bottles mixed in with other debris. As we traveled through the marsh, we kept seeing more, and increasingly tiny, bits of plastic. Not just straws, lighters, combs, and fishing line, but unidentifiable and seemingly never-ending small pieces, ranging in size from as big as my hand to as small as grains of sand. You could stay in the hinterlands plucking trash and never leave. Even in one of the less-polluted parts of the East Coast, outside a city with organized waste management and a recycling system, the land and water are awash in plastic waste. Plastic, and the profusion of waste it creates, can hide in plain sight, a ubiquitous part of our lives we rarely question. But a closer examination of the situation can be shocking. Indeed, the scale of the problem is hard to internalize. To date, humans have created around 11 billion metric tons of plastic. This amount surpasses the biomass of all animals, both terrestrial and marine, according to a2020study published inNature. Currently, about 430 million tons of plastic is producedyearly, according to the United Nations Environment Programme (UNEP)—significantlymore than the weight of all human beingscombined. One-third of this total takes the form of single-use plastics, which humans interact with for seconds or minutes before discarding. A total of 95% of the plastic used in packaging is disposed of after one use, a loss to the economy of up to $120 billion annually, concludesa reportby McKinsey. (Just over a quarter of all plastics are used for packaging.) One-third of this packaging is not collected, becoming pollution that generates “significant economic costs by reducing the productivity of vital natural systems such as the ocean.” This causes at least $40 billion in damages, the report states, which exceeds the “profit pool” of the packaging industry. New research could help turn a mix of plastic waste into new products. These numbers are understandably hard to make concrete sense of, even at the scale of specific companies, such as Coca-Cola, which produced 3 million tons of plastic packaging in 2017. That’s the equivalent of making200,000 bottles per minute. Notably, what doesn’t get reused or recycled does not chemically degrade but rather becomes a fixture of our world; it breaks apart to form microplastics, pieces smaller than five millimeters in diameter. In the past few years, scientists have found significant quantities of microplastics in the further reaches of the ocean; in snow and rainfall in seemingly pristine places worldwide; in the air we breathe; and in humanblood,colons, lungs,veins, breast milk,placentas, and fetuses. Onepaperestimated that the average person consumes five grams of plastic every week—mostly from water. About95% of the tap waterin the United States is contaminated. Microplastics are also widely found in beer, salt, shellfish, and other human foods. Significant quantities of these plastic bits have turned up in common fruits and vegetables, asone recent study in Italyfound. All this meant that our journey in the kayaks, picking up plastic waste along the way, looking after our local environment, was—while a genuinely helpful service to our fellow humans—only fixing a symptom of a larger problem. The solution to that problem lies further upstream: to address plastic pollution, those who produce plastics need to pay for the damage it causes, and the world will also have to make less of it. We’ll have to develop better, more recyclable products. We’ll also have to find sustainable alternatives and increase what ecologists call circularity—keeping those products in use as long as possible and finding ways to reuse their materials after that. While these are not exactly new ideas, they’ve received renewed attention from global policymakers, innovators, and companies looking to make a sustainable future profitable. Making less is the most important goal—and the most politically charged one, given the immense profits and political power of plastic producers. “What’s the best way to manage waste?” saysJenna Jambeck, an environmental engineer at the University of Georgia. “To not produce it in the first place.” Because consider this: most of the plastic we make, 72%, ends up in landfills or the environment, according to a2022 report from the Organisation for EconomicCo-operation and Development. Only 9% of the plastic ever produced has been recycled, and 19% has been incinerated. Some of it reaches the sea; estimates suggest that between 8 million and 11 million tons of plastic waste enter the ocean each year.According to the National Academy of Sciences, that’s the equivalent of dumping a garbage truck of plastic into the ocean every minute. Plastic production has grown dramatically in recent years; in fact, half of all plastics in existence have been produced in just the last two decades. Production is projected to continue growing, at about 5% annually. If current trends continue, humans will have produced 34 billion tons of plastics by 2050—three times the current total. Plastic pollution—“a scourge on a planetary scale,” as French president Emmanuel Macron has put it—most affects those least able to deal with its consequences. Noting that the plastic industry generates upward of $700 billion a year in revenues, the UN Environment Programme also concluded that the industry “inflicts a heavy burden on human health and environmental degradation, with the poorest in societyfacing the highest impactswhilst contributing the least to plastic over-consumption and waste.” This is true at every stage of plastic’s life cycle. Manufacturing plants are concentrated in communities of color—such as in Louisiana, in an area along the Mississippi River often called “Cancer Alley,” which is home to nearly 150 oil refineries, plastics plants, and chemical facilities. Such plants emit air pollution that raises risks of cancer and other diseases. A panel of UN human rights expertssaid the situationamounts to a “form of environmental racism [that] poses serious and disproportionate threats to the ... human rights of its largely African American residents.” This pollution also disproportionately harms poor and developing countries that produce little or no plastic, such as those in Africa, the Pacific, and elsewhere. “We have to dramatically reduce the amount of plastic that we make. Everything else is second order.” Solutions such as recycling and reuse cannot deal with this much waste, saysMarcus Eriksen, a marine scientist and cofounder of the 5 Gyres Institute, which studies plastic pollution. “There have to be drastic cuts in production,” he says, especially of single-use plastics. Dozens of studies and institutional reports—from the likes of the United Nations, the National Academy of Sciences, and the Pew Charitable Trusts—conclude that continued increases in production of virgin plastics will overwhelm actions to combat the problem. Alarmed by such data, and animated by growing public awareness of the issue, the United Nations Environment Assembly resolved at a March 2022meetingto begin working toward a global treaty to end plastic pollution, forming an intergovernmental negotiating committee to accomplish this goal. This group has gathered twice and will meet another three times before the treaty is finalized in late 2024. All parties agree that it will be binding and will put forth a range of mandatory and voluntary approaches. Some have likened its importance to that of the Paris accords on climate change. Few details have yet been ironed out, but the majority of countries agree that a primary way to prevent plastic from polluting the environment is to make less of it. Neil Tangri, a researcher at the University of California, Berkeley, and a member of an informal advisory group called theScientists’ Coalition for an Effective Plastics Treaty, strongly agrees: “We have to dramatically reduce the amount of plastic that we make. Everything else is second order.” At the second round of talks in Paris this summer, international leaders made this desire clear. Humanity has a duty to begin “[reducing] the production of new plastics,” said Macron, “and to ban as soon as possible the most polluting products.” Representatives from many other countries, from Ghana to Mauritius to Norway, argued the same. Yet the countries that have not yet embraced limits on production include the biggest producers, such as China and the United States, though they are participating in the process. Limits or levies on production are not currently being considered as a solution, according to a member of the US State Department (which coordinates the country’s delegation at the UN meetings), who was not authorized to speak publicly on the matter. “We really need to find a way to bring everybody on board,” this person said, and such “supply side” changes might be unpalatable to certain countries. “We want the strongest and most ambitious obligations that we can get consensus around.” The American Chemistry Council, the trade group that represents plastic producers, has also not embraced such policies. Limits or levies could “affect all sectors of the economy” and “create a lot of unintended consequences for those least able to afford it,” saysStewart Harris, the group’s senior director of global plastics policy. How can we make less plastic, and deal with the pollution that already exists? Circularity may be the most promising answer. Circularity can mean reusing or recycling plastics, or employing alternatives that can be reused or recycled as well. Proponents often describe the concept as an attempt to imitate the natural world, where there is no waste; everything has a use. Ghana and several other countries worldwide are currently working to establish a country-level circular economy for plastic, saysOliver Boachie, who chairs the African Group of Negotiators for the UN treaty-making process and is an advisor to the Ghanaian government. This will involve gradually banning single-use plastics that have little reuse value, such as thin plastic films used in food packaging, as well as instituting robust collection, reuse, and recycling efforts. Many existing waste management techniques have already been shown to reduce plastic pollution and demand for plastic in the first place. But they are energy and time intensive. In Tanzania, for instance, a group called Nipe Fagio (“give me the broom” in Swahili) runs waste management and recycling systems that have reduced landfill waste by 75% to 80% in neighborhoods in several cities. Waste collectors visit households once a week to gather four different varieties of trash before transporting it to a collection center. There, workers further sort the recyclable materials for sale, turn organic waste into compost and chicken feed, and send the rest to the landfill. “The amount of plastic on our planet—it’s like one big oil spill.” To help fund programs like Nipe Fagio, and to help them grow on a much larger scale, many countries are looking toextended producer responsibility(EPR) plans, policies requiring producers of plastic bottles, packaging, and the like to provide some funding to support management of these materials after their initial use. Just about every country in Europe has an EPR scheme, and Ghana too is working to create a national program. Currently, however, EPR schemes are limited in their impact, since those that have done the most to embrace and pay for them are bottlers and manufacturers of products like beverages, known as “midstream” producers. To make a bigger difference, the programs need to bring in the “upstream” producers—those that create virgin plastics and polymers, like Exxon, Dow, Sinopec, and Saudi Aramco. An overwhelming 98% of plastics come from fossil fuels, and plastic production and use accounts for 3.4% of humanity’s carbon emissions. Many big plastic producers—such as the world’s biggest, ExxonMobil—are highly entangled with Big Oil or representatives of it. “Beyond a physical pollution crisis, it’s becoming an energy crisis,” saysKatrina Knauer,a polymer scientist with the National Renewable Energy Laboratory. “The amount of plastic on our planet—it’s like one big oil spill.” Nevertheless, these companies do not currently pay for the consequences of plastic pollution, Boachie says, adding: “We believe that those who are [most] heavily responsible for the proliferation of plastics around the world are the polymer and virgin plastics producers, and they should be responsible for providing funds for countries to manage the plastic waste that they create.” Ghana has introduced a proposal to the UN to extend the “polluter pays” principle to these polymer producers, and Boachie says he believes elements of it will find their way into the final UN agreement. That would “allow us to mobilize a significant amount of resources to provide all countries the means to manage their plastics.” But Ana Lê Rocha, the executive director of Nipe Fagio in Tanzania, argues that waste management is not actually a solution to the pollution crisis but merely a way to deal with a symptom. “We need to remember that the main issue—the main goal of the UN treaty—must be to reduce production,” she says. Reuse is the most energy-efficient version of circularity. Collecting, cleaning, and refilling glass bottles was once common and widespread, and it remains a small but significant part of the economy in many countries. It’s also the norm in many places to buy foods in bulk and transport them in reusable bags. But one of the biggest obstacles to circularity is a lack of infrastructure, says Ellie Moss, CEO of a company calledPerpetual, which is “looking to stand up a whole reuse ecosystem [at] the scale of a small city” to change that. Four cities, to be exact—Galveston, Texas; Hilo, Hawaii; Ann Arbor, Michigan; and Savannah, Georgia. In Galveston, where Perpetual is furthest along, it is working to create a system whereby metal beverage containers can be reused by many restaurants in the city, saving large amounts of plastic and creating new green jobs. It hopes to hire companies that will have the program up and running there by the middle of 2024. “If we want reuse to work, it has to happen at scale, and the community has to have a voice in how the system is set up,” Moss says. Plastics are…complicated. Can new recycling methods clean them up? Other companies are also exploring refill and reuse schemes. One Chilean company, Algramo, founded in 2013, allows customers to buy various liquid products such as shampoo, laundry detergent, and soaps in reusable plastic bottles, purchased from a large network of filling stations. The company has the explicit goal of eliminating the “poverty tax,” the penalty that lower-income people often have to pay for not being able to buy in bulk; it charges the same unit price for each item regardless of how much volume is sold. Algramo (which means “by the gram” in Spanish) has expanded throughout Chile and is now opening locations in the United Kingdom. These schemes can be thought of as a type of system redesign, requiring a radical shift in infrastructureandbehavior. We spent nearly a century “building out an exceptionally complex linear economy for these materials,” saysKathryn Beers, a polymer chemist at the National Institute of Standards and Technology, who leads an institute-wide program geared toward facilitating a circular economy. But we never “built the second half of the system” that would make it circular, she says. “It needs all the complexity and nuance of the front half—and that takes time.” Awareness helps prompt such shifts—viral moments such as the video of a turtle with a straw in its nose that circulated widely in 2017 are credited with greatly increased demand for straw bans or alternatives. But for real change, policies are necessary, including bans as well as fees and taxes. Research shows thatall of the abovecangreatly reduceplastic waste. Redesigning products to use less plastic and to be more easily reused or recycled is also critical, said Inger Andersen, executive director of UNEP, at the opening of the second meeting. “Is there a good reason that businesses can’t look at refillable bottles, reusable packaging, take-back services, and so on? Of course not,” she said. Some manufacturers have already made strides to use less plastic in their products. Such incremental changes help but will still not be enough. To solve the pollution crisis, many “unnecessary and problematic” plastics—such as polyvinyl chloride, or PVC—will have to be eliminated and replaced with more sustainable alternatives, saysImari Walker-Franklin, a research chemist whopublished a book with MIT Presson plastics earlier this year. PVC, which is often used to make pipes and other materials, breaks down into toxic chlorine-­containing components and cannot be recycled. One of the most promising replacements is a substance called PHA, or polyhydroxyalkanoate, a type of bio-polyester made by bacterial fermentation of sugars and lipids. “We’d love to see an all-PHA future,” NREL’s Knauer says, in part because the plastic can degrade into nontoxic components over the course of months. It’s important to note, however, that producing more sustainable plastics is difficult, and most of the so-called “biodegradable” and “compostable” plastics on the market biodegrade only in industrial reactors. Industrial composters, for example, reach temperatures that cannot be achieved in people’s yards or homes. Moreover, most of these materials are not actually less toxic than conventional plastics, saysBethanie Almroth, an ecotoxicologist with Sweden’s University of Gothenburg. “Bioplastics are plastics. And they are usually quite harmful,” Lê Rocha agrees. For that reason, it’s vital that bio-based plastics don’t just become a replacement. “The best alternative is reusable systems, because replacing a single-use plastic with a single-use bioplastic won’t change the problem,” says Andrea Lema, an advocate for zero-waste systems in Quito, Ecuador, who’s involved in the UN process. Non-plastic alternatives, such as packaging made from fungi, hemp, and other environmentally friendly materials, may hold the most promise in the long term, but in the short term they are generally not economically viable given how cheap plastic is. That could change with the right set of progressive policies and economic incentives. In the United States, only about 5% to 6% of plastics are being recycled each year—a paltry rate. As with reuse, increasing this rate should decrease the demand for virgin polymers. The biggest problem is a shortage of the costly infrastructure that’s required, saysKate Bailey, chief policy officer with the Association of Plastic Recyclers. The further you get from large cities, the less recycling there is, because rural areas can’t afford it, says Knauer: “We need more state and federal incentives to build an infrastructure for collection.” The vast majority of “recycling” involves grinding up plastic, melting it down, and re-forming it. Doing this type of mechanical recycling well involves properly sorting and cleaning materials, which can be time intensive and expensive. It’s also very difficult or impossible to recycle many types of plastic more than once without causing the material to acquire defects and contaminants. In fact, many recycled materials commonly contain significant levels of unwanted toxins, Almroth says. Local policies can make a huge difference in encouraging recycling. In Maine and Oregon, which have invested in recycling programs, up to 80% of bottles made from PET (polyethylene terephthalate) are recycled, Bailey says. In some states, such as in the South, that percentage is in the single digits. The national average for these materials is 30%, which is a shame, Bailey says, because 100% of PET bottles could be recycled. Some states, though, have instituted policies that actually hinder progress. Industry lobbyists are increasingly helping to institute state-level laws that prevent bans or limits on the use of plastics, especially plastic bags. Over a dozen states currently havepreemptive lawson the books to prevent ordinances limiting plastics, though some of the same states arealso trying to pass anti-preemption laws. Fundamentally, to solve the plastic pollution crisis, society must address the root problem: plastics are shockingly profitable and cheap. One way to improve recycling—and prevent unwanted health effects and environmental problems—would be to simplify and standardize the process of plastic production, Walker-Franklin says. Currently, more than 10,000 chemicals are used in the production of plastics, and upward of 3,200 have “one or more hazardous properties of concern,” with the potential to harm humans and wildlife,according to UNEP. Very little or nothing is known about the health effects or basic properties of thousands more. Another way to improve recycling would be to find a way to process mixed polymers into useful materials instead of having to sort everything first. One promising technique, described in an October 2020 study coauthored by Julie Rorrer, then a researcher at MIT, canprocess polypropylene and polyethylene into propane. Another process,described in a studypublished inSciencethe same month, can break down mixtures of common consumer plastics and re-form them into a bioplastic, in part by using an engineered soil bacterium. Others dream of a day when microbes could be used to recycle or clean up all this waste. One French biotechnology company, Carbios, opened a pilot plant in September 2021 to break down and recycle PET using an engineered form of an enzyme first discovered in compost; it’s currently building a full-scale facility due to open in 2025. In theory, this type of recycling could be truly circular, as it wouldn’t require the high heat that normally causes much of the degradation seen with recycled plastics. A microbe discovered in Japan in 2016, calledIdeonella sakaiensis, produces two other enzymes that can break down PET. This microbe is especially intriguing because it is the first one identified that can live solely upon plastic as a food source. MIT researcher Linda Zhong-Johnson is working to createmore efficient versions of the enzymesby tinkering with microbial genes. So far, one mutation she has identified creates an enzyme that appears to be up to 30% more efficient than its original wild form. Fundamentally, to solve the plastic pollution crisis, society must address the root problem: plastics are shockingly profitable and cheap because polymer producers do not pay for the abundant harm they cause. Any solution will require policy and behavioral changes small and large. As an example of the former, policymakers in Washington, DC, instituted a five-cent charge on plastic bags that began in 2010. Estimates suggest that the number of bags used quickly dropped—bymore than halfin the months after it was instituted—and the quantity found in local waterwaysdropped between 30% and 70%thereafter. Seemingly tiny changes like this can add up to reduce demand and decrease pollution. Meanwhile, a global EPR scheme would be an example of a major shift, and the UN process is seeking other big changes to the status quo. Of course, such changes will be difficult, but they can be instituted in gradual ways that don’t hurt businesses, Boachie says: “My hope emanates from the fact that what we are talking about is not something that will impede the growth and success of any company.” On the contrary, he adds, creating incentives for alternatives will spur innovation and create new jobs. A lot of such innovation will doubtless be needed to reverse situations like what I saw in the Connecticut salt marsh. At one point we came upon a couple of osprey nests from which plastic strands billowed, unwittingly collected by the birds as they built their nests. Later, we found a vinyl firehose lodged intractably in the muck between oysters. I couldn’t pull it out, nor could I cut into it with a small pocketknife. We reluctantly left it behind. Douglas Main is a journalist and former senior editor and writer at National Geographic. This story was part of our November/December 2023 issue. It uncovered systemic problems with offset markets and recommended that the public university system focus on cutting its direct emissions instead. Companies need to invest in energy-efficient infrastructure and optimize data practices, says Ian Clatworthy, director of data platform product marketing at Hitachi Vantara. A year ago, scientists generated net energy with a fusion reactor. This is what’s happened since then. Sustainable computing practices have the power to both infuse operational efficiencies and greatly reduce energy consumption, says Jen Huffstetler, chief product sustainability officer at Intel.

Minds of machines: The great AI consciousness conundrum
https://www.technologyreview.com/2023/10/16/1081149/ai-consciousness-conundrum/
No date
Grace Huckins
Philosophers, cognitive scientists, and engineers are grappling with what it would take for AI to become conscious. David Chalmers was not expecting the invitation he received in September of last year. As a leading authority on consciousness, Chalmers regularly circles the world delivering talks at universities and academic meetings to rapt audiences of philosophers—the sort of people who might spend hours debating whether the world outside their own heads is real and then go blithely about the rest of their day. This latest request, though, came from a surprising source: the organizers of the Conference on Neural Information Processing Systems (NeurIPS), a yearly gathering of the brightest minds in artificial intelligence. Less than six months before the conference, an engineer named Blake Lemoine, then at Google, had gone public with his contention that LaMDA, one of the company’s AI systems, had achieved consciousness. Lemoine’s claims were quickly dismissed in the press, and he was summarily fired, but the genie would not return to the bottle quite so easily—especially after the release of ChatGPT in November 2022. Suddenly it was possible for anyone to carry on a sophisticated conversation with a polite, creative artificial agent. Chalmers was an eminently sensible choice to speak about AI consciousness. He’d earned his PhD in philosophy at an Indiana University AI lab, where he and his computer scientist colleagues spent their breaks debating whether machines might one day have minds. In his 1996 book,The Conscious Mind, he spent an entire chapter arguing that artificial consciousness was possible. If he had been able to interact with systems like LaMDA and ChatGPT back in the ’90s, before anyone knew how such a thing might work, he would have thought there was a good chance they were conscious, Chalmers says. But when he stood before a crowd of NeurIPS attendees in a cavernous New Orleans convention hall, clad in his trademark leather jacket, he offered a different assessment. Yes, large language models—systems that have been trained on enormous corpora of text in order to mimic human writing as accurately as possible—are impressive. But, he said, they lack too many of the potential requisites for consciousness for us to believe that they actually experience the world. “Consciousness poses a unique challenge in our attempts to study it, because it’s hard to define.” At the breakneck pace of AI development, however, things can shift suddenly. For his mathematically minded audience, Chalmers got concrete: the chances of developing any conscious AI in the next 10 years were, he estimated, above one in five. Not many people dismissed his proposal as ridiculous, Chalmers says: “I mean, I’m sure some people had that reaction, but they weren’t the ones talking to me.” Instead, he spent the next several days in conversation after conversation with AI experts who took the possibilities he’d described very seriously. Some came to Chalmers effervescent with enthusiasm at the concept of conscious machines. Others, though, were horrified at what he had described. If an AI were conscious, they argued—if it could look out at the world from its own personal perspective, not simply processing inputs but also experiencing them—then, perhaps, it could suffer. AI consciousness isn’t just a devilishly tricky intellectual puzzle; it’s a morally weighty problem with potentially dire consequences. Fail to identify a conscious AI, and you might unintentionally subjugate, or even torture, a being whose interests ought to matter. Mistake an unconscious AI for a conscious one, and you risk compromising human safety and happiness for the sake of an unthinking, unfeeling hunk of silicon and code. Both mistakes are easy to make. “Consciousness poses a unique challenge in our attempts to study it, because it’s hard to define,” says Liad Mudrik, a neuroscientist at Tel Aviv University who has researched consciousness since the early 2000s. “It’s inherently subjective.” Over the past few decades, a small research community has doggedly attacked the question of what consciousness is and how it works. The effort has yielded real progress on what once seemed an unsolvable problem. Now, with the rapid advance of AI technology, these insights could offer our only guide to the untested, morally fraught waters of artificial consciousness. “If we as a field will be able to use the theories that we have, and the findings that we have, in order to reach a good test for consciousness,” Mudrik says, “it will probably be one of the most important contributions that we could give.” When Mudrik explains her consciousness research, she starts with one of her very favorite things: chocolate. Placing a piece in your mouth sparks a symphony of neurobiological events—your tongue’s sugar and fat receptors activate brain-bound pathways, clusters of cells in the brain stem stimulate your salivary glands, and neurons deep within your head release the chemical dopamine. None of those processes, though, captures what it is like to snap a chocolate square from its foil packet and let it melt in your mouth. “What I’m trying to understand is what in the brain allows us not only to process information—which in its own right is a formidable challenge and an amazing achievement of the brain—but also to experience the information that we are processing,” Mudrik says. Studying information processing would have been the more straightforward choice for Mudrik, professionally speaking. Consciousness has long been a marginalized topic in neuroscience, seen as at best unserious and at worst intractable. “A fascinating but elusive phenomenon,” reads the “Consciousness” entry in the 1996 edition of theInternational Dictionary of Psychology. “Nothing worth reading has been written on it.” Plus: Five big takeaways from Europe’s AI Act. Mudrik was not dissuaded. From her undergraduate years in the early 2000s, she knew that she didn’t want to research anything other than consciousness. “It might not be the most sensible decision to make as a young researcher, but I just couldn’t help it,” she says. “I couldn’t get enough of it.” She earned two PhDs—one in neuroscience, one in philosophy—in her determination to decipher the nature of human experience. As slippery a topic as consciousness can be, it is not impossible to pin down—put as simply as possible, it’s the ability to experience things. It’s often confused with terms like “sentience” and “self-awareness,” but according to the definitions that many experts use, consciousness is a prerequisite for those other, more sophisticated abilities. To be sentient, a being must be able to have positive and negative experiences—in other words, pleasures and pains. And being self-aware means not only having an experience but alsoknowingthat you are having an experience. In her laboratory, Mudrik doesn’t worry about sentience and self-­awareness; she’s interested in observing what happens in the brain when she manipulates people’s conscious experience. That’s an easy thing to do in principle. Give someone a piece of broccoli to eat, and the experience will be very different from eating a piece of chocolate—and will probably result in a different brain scan. The problem is that those differences are uninterpretable. It would be impossible to discern which are linked to changes in information—broccoli and chocolate activate very different taste receptors—and which represent changes in the conscious experience. The trick is to modify the experience without modifying the stimulus, like giving someone a piece of chocolate and then flipping a switch to make it feel like eating broccoli. That’s not possible with taste, but it is with vision. In one widely used approach, scientists have people look at two different images simultaneously, one with each eye. Although the eyes take in both images, it’s impossible to perceive both at once, so subjects will often report that their visual experience “flips”: first they see one image, and then, spontaneously, they see the other. By tracking brain activity during these flips in conscious awareness, scientists can observe what happens when incoming information stays the same but the experience of it shifts. With these and other approaches, Mudrik and her colleagues have managed to establish some concrete facts about how consciousness works in the human brain. The cerebellum, a brain region at the base of the skull that resembles a fist-size tangle of angel-hair pasta, appears to play no role in conscious experience, though it is crucial for subconscious motor tasks like riding a bike; on the other hand, feedback connections—for example, connections running from the “higher,” cognitive regions of the brain to those involved in more basic sensory processing—seem essential to consciousness. (This, by the way, is one good reason to doubt the consciousness of LLMs: they lack substantial feedback connections.) A decade ago, a group of Italian and Belgian neuroscientistsmanaged to devise a testfor human consciousness that uses transcranial magnetic stimulation (TMS), a noninvasive form of brain stimulation that is applied by holding a figure-eight-shaped magnetic wand near someone’s head. Solely from the resulting patterns of brain activity, the team was able to distinguish conscious people from those who were under anesthesia or deeply asleep, and they could even detect the difference between a vegetative state (where someone is awake but not conscious) and locked-in syndrome (in which a patient is conscious but cannot move at all). That’s an enormous step forward in consciousness research, but it means little for the question of conscious AI: OpenAI’s GPT models don’t have a brain that can be stimulated by a TMS wand. To test for AI consciousness, it’s not enough to identify the structures that give rise to consciousness in the human brain. You need to know why those structures contribute to consciousness, in a way that’s rigorous and general enough to be applicable to any system, human or otherwise. “Ultimately, you need a theory,” says Christof Koch, former president of the Allen Institute and an influential consciousness researcher. “You can’t just depend on your intuitions anymore; you need a foundational theory that tells you what consciousness is, how it gets into the world, and who has it and who doesn’t.” Here’s one theory about how that litmus test for consciousness might work: any being that is intelligent enough, that is capable of responding successfully to a wide enough variety of contexts and challenges, must be conscious. It’s not an absurd theory on its face. We humans have the most intelligent brains around, as far as we’re aware, and we’re definitely conscious. More intelligent animals, too, seem more likely to be conscious—there’s far more consensus that chimpanzees are conscious than, say, crabs. But consciousness and intelligence are not the same. When Mudrik flashes images at her experimental subjects, she’s not asking them to contemplate anything or testing their problem-solving abilities. Even a crab scuttling across the ocean floor, with no awareness of its past or thoughts about its future, would still be conscious if it could experience the pleasure of a tasty morsel of shrimp or the pain of an injured claw. Susan Schneider, director of the Center for the Future Mind at Florida Atlantic University, thinks that AI could reach greater heights of intelligence by forgoing consciousness altogether. Conscious processes like holding something in short-term memory are pretty limited—we can only pay attention to a couple of things at a time and often struggle to do simple tasks like remembering a phone number long enough to call it. It’s not immediately obvious what an AI would gain from consciousness, especially considering the impressive feats such systems have been able to achieve without it. As further iterations of GPT prove themselves more and more intelligent—more and more capable of meeting a broad spectrum of demands, fromacing the bar examtobuilding a website from scratch—their success, in and of itself, can’t be taken as evidence of their consciousness. Even a machine that behaves indistinguishably from a human isn’t necessarily aware of anything at all. Understanding how an AI works on the inside could be an essential step toward determining whether or not it is conscious. Schneider, though, hasn’t lost hope in tests. Together with the Princeton physicist Edwin Turner, she has formulated what she calls the “artificial consciousness test.” It’s not easy to perform: it requires isolating an AI agent from any information about consciousness throughout its training. (This is important so that it can’t, like LaMDA, just parrot human statements about consciousness.) Then, once the system is trained, the tester asks it questions that it could only answer if it knew about consciousness—knowledge it could only have acquired from being conscious itself. Can it understand the plot of the filmFreaky Friday, where a mother and daughter switch bodies, their consciousnesses dissociated from their physical selves? Does it grasp the concept of dreaming—or even report dreaming itself? Can it conceive of reincarnation or an afterlife? There’s a huge limitation to this approach: it requires the capacity for language. Human infants and dogs, both of which are widely believed to be conscious, could not possibly pass this test, and an AI could conceivably become conscious without using language at all. Putting a language-based AI like GPT to the test is likewise impossible, as it has been exposed to the idea of consciousness in its training. (Ask ChatGPT to explainFreaky Friday—it does a respectable job.) And because we still understand so little about how advanced AI systems work, it would be difficult, if not impossible, to completely protect an AI against such exposure. Our very language is imbued with the fact of our consciousness—words like “mind,” “soul,” and “self” make sense to us by virtue of our conscious experience. Who’s to say that an extremely intelligent, nonconscious AI system couldn’t suss that out? If Schneider’s test isn’t foolproof, that leaves one more option: opening up the machine. Understanding how an AI works on the inside could be an essential step toward determining whether or not it is conscious, if you know how to interpret what you’re looking at. Doing so requires a good theory of consciousness. A few decades ago, we might have been entirely lost. The only available theories came from philosophy, and it wasn’t clear how they might be applied to a physical system. But since then, researchers like Koch and Mudrik have helped to develop and refine a number of ideas that could prove useful guides to understanding artificial consciousness. Numerous theories have been proposed, and none has yet been proved—or even deemed a front-­runner. And they make radically different predictions about AI consciousness. Some theories treat consciousness as a feature of the brain’s software: all that matters is that the brain performs the right set of jobs, in the right sort of way. According to global workspace theory, for example, systems are conscious if they possess the requisite architecture: a variety of independent modules, plus a “global workspace” that takes in information from those modules and selects some of it to broadcast across the entire system. Other theories tie consciousness more squarely to physical hardware. Integrated information theory proposes that a system’s consciousness depends on the particular details of its physical structure—specifically, how the current state of its physical components influences their future and indicates their past. According to IIT, conventional computer systems, and thus current-day AI, can never be conscious—they don’t have the right causal structure. (The theory was recently criticized by some researchers, who think it has gotten outsize attention.) Anil Seth, a professor of neuroscience at the University of Sussex, is more sympathetic to the hardware-­based theories, for one main reason: he thinks biology matters. Every conscious creature that we know of breaks down organic molecules for energy, works to maintain a stable internal environment, and processes information through networks of neurons via a combination of chemical and electrical signals. If that’s true of all conscious creatures, some scientists argue, it’s not a stretch to suspect that any one of those traits, or perhaps even all of them, might be necessary for consciousness. My avatars were cartoonishly pornified, while my male colleagues got to be astronauts, explorers, and inventors. Because he thinks biology is so important to consciousness, Seth says, he spends more time worrying about the possibility of consciousness in brain organoids—clumps of neural tissue grown in a dish—than in AI. “The problem is, we don’t know if I’m right,” he says. “And I may well be wrong.” He’s not alone in this attitude. Every expert has a preferred theory of consciousness, but none treats it as ideology—all of them are eternally alert to the possibility that they have backed the wrong horse. In the past five years, consciousness scientists have started working together on a series of “adversarial collaborations,” in which supporters of different theories come together to design neuroscience experiments that could help test them against each other. The researchers agree ahead of time on which patterns of results will support which theory. Then they run the experiments and see what happens. In June, Mudrik, Koch, Chalmers, and a large group of collaboratorsreleased the resultsfrom an adversarial collaboration pitting global workspace theory against integrated information theory. Neither theory came out entirely on top. But Mudrik says the process was still fruitful: forcing the supporters of each theory to make concrete predictions helped to make the theories themselves more precise and scientifically useful. “They’re all theories in progress,” she says. At the same time, Mudrik has been trying to figure out what this diversity of theories means for AI. She’s working with an interdisciplinary team of philosophers, computer scientists, and neuroscientists who recently put out awhite paperthat makes some practical recommendations on detecting AI consciousness. In the paper, the team draws on a variety of theories to build a sort of consciousness “report card”—a list of markers that would indicate an AI is conscious, under the assumption that one of those theories is true. These markers include having certain feedback connections, using a global workspace, flexibly pursuing goals, and interacting with an external environment (whether real or virtual). In effect, this strategy recognizes that the major theories of consciousness have some chance of turning out to be true—and so if more theories agree that an AI is conscious, it is more likely to actually be conscious. By the same token, a system that lacks all those markers can only be conscious if our current theories are very wrong. That’s where LLMs like LaMDA currently are: they don’t possess the right type of feedback connections, use global workspaces, or appear to have any other markers of consciousness. The trouble with consciousness-­by-committee, though, is that this state of affairs won’t last. According to the authors of the white paper, there are no major technological hurdles in the way of building AI systems that score highly on their consciousness report card. Soon enough, we’ll be dealing with a question straight out of science fiction: What should one do with a potentially conscious machine? In 1989, years before the neuroscience of consciousness truly came into its own,Star Trek: The Next Generationaired an episode titled “The Measure of a Man.” The episode centers on the character Data, an android who spends much of the show grappling with his own disputed humanity. In this particular episode, a scientist wants to forcibly disassemble Data, to figure out how he works; Data, worried that disassembly could effectively kill him, refuses; and Data’s captain, Picard, must defend in court his right to refuse the procedure. Picard never proves that Data is conscious. Rather, he demonstrates that no one can disprove that Data is conscious, and so the risk of harming Data, and potentially condemning the androids that come after him to slavery, is too great to countenance. It’s a tempting solution to the conundrum of questionable AI consciousness: treat any potentially conscious system as if it is really conscious, and avoid the risk of harming a being that can genuinely suffer. Treating Data like a person is simple: he can easily express his wants and needs, and those wants and needs tend to resemble those of his human crewmates, in broad strokes. But protecting a real-world AI from suffering could prove much harder, says Robert Long, a philosophy fellow at the Center for AI Safety in San Francisco, who is one of the lead authors on the white paper. “With animals, there’s the handy property that they do basically want the same things as us,” he says. “It’s kind of hard to know what that is in the case of AI.” Protecting AI requires not only a theory of AI consciousness but also a theory of AI pleasures and pains, of AI desires and fears. “With animals, there’s the handy property that they do basically want the same things as us. It’s kind of hard to know what that is in the case of AI.” And that approach is not without its costs. OnStar Trek, the scientist who wants to disassemble Data hopes to construct more androids like him, who might be sent on risky missions in lieu of other personnel. To the viewer, who sees Data as a conscious character like everyone else on the show, the proposal is horrifying. But if Data were simply a convincing simulacrum of a human, it would be unconscionable to expose a person to danger in his place. Extending care to other beings means protecting them from harm, and that limits the choices that humans can ethically make. “I’m not that worried about scenarios where we care too much about animals,” Long says. There are few downsides to ending factory farming. “But with AI systems,” he adds, “I think there could really be a lot of dangers if we overattribute consciousness.” AI systems might malfunction and need to be shut down; they might need to be subjected to rigorous safety testing. These are easy decisions if the AI is inanimate, and philosophical quagmires if the AI’s needs must be taken into consideration. Seth—who thinks that conscious AI is relatively unlikely, at least for the foreseeable future—nevertheless worries about what the possibility of AI consciousness might mean for humans emotionally. “It’ll change how we distribute our limited resources of caring about things,” he says. That might seem like a problem for the future. But the perception of AI consciousness is with us now: Blake Lemoine took a personal risk for an AI he believed to be conscious, and he lost his job. How many others might sacrifice time, money, and personal relationships for lifeless computer systems? Even bare-bones chatbots can exert an uncanny pull: a simple program called ELIZA, built in the 1960s to simulate talk therapy, convinced many users that it was capable of feeling and understanding. The perception of consciousness and the reality of consciousness are poorly aligned, and that discrepancy will only worsen as AI systems become capable of engaging in more realistic conversations. “We will be unable to avoid perceiving them as having conscious experiences, in the same way that certain visual illusions are cognitively impenetrable to us,” Seth says. Just as knowing that the two lines in the Müller-Lyer illusion are exactly the same length doesn’t prevent us from perceiving one as shorter than the other, knowing GPT isn’t conscious doesn’t change the illusion that you are speaking to a being with a perspective, opinions, and personality. In 2015, years before these concerns became current, the philosophers Eric Schwitzgebel and Mara Garza formulated a set of recommendations meant to protect against such risks. One of their recommendations, which they termed the “Emotional Alignment Design Policy,” argued that any unconscious AI should be intentionally designed so that users will not believe it is conscious. Companies have taken some small steps in that direction—ChatGPT spits out a hard-coded denial if you ask it whether it is conscious. But such responses do little to disrupt the overall illusion. Schwitzgebel, who is a professor of philosophy at the University of California, Riverside, wants to steer well clear of any ambiguity. In their 2015 paper, he and Garza also proposed their “Excluded Middle Policy”—if it’s unclear whether an AI system will be conscious, that system should not be built. In practice, this means all the relevant experts must agree that a prospective AI is very likely not conscious (their verdict for current LLMs) or very likely conscious. “What we don’t want to do is confuse people,” Schwitzgebel says. Avoiding the gray zone of disputed consciousness neatly skirts both the risks of harming a conscious AI and the downsides of treating a lifeless machine as conscious. The trouble is, doing so may not be realistic. Many researchers—like Rufin VanRullen, a research director at France’s Centre Nationale de la Recherche Scientifique, who recently obtained funding to build an AI with a global workspace—are now actively working to endow AI with the potential underpinnings of consciousness. The downside of a moratorium on building potentially conscious systems, VanRullen says, is that systems like the one he’s trying to create might be more effective than current AI. “Whenever we are disappointed with current AI performance, it’s always because it’s lagging behind what the brain is capable of doing,” he says. “So it’s not necessarily that my objective would be to create a conscious AI—it’s more that the objective of many people in AI right now is to move toward these advanced reasoning capabilities.” Such advanced capabilities could confer real benefits: already, AI-designed drugs are beingtested in clinical trials.It’s not inconceivable that AI in the gray zone could save lives. VanRullen is sensitive to the risks of conscious AI—he worked with Long and Mudrik on the white paper about detecting consciousness in machines. But it is those very risks, he says, that make his research important. Odds are that conscious AI won’t first emerge from a visible, publicly funded project like his own; it may very well take the deep pockets of a company like Google or OpenAI. These companies, VanRullen says, aren’t likely to welcome the ethical quandaries that a conscious system would introduce. “Does that mean that when it happens in the lab, they just pretend it didn’t happen? Does that mean that we won’t know about it?” he says. “I find that quite worrisome.” Academics like him can help mitigate that risk, he says, by getting a better understanding of how consciousness itself works, in both humans and machines. That knowledge could then enable regulators to more effectively police the companies that are most likely to start dabbling in the creation of artificial minds. The more we understand consciousness, the smaller that precarious gray zone gets—and the better the chance we have of knowing whether or not we are in it. For his part, Schwitzgebel would rather we steer far clear of the gray zone entirely. But given the magnitude of the uncertainties involved, he admits that this hope is likely unrealistic—especially if conscious AI ends up being profitable. And once we’re in the gray zone—once we need to take seriously the interests of debatably conscious beings—we’ll be navigating even more difficult terrain, contending with moral problems of unprecedented complexity without a clear road map for how to solve them. It’s up to researchers, from philosophers to neuroscientists to computer scientists, to take on the formidable task of drawing that map. Grace Huckins is a science writer based in San Francisco. This story was part of our November/December 2023 issue. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems. It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it?

Government technology is famously bad. It doesn’t have to be.
https://www.technologyreview.com/2023/10/18/1081374/new-york-city-government-low-tech-solutions-hard-problems/
October 18, 2023
Tate Ryan-Mosley
New York City is fixing the relationship between government and technology–and not in the ways you’d expect. Every Tuesday, Jessica Ramgoolam heads down to the New Amsterdam branch of the New York City Public Library, sets up a small folding table, and takes a seat with her laptop. She lays out piles of paper flyers, and it’s clear she has information to share, like a fortune teller awaiting a passing seeker. Just before 11 a.m., when the library opens, people may begin lining up for her assistance. With the aid of her team, she can communicate with people in nearly 20 languages, and her iPhone can help her manage many more. Though she holds no unique powers of foresight, Ramgoolam represents for many the keys to the future. Sitting behind a bright yellow sign reading “GetCoveredNYC,” she’s there to help people—anyone—enroll in health care. Determining what programs you might be eligible for, gathering the bewildering amount of information required for different applications, and navigating the submission process is a headache, even for the most administratively savvy. That’s true even though most New Yorkers have already submitted information about their income and employment to the city many times over, and more and more residents get regular updates from and about the city government through websites, phone calls, chatbots, text messages, Twitter, email, Facebook and Instagram, livestreams, TV, and radio—all of which are used to communicate everything from emergency notifications to trash collection schedules. Not to mention the overwhelming volume of information online devoted specifically to the several public health-care plans available. But even with those programs and a variety of tax credits, there are still hundreds of thousands of people in the city who do not have health insurance. It’s a reality of politics that is often overlooked: once a law is passed, it needs to evolve from an idea into a plan with a budget and a staff, and from there it needs to actually reach the lives of millions of people. Moving from policy to implementation has always been a hard part of governing, but today it’s easy to assume technology can make it easier. Yet even as technology presents unprecedented opportunities to bridge the gap between government programs and the people they serve, it also brings unprecedented challenges. How do we modernize without leaving people behind? How do we increase access without unduly burdening citizens? How do we increase efficiency and make services easier to use while still protecting sensitive data? The law is a first step in regulating AI, but critics aren’t happy Today, technology is both an instrument and a medium of government, and in turn, it’s transforming the way citizens and states interact with each other. And it’s essential, even urgent, that governments understand this relationship—and how easily it can be broken, even by the tools meant to bolster it. After all, civic technology has the power to help, but noteverythingcan be technologically simplified. Not everything can be automated. Bureaucrats can make forms all day long, but they are useless if people don’t know how to use them—or if they don’t even have the resources to access them or fill them out. Which is why, every week, Ramgoolam supports uninsured New Yorkers as they navigate the ever growing, ever changing, always tangled web of online forms that promise access to affordable care. “I’ve come across, in my lifetime, so many folks who have had many detrimental issues with the health insurance system,” she told me. “What motivates me is how great it makes me feel to know that I’ve succeeded in helping someone.” New York City is something of a test lab for strategies to confront some big problems that plague the modern state. Akin to a country in the budget and bureaucratic complexity of its government, it is, and has been, dealing with the key question of how to make government work for people today. And through its experimentation, it is finding that sometimes the solution to doing big things also involves doing a lot of small things, sometimes with the lowest tech possible: a human sitting behind a table. When President Barack Obama took office in 2009, his administration was heralded as more technologically savvy than any that had come before. At the dawn of Web 2.0 and with immense faith in the power of technology to do big things, it hired the country’s first chief information officer, started the US Digital Service to modernize the executive branch, and issued a directive to “build a 21st-century digital government.” Technology was envisioned as a key to the administration’s ambitious plan for expanding access to health insurance. Yet when Healthcare.gov launched in 2013, after three years of work and a cost of more than $300 million, the website crashed. Fewer than 10 people were able to enroll on the first day. In the years since, the Healthcare.gov fiasco has turned into a sort of parable for those working in policy implementation. The program’s tech-forward approach was meant to make it easier for people to compare the costs of health-care plans and enroll in one, but at least at first, the tech failed in spectacular fashion. The crash was indicative of massive challenges that the US still faces when it comes to government use of technology. Jennifer Pahlka was serving as deputy chief technology officer of the White House Office of Science and Technology Policy at the time. As she explains in her bookRecoding America: Why Government Is Failing in the Digital Age and How We Can Do Better, the failed site launch was a reflection of just how big the “glaring gap between policy intentions and actual outcomes” really is. In the book, Pahlka—who also founded Code for America, a nonprofit that pairs engineers, designers, and product managers with government agencies to improve public services—lays out the problem. “Whether for good or for ill, the essence of the digital revolution is that it has become easier to implement ideas of all kinds—business, cultural, and social,” she writes. “Inside government, however, the digital revolution has played out very differently. Even as our expectations about the immediacy and accuracy of services have skyrocketed, the implementation of laws has become anything but easier.” In several conversations, Pahlka explained to me how well-­intended policies morph between the time they pass a legislature and the time they finally trickle through the bureaucracy and down to the lives of everyday Americans. And today, of course, the way Americans interact with those policies is so often through technology—government websites, data management and record keeping, or benefit enrollment. “Ultimately, we tell the American public we’re gonna do this thing,” she told me, “and then the actual outcome that was desired may or may not occur.” The reason, she argued, is that policy implementation has grown so complex—and technology often complicates it even further. What’s more, the American system isn’t designed to empower technology designers in this process. Instead, legislators are making the choices without necessarily understanding what technology would help carry them out most effectively. “We need to rediscover what democracy offers to us and apply that in the context of building services, making decisions, and doing regulation that works for people in a way that’s less like ‘Everybody throws their stuff in the pot of soup and then that’s what the soup is,’” she told me. Officials working on digital transformation and public services in New York, San Francisco, and Boston all told me that there is no silver bullet. Technology can be as much a part of the problem as it is part of the solution. As Cyd Harrell, the chief digital services officer of San Francisco, put it, the story of government technology is a story of the question “Why can’t we just …?” In other words, the contrast between the opportunities technology seems to offer and the challenges it often creates can make modern governing maddening. Even if the technology is promising, deploying it takes money and talent. There are challenges with procurement and integrating new systems with legacy tech. There are the realities of budgets, bureaucratic red tape,election cycles, and ever-growing legal complexities. And getting the technology itself right is no simple task, especially when citizens are accustomed to easy-to-use interfaces and information management systems from the likes of Apple, Microsoft, and Google. It’s all these things at once that make the problems with government technology so intractable. But at the same time, it’s never been more critical to improve government effectiveness. “The stakes matter more at this moment than they ever have,” says Pahlka. “The [Inflation Reduction Act] is trying to save us from a climate collapse, the CHIPS Act is trying to save us from potential national security disasters, the infrastructure [law] is trying to save us from driving over bridges that might fall. “These are all core issues where I think if the American public doesn’t see government deliver, I think it’s less that they get driven toward one party or another, and more that they get driven away from government altogether.” In fact, according to recent survey data, trust in government is near record lows. Research has shown, too, that people who have had an unpleasant experience with government services are less likely to engage in civic activities like voting—and democracy depends on this kind of involvement from the people it serves. People invest trust in their government when it works for them. And right now it isn’t working. Bridging the gap between policy and implementation is just what Ramgoolam, the health-care specialist in New York, is doing at her table. She is a staffer for the Public Engagement Unit of the New York City Mayor’s Office, which was first created by Mayor Bill DeBlasio in 2015 and was specifically designed to boost enrollment in underutilized programs. Before this, New Yorkers who needed help had to call 311 for assistance or physically show up at the offices of the Human Resources Administration. “Unfortunately,” says Adrienne Lever, the executive director of the Public Engagement Unit, “there are resources that are underused, and that is just a waste. There is a resource, and there is a person in need. We just need to figure out how to make that connection happen.” Lever told me that often those most in need of benefits are the least equipped to navigate a complex process required to access them, and the discrepancy becomes particularly acute when someone is in crisis. “PEU’s target populations are often lower income. We work with a lot of seniors. Many of them don’t have access to computers, let alone the internet. Some are homebound and don’t have the ability to go out,” Lever explains. “So with those populations in mind, even if the technology is not flawed in and of itself, they may not have the resources or the information to be able to just fill out a simple Google form.” And many applications are much more difficult to navigate than a Google form. Take New York City’s Senior Citizen Rent Increase Exemption (SCRIE) program, which enables people over 62 to have their rent frozen, depending on their income, even if a landlord raises the price. The city then reimburses the landlord through a tax credit. The PEU has reached out to 20,000 New Yorkers so far this year who might be eligible for a rent freeze. Stacy Horn set out to create something new and very New York. She didn’t expect it to last so long. Lever told me about one eligible New Yorker, whom she identified only by his first initial. D called the city asking for help renewing his enrollment in the program, but he was missing some required documentation, including a renewed lease. He also had severe cognitive and physical disabilities after suffering a stroke, which made it impossible for him to navigate the rest of the application online, or even with help over the phone. Benefit programs like SCRIE and those related to health care are particularly troublesome. They’re often the product of complex regulation that has been chewed on by many policymakers and regulatory agencies with lots of legal requirements, stipulations, and definitions, necessitating lots of compromises. The frequent upshot is that these programs are implemented only partially or with so many barriers that they are inaccessible to people most in need. As a result, many policies lose their impact. The SCRIE program, for example, had nearly 76,000 people enrolled as of 2019, though it’s estimated that around 135,000 New Yorkers were eligible, according to an October 2022 status report. Many benefit programs in the city—including Fair Fares, which offers lower public transportation prices for eligible travelers, and NYC Care, which increases access to low-cost and no-cost health care—are also underenrolled. Making matters worse, the system is always growing as more laws are written and more programs are started—but different public benefit programs are administered by different agencies, each with its own databases and registration processes. When people are eligible for a number of separate programs, which is common, they have to work through each of these agencies individually to enroll. New York doesn’t currently have a centralized database that manages city benefits, in part because of regulatory constraints that limit data sharing and in part because siloed processes and legacy technology make it difficult to stitch all these processes together. Virtually every government office across the US faces or has recently faced a similar problem. In 2015, for instance, there were over 450 different websites just for veteran services before theUS Digital Service swooped into overhaul the online registration processes through a redesign of Vets.gov. As the world moves online, policy implementation that doesn’t center citizen accessibility will increasingly lead to undersubscribed benefits programs or laws that, in practice, look very different from what their drafters intended. Vivek Kundra, who served as the first chief information officer of the United States in the Obama administration, told me that government is working, even if slowly, to adapt to this new reality. “I think we have to reimagine and even rethink what we mean when we talk about policy,” Kundra said. “There’s going to be a massive impact on the regulatory front that we haven’t even conceived yet.” New York City’s Public Engagement Unit has found that it needs to deploy low-tech interventions to bring people into the high-tech ecosystem. Consistent outreach through multiple channels is the most effective way it’s found to support people eligible for city programs as they cope with the bureaucratic complexity. Above everything else, the unit’s staffers aim to take some of the burden, technological or otherwise, off average city residents. Lever told me she believes it’s the government’s responsibility to “help people break through that struggle and find the resources they need to get access to the services that they deserve.” So the unit applies what it calls “campaign tactics” to policy implementation, proactively engaging with New Yorkers through door knocking, phone banking, text messages, emails, and public events to share information about city services like rent assistance, public transportation subsidies, and—of course—health care and help people sign up for them. “My goal and my team’s goal is to limit the technical complexity and, as much as possible, also minimize the amount of times that you have to provide the same piece of information.” The specific outreach approach depends on the population involved. For young people in the city, texts alone might do the trick. If the unit wants to target seniors, it might also start with a mass text campaign, since most people are comfortable with cell phones, and quickly move on to door knocking and in-person support for those who don’t respond to texts. To reach those who are not accessible by phone or at home, staffers work with community-based organizations and in public spaces like libraries to meet people in person. I recently tested the PEU’s system, texting the unit to ask for help with my health insurance options. I received an immediate text back and two follow-up calls the same day. When I didn’t reply, I continued to get texts and calls consistently throughout the week until I informed them that I did not need help any longer. It was almost annoying, but it was effective. The PEU has seen that people are significantly more likely to sign up for government programs when the city comes to them, whether it’s through texts, calls, or some other approach. In one study of a campaign to enroll New Yorkers in the Fair Fares program, the PEU targeted people already registered in the Supplemental Nutrition Assistance Program (SNAP), since the eligibility requirements are similar. It found that people it texted were 46% more likely to sign up for Fair Fares than those it didn’t reach out to. And eligible New Yorkers who texted back were 168% more likely to enroll. The PEU is proving that more, or more complicated, tech is not always the answer. Shiny tech-savvy government projects touted by politicians can prove to be radical letdowns. Takeblockchain voting, which West Virginia briefly piloted during the 2020 election; after much media attention, the experiment was abandoned once it was clear the technology couldn’t provide any increased security for electronic voting. Or consider the rise, and rapid fall, of education technology programs during the pandemic; at first, Zoom and personalized online lessons seemed like a great way to replace in-person teaching, but core learning metrics dipped dramatically across the country. In many cases, advances in technology meant to help implement public policy have actually harmed people they were supposed to help. Think of electronic health records, which have led to infringements onpatient privacy, andeven deaths, caused by data errors. Or the use of facial recognition in policing, which is less accurate for Black and brown people, leading to false arrests and actually decreasing public safety for large swaths of the population. But this hasn’t stopped political leaders from pinning their administrations’ fates on new technology, even in New York City. In December 2022, toward the end of his first year in office, Mayor Eric AdamstoldPolitico: “It blows my mind how much we have not embraced technology, and part of that is because many of our electeds are afraid. Anything technology, they think, ‘Oh, it’s a boogeyman. It’s Big Brother watching you.’ “No, Big Brother is protecting you,” he added. The comments have somewhat defined Adams’s style in office since. He has supported the deployment of police tech, including facial recognition, and he has prioritized incorporating technological solutions into city programs. This includes finally building a centralized database residents can use for city services—a potential one-stop shop for benefits access. “The newly launched MyCity online portal will allow New Yorkers to go online [and] easily search, apply for, and track city services and benefits right from their smartphones or computers,” Adams said in March 2023. “We are using the power of technology to reduce the bureaucracy and red tape in our government, to help New Yorkers get the services their taxes pay for, and to get stuff done for the working people of this city.” (The mayor’s press office did not respond to requests for comment.) NYC chief technology officer Matt Fraser has high hopes for the project, which will focus first on child-care benefits. (It’s a particularly daunting initial target; infant care in New York City costs over $21,000 per year on average, and according to the federal affordability standard, a household would need a combined income of over $300,000 in order to afford that.) The city offers several subsidized child-care programs, which are administered by at least three separate agencies; the sign-up process previously started with a separate paper form for each of them. In March, MyCity launched a child-care benefit portal that can screen applicants for two of the programs online and at once. “My goal and my team’s goal is to limit the technical complexity and, as much as possible, also minimize the amount of times that you have to provide the same piece of information,” Fraser told me. The ability to go to one website, be screened, and submit one application for all city programs they may be eligible for would be a major upgrade for New Yorkers who struggle to navigate so many disparate, confusing applications today. The Adams administration isn’t the first to try to achieve this, though. In fact, he’s the third mayor to attempt to centralize and streamline city benefits enrollment online. And while some more limited projects have had considerable success, like the DeBlasio administration’s redesign of the central screening tool Access NYC, no one managed to create and sustain the technology for a comprehensive centralized registration portal. Ariel Kennan, a product designer and government tech researcher who led the redesign of Access NYC in 2016, told me that MyCity’s success depends on both political will and an internal investment in designing human-centered technology. The work of building the portalhas been contracted out, as is common with government technology projects, even though Kennan notes that many similar projects have failed after outsourcing. Hiring contractors can lead to slow and expensive procurement cycles, high turnover, and minimal investment in technology and design teamswithingovernment, which ultimately makes it hard to turn digital services into sustainable, evolving solutions. Noel Hidalgo, cofounder and executive director ofBetaNYC, a civic technology organization, echoes these sentiments. “Technology is a manifestation of bureaucracy and its complexities,” he told me. “These systems are built over decades, and we need technologists and designers to go work inside of city government.” (Fraser said that government employees “remain very involved” in MyCity.) For his part, Fraser recognizes the bleak history of government’s digital services, but he told me he’s committed to making MyCity a success; he sees the project as part of a greater mission.By expanding access to benefits through an easy online interface, MyCity will help “bring equity to government,” he said, adding that other initiatives to increase connectivity, digital access, and online literacy in largely offline communities are helping the city close the digital divide. Still, there are New York residents like D, the senior citizen who was trying to renew his SCRIE benefits. For him, technology simply couldn’t replace in-person assistance. After he had an unsuccessful phone call with the PEU, one of the unit’s specialists, Hakim Hamsi, showed up at his door and walked him through the forms. Hamsi expedited D’s application, and D’s rent dropped from $1,000 a month back down to his original rate of $850. D also introduced Hamsi to a neighbor, who now helps him stay on top of his renewal forms. “All of this takes time,” says Hidalgo. “Government doesn’t work at the speed of the internet, and that’s fine—so long as it’s working to actually address these problems for New Yorkers.” This story was part of our November/December 2023 issue. Experts say its emphasis on content labeling, watermarking, and transparency represents important steps forward. Legislators are responding quickly after teens used AI to create nonconsensual sexually explicit images. Here's what you need to know. There’s still so much we don’t know about social media’s impact. But Meta president of global affairs Nick Clegg tells MIT Technology Review that he hopes new tools the company just released will start to change that.

Inside NASA’s bid to make spacecraft as small as possible
https://www.technologyreview.com/2023/10/24/1081404/small-spacecraft-engineering-physics-space/
October 24, 2023

When it comes to exploring the solar system, we must grapple with the hard limits of physics. The NASA probe’s retrorockets pressed desperately against the apricot afternoon skies of Mars. It was November 26, 2018, by Earth’s calendar. As the InSight lander worked its way down, slowing from 12,000 miles per hour to a graceful landing, overhead a pair of robots coursing through space monitored its progress. Though InSight was the size of a grand piano and the twin Mars Cube One spacecraft the size of cereal boxes, the lander was, in some sense, the easier challenge. Since the 1970s, we’ve sent a lot of big things to Mars. Until that moment, we had never sent something so small. Engineers designed the tiny travel companions to act as radio relays, sending InSight’s telemetry back to Earth. Technically their job was a nice-to-have: InSight was landing autonomously, and it would communicate with Earth via the Mars Reconnaissance Orbiter after touching down. But just making it this far heralded a new age in space exploration. And engineers were only more pleased when the Deep Space Network, a global array of radio antennas, picked up the tiny explorers’ real-time signals from Mars. InSight was healthy, said MarCO. Its parachute had deployed, the cubes added. The lander had separated from the back-shell and chute; it was on rockets now. One minute later, it was done. InSight, the small spacecraft reported, had survived. In this diminutive mission, NASA as an agency, and the community of planetary science researchers, caught a glimpse of a future long sought: a pathway to much more affordable space exploration. Each MarCO was the smallest, cheapest spacecraft ever to fly beyond the Earth-moon system. The pair cost less than $20 million to construct, launch, and operate. If engineers could build more such spacecraft—and make them even more capable in the process—they’d be an attractive alternative to multibillion-dollar flagships that launched only every 20 years or so, or even near-billion-dollar probes like InSight. The media ran with the vision. The Wall Street Journal championed MarCO as the vanguard of a new era of “swarms of tiny probes prowling the solar system.” The New York Times reported the potential for “whole fleets of MarCO-like satellites” exploring deep space. NASA had been quietly building toward the notion of small solar system explorers. In addition to greenlighting MarCO, it had launched a program to develop other small planetary probes. As MarCO sped toward Mars, making trajectory maneuvers and phoning home like any large spacecraft,Thomas Zurbuchen, then associate administrator for NASA’s science mission directorate, declared that every rocket launched by NASA’s science program would include a payload adapter for small spacecraft to hitch a lift. “We’re not going to ask whether we need it,” he said. “You have to convince us that we don’t need it.” There was a catch, though—one that NASA soon had to grapple with. Miniaturization can only go so far before it comes to a crashing halt against some very fundamental laws of physics. “Have you ever heard of a wicked problem?” Alfred Nash asks me. Five years after the InSight landing, we are in his office on the third floor of the formulation building at the Jet Propulsion Laboratory, NASA’s sprawling research and development facility in Pasadena, California. The room is sparsely decorated. He recently changed offices, and many of his things are still in boxes stacked off to the side. The moon, private space travel, and the wider solar system will all have major missions over the next 12 months. In 1973, he explains, two professors at UC Berkeley published a paper asserting that there are two kinds of problems. One type—the “tame” kind—can be solved with science and brute-force engineering. But there is another sort that is resistant to being solved with math and physics. In these complex problems, a group of stakeholders with differing value propositions want distinct and oftentimes contradictory outcomes. These are the “wicked” problems. Nash says that when people think of formulating a space mission—turning a probe from scribbles on a notepad into hardware on a lathe—they imagine only the system-level design of a spacecraft. “What valves am I going to use? That kind of thing. But the problem is much, much more complex than that,” he says. It is, indeed, a wicked problem, with scientists, engineers, and project managers pulling projects in conflicting directions. Nash helps design missions for a living. When planetary scientists from NASA and academia want to send robots somewhere strange to study something hard, they come to him and a small cadre at the JPL Innovation Foundry, not only to develop the spacecraft but to figure out what scientific measurements of a celestial object they can actually make given the budget and mass constraints that limit instrument payloads, and where such data would fit in the NASA portfolio. In one form or another, Nash has touched about half of all current JPL flight projects. Now, to show me what he’s talking about, he grabs a marker and draws a Venn diagram of three circles on his office whiteboard. He labels one desirability, one feasibility, and one viability. Where the three intersect, he writes the word possibility. “Rocket science is not the hard part of this job,” he says. Before any spacecraft flies, it must satisfy three conditions: a scientist must need the data it can collect, engineers must be able to build it, and NASA must be willing to pay for it. Each step in the development of the mission goes toward strengthening those three things simultaneously—and not everyone can get everything they want. The zero-sum principles of game theory apply; no player can do better without somebody else doing worse. “That’s mission design in a nutshell,” he says. In developing a spacecraft, Team X engineers for each subsystem work in parallel. You’re building a house all at once. While someone is building the chimney, someone else is building the roof and another is designing the air-conditioning system. NASA science missions generally come in two flavors: directed and competed. The James Webb Space Telescope was a directed mission. NASA headquarters told Goddard Space Flight Center what it wanted built, and for how much. Directed missions tend to be high dollar and high profile. Meanwhile, the asteroid mission Psyche, whichlaunched in Octoberjust after this issue went to press, took shape as a competed mission. NASA headquarters issued an “announcement of opportunity” in 2014 for institutions to propose deep-space robotic missions that cost less than $450 million. Researchers in government, industry, and academia developed missions that fit within those parameters. After 28 proposed missions were reviewed in an independent process, the agency selected Psyche, built by JPL, and Lucy, a mission developed by the Southwest Research Institute in Boulder, Colorado. It has already launched and will explore asteroids that share Jupiter’s orbit. Most prominent within the Innovation Foundry are two groups that work on such competed missions, striving to turn an inkling of an idea for a spacecraft into a concept mature enough for NASA to select for flight. Nash leads the A-Team (the A stands for “architecture”), which can bring a space mission from a mere notion to a detailed study with sharply defined science objectives and a plan for how to achieve them. Afterward, a group called Team X takes it from there, using the mission plan to design the actual spacecraft. JPL created Team X in 1995, during NASA’s so-called Faster, Better, Cheaper era. The agency had recently established a planetary program called Discovery, whose initial purpose was to launch one low-cost, tightly constrained mission every 12 to 18 months. To keep up with such an aggressive cadence, the lab needed a way to design, analyze, and evaluate mission concepts rapidly. Team X ultimately developed a system in which experienced engineers, using extensive databases of spacecraft components as well as lessons learned from practically every mission going back to 1958, concurrently design a complete and credible science mission in a matter of days—a plan for an instrumented spacecraft able to make specific measurements at specific places for a specific cost. In 2012, JPL stood up the A-Team to help scientists get a better grip on the science and architecture of their missions before subjecting them to the intensity of Team X. The A-Team, which meets in a room called Left Field (“because that’s where good ideas come from”), helps scientists develop testable hypotheses, determine the measurements and scientific instruments necessary to assess them, and work out the best type of mission to carry those instruments: perhaps it’s an orbiter, a flyby, or a six-wheeled, nuclear-powered car. In terms of Nash’s Venn diagram, the A-Team endeavors to sketch out a mission that sits in the middle: one that will give scientists the data they want, be something that engineers can build, and merit NASA’s approval to buy and fly. After leaving Left Field, scientists take their proposed mission to the Team X project design center. For two or three days, they sit beside engineers in a room that looks something like a computer science classroom, with multiple rows of workstations. Signs emblazoned with words like “Propulsion,” “Cost,” Mechanical,” and “Telecom” sit atop each console. There, the sky is not the limit. You are going to make some very hard choices. Every NASA spacecraft is an expensive box of compromises. In one corner of the room, engineers have hung a sign describing the five stages of grief. Every decision the spacecraft designers make has cascading effects. Science goals affect the instrument payload necessary for a successful mission. The instrument payload affects the command and data subsystems (which handle signals sent from Earth and data to be returned). This, in turn, affects spacecraft telecommunications hardware (which performs the actual transmissions). It can affect the power necessary to keep the spacecraft alive. And so on. If scientists desire something as seemingly simple as a higher-resolution image, dominoes can fall in such a way that the spacecraft can no longer regulate its fuel temperature or is too heavy to launch. In developing a spacecraft, Team X engineers for each subsystem work in parallel. You’re building a house all at once. While someone is building the chimney, someone else is building the roof and another is designing the air-conditioning system. Because each spacecraft subsystem affects every other, consoles are arranged so that the people who need to talk to each other can lean over and chat easily. Periodically, the team checks to see whether the spacecraft design “closes”—whether the myriad parts of the system work with each other to form an internally consistent whole that achieves its objectives given the space provided and for the correct price. These studies, which generally take three days or less, can be intense for would-be mission teams. “You’re sitting there starstruck,” says Lindy Elkins-Tanton, the principal investigator for Psyche, a mission matured in the Innovation Foundry. “All the consoles with all the experts are manipulating subsystems, and numbers are clicking this way and that, and mass and power and dollar totals are changing, and the experts are shouting back and forth with each other.”If we use this trajectory, how much xenon do you need? If we measure this instead of that, how do your power needs change? What mass is that instrument? What kind of orbit control will achieve that?She says it has an energy similar to Mission Control, in terms of focus and import. The systems do not always close—they didn’t at first with Psyche, even after two rounds with Team X. Scientists often must reconsider their goals in the face of mass, power, fuel, or funding. “During the process, I was feeling really good,” Elkins-Tanton says. “We had made the hard decisions, we brought really useful information, we could get the power, we could get the mass—and then cost came through way above the cost cap. I almost couldn’t believe it. I just thought, ‘That’s not possible.’” Eventually, the Psyche team opted to use an off-the-shelf spacecraft bus, the component that forms the main body of the spacecraft. It was much less expensive than the custom-made bus they’d originally planned to build, and that solved the cost problem. “To me, the magic of Team X is it gives you structure, and it gives you the perspective of what a mission looks like [to] a disinterested outside party,” she says. A team of scientists can go into Team X having convinced each other of the rightness of their plan, their prospective payload, and the measurements they intend to make. In some of those cases, she notes, “it is very important for someone to tell you,I’m sorry, but that does not work.” Mission competitions are relatively rare. NASA released the announcement of opportunity for its most recent small, sub-­billion-dollar Discovery-class mission in 2019. The agency will not likely release another until 2025 at the soonest—and Discovery missions are the competitions that run most frequently. Missions in the billion-­dollar New Frontiers class are rarer still, and their destinations tend to be tied to a short list identified in the Decadal Survey, a community report written by planetary scientists that is released every 10 years. The most recent list,announced in 2022, called for missions to three targets in the Saturnian system as well as three missions to small bodies, one to the moon, and one to the surface of Venus. If for no other reason, then, the availability of a new class of miniature, inexpensive spacecraft of the MarCO variety is deeply enticing to planetary scientists. In 2014, NASA headquarters created the Small, Innovative Missions for Planetary Exploration program, or SIMPLEx, to fund such small, high-risk (failure is an option)planetary science probes. In addition, in 2016 the agency commissioned concept studies for deep-space planetary science missions that could employ small satellites, individually or in constellations. There is no universally agreed-upon definition of a small sat, but they are generally less than 2,600 pounds (though they can be as small as a postage stamp). The most recent SIMPLEx announcement of opportunity limits small sats for non-Earth missions to dimensions that would fit on a specific payload adapter—about the size of a dorm-room fridge and a weight of 400 pounds. A “cube sat,” such as MarCO, is formally defined as one or more 10-centimeter-wide cubes, each weighing about four pounds. A marvel of precision engineering, JWST could revolutionize our view of the early universe. Scientists submitted 102 proposals to the 2016 study, 19 of which NASA funded for further analysis. Two years later, Zurbuchen, then the associate administrator of NASA’s Science Mission Directorate, announced an annual $100 million investment in small sats, more than half of which would go toward planetary science missions. And yet today, though small sats of every flavor circle the Earth, no swarms darken the skies of other worlds. Five years is not an enormous span of time in deep-space exploration, but JPL developed Mars Pathfinder, the first rover on another planet, in three. Surely building a Neptune orbiter the size of a shoebox should have been trivial in comparison, especially in an age of advanced semiconductors and reusable rocketry. Earlier this year, the European Space Agency launched the Juice mission to study Jupiter and three of its moons. Next year, NASA will launch Europa Clipper to the same system, focusing on the moon that could potentially harbor extant life. Neither spacecraft will carry small sat companions. No follow-on to MarCO accompanied the Perseverance rover to Mars in 2020, either. (Perseverance did carry a four-pound helicopter to the surface, though neither NASA nor JPL described it as a cube sat.) Psyche was set to launch with cube sats, but NASA put the small spacecraft in cold storage when Psyche’s schedule slipped. Agency officials have hopes, but no plans, to launch them. To the extent that the most recent Decadal Survey even mentioned cube sats or small sats, it was to request more money for them. It also recommended that NASA pursue a flagship Uranus orbiter and probe for $4.2 billion. But why put “all eggs in one basket” on a flagship, as Zurbuchen warned when announcing the $100 million program, when you could build many small sats? In part, Nash explained, it is a matter of mission design. The simple novelty of a spacecraft—in this case, an extremely small spacecraft—is not an adequate reason to fly it. A small sat sent to Mars faces a daunting task because we already know so much about the planet. It wasn’t always so. Before 1965, we had no idea what the surface of Mars looked like beyond what we could see through our Earth-bound telescopes. Every picture the Mars probe Mariner 4 returned was thus game-changing. “The universe is not so much that way anymore,” Nash says. “We’re victims of our own success. The science floor is moving up all the time.”   Two groups within the Innovation Foundry turn ideas into mature concepts for NASA: the A-Team (the A stands for “architecture”) defines the science objectives and devises the mission plan, and Team X designs the actual spacecraft. And when it comes to designing a capable small spacecraft, engineers are up against formidable foes. Nash first noticed them during the NASA-funded 2016 concept studies. The agency had sent several of the prospective projects to Team X for development. For days Nash, who at the time was its lead engineer, worked alongside Alex Austin, the lead engineer for Team Xc, the Innovation Foundry group dedicated to small sats. Amid the din of spacecraft development, with consoles working though thermal, telecom, fuel, and computer issues—“That instrument won’t work” and “That mass is too high” and “That’s not enough power”—Nash and Austin felt they were beating their heads against a wall. Forget issues like radiation shielding or autonomy at Jupiter—they couldn’t even seem to get a cube sat into orbit around Mars, something we’ve been doing with large satellites for more than 50 years. Scientists wanted images at a certain resolution, but the cube sats couldn’t carry a camera capable of it. Once at their targets, they were unable to get data back to Earth. A small sat could reach its destination but would then just fly right on by it, because it had no way of slowing down. After a particularly grim day in the project design center, they walked back to their desks, grumbling all the way, vexed and annoyed. Between the two of them, they had participated in hundreds and hundreds of studies, tackling practically every engineering problem a space program could muster—and still, the designs of these little shoeboxes proved resistant to closure. Austin pulled out his desk chair, slumped into it, and leaned back in head-pounding thought. “It’s those dead Europeans!” Nash exclaimed from his own desk. I met Austin and Nash in Left Field to find out who these Europeans were and what, exactly, their problem was. Austin is young and beardless, where Nash’s beard is going gray. A Dumbledore/Harry Potter comparison is too facile, but not absurd. “The thing that enabled small sats is the miniaturization of electronics,” says Austin. “And that’s great—but alone, it’s not enough for planetary missions, which have to deal with really hard physics first described by dead Europeans.” Thousands of small sats fly above Earth today. They can be launched from the International Space Station, from small rockets, even from high-altitude balloons. Among the more high-profile of these spacecraft are the Starlink constellation (though the size of its individual satellites is growing with each iteration); Planet Labs’ Earth-imaging Dove satellites; the Cyclone Global Navigation Satellite System developed by NASA, the University of Michigan, and the Southwest Research Institute, which measures the wind speed inside hurricanes; and NASA and MIT Lincoln Laboratory’s TROPICS, a painful acronym for “Time-Resolved Observations of Precipitation structure and storm Intensity with a Constellation of Smallsats” (which does what the name suggests). It is not a NASA-only club, either. Among others, the European Space Agency, the Indian Space Research Organization, and the China National Space Administration have launched cube sats to study everything from the weather to civil aircraft traffic. In 2020, the Japan Aerospace Exploration Agency launched one with two action figures on board, so they could be imaged in front of the Earth as part of a promotion for the Olympics and Paralympics. Given such successes in Earth’s orbit, and NASA’s desire to launch less expensive missions more frequently, it surprises no one that scientists want to send small sats to orbit other worlds. But as Nash and Austin realized, it is not so simple. You can start with pretty much any subsystem to see the limits. Consider the need for high-resolution images. The higher the resolution, the larger the aperture size of the camera must be, as the British physicist Lord Rayleigh determined in a famous equation in the 19th century. But small spacecraft can only hold cameras so large. “If someone came to me and said they absolutely want to do a cube sat at Uranus, there is some mission there—I’m just not sure if it’s the mission they really want.” Even if you could find a way around this fundamental concept in optics, you’ve got to get your data home. High-resolution images use large amounts of data. As the quantity of data grows, so do the mass requirements of the telecom subsystem and the necessary power source. The antenna and commensurate power requirements are driven by the Friis transmission equation, worked out by the Danish-American engineer Harald Friis.  Augustin-Louis Cauchy, the French mathematician who founded the field of continuum mechanics, would insist on dedicating more spacecraft mass to withstanding the mechanical stress of launch. His equations for the distribution of forces through a material say that a six-pound cube sat, atop a Falcon Heavy with 5 million pounds of thrust, will need to be pretty tough. As the spacecraft mass slowly creeps up, the reaction wheels necessary for pointing it this way or that likewise grow in size, as Isaac Newton and his laws of motion would explain. And the farther the small sat gets from the sun, the less solar energy it receives, as the Scottish physicist James Clerk Maxwell could (and did) tell you. Then there is the temperature of the spacecraft. The requirements for thermal equilibrium, which it must maintain, vary on the basis of mass and volume. A hot spacecraft needs to dissipate the heat. A cold spacecraft needs to heat itself (which also affects the power subsystems). Principles articulated by the 19th-century Austrian physicist Ludwig Boltzmann describe the difficult thermal situation for a flying shoebox. Perhaps the most daunting of all the dead Europeans is the Russian rocket scientist Konstantin Tsiolkovsky, because his rocket equation is so unforgiving. It calculates the change in a spacecraft’s velocity as it uses fuel and ejects exhaust. In short, a spacecraft’s mass diminishes as it uses propellant. The lower the mass, the faster the spacecraft flies. This becomes precarious when a spacecraft needs to slow down and enter orbit around a target: it needs more propellant yet, which requires a bigger tank to hold it, which requires more propellant to push the bigger tanks, and so on—and all this affects other subsystems like thermal protection, to keep the propellant at the correct temperature. Overcoming all these problems becomes increasingly difficult as you travel farther from Earth. But, as demonstrated in 2018, it is not impossible with the right design. Launching as soon as next year, Rocket Lab’s low-cost mission will be brief, but it could transform the search for alien biology. “MarCO was our first Team Xc study,” says Kelley Case, head of the Innovation Foundry’s Concept Office, who joined us in Left Field. “At that point, there were not that many cube sat missions, and people were excited about it.” As originally envisioned, Case explains, drawing the mission in the air with her finger, MarCO would put two cube sats in Martian orbit to do radio occultation science, sending radio signals to each other as a way of studying the atmosphere. During the Team X process, however, engineers realized that it was infeasible for the MarCO craft to fly to Mars and enter orbit on their own. “So the MarCO team pivoted, which is really important as a case study for other small sat missions,” she says. “They said,Okay, let’s not focus on what science we could do. What could we do with a cube sat at Mars, period?” The answer: solve the communications blackout invariably faced by Mars landers. “InSight was its opportunity,” she says. Austin calls MarCO a huge achievement, not only for what it did, but for what it forced scientists and engineers to do. “We are very proud of MarCO, but there is a reason that MarCO worked: It was a very focused mission. It was a flyby. They didn’t have to deal with a lot of these physics problems.” It is not that exploring other planets with small sats is impossible, he explains. “If someone came to me and said they absolutely want to do a cube sat at Uranus, there is some mission there—I’m just not sure if it’s the mission they really want.” SIMPLEx missions are standalone projects; a small sat must address all the problems of classical physics on its own (once it gets into space). A new mission class could help solve that, says Austin. “There is no medium class after SIMPLEx. You jump straight to Discovery. I would argue that NASA should consider something that is in the middle that would allow you to conquer a few more dead Europeans.” Such a class could afford larger spacecraft to ferry small sats to bodies, for example—offloading Tsiolkovsky—or carry communications relays, thwarting Friis. (In a similar fashion, there has been talk of using NASA’s Lunar Gateway space station, currently slated to launch in 2025, to deploy  small spacecraft from lunar orbit.) For now, the scientific community should take note of small sats’ successes over the Earth. “Earth science small sat missions are leading the way for planetary ones,” Austin says. They deal with vastly fewer dead-European problems, yes—but Earth scientists are also thinking in new and interesting ways about how to leverage the unique strengths of small sats, rather than simply trying to stuff big-satellite science into a small box. For example, the Investigation of Convective Updrafts (INCUS) mission, currently slated to launch in a few years, will place three cube sats in sequential orbit around Earth to study how storms form. The cube sats will not contain the most powerful radars ever built, but they will do something those radar systems cannot: visit the same place in rapid succession, seeing how storms evolve in a short period of time. “It’s not the same question a large satellite might ask, but better,” says Nash. “It is an orthogonal question. It is a question that has never been asked, because you couldn’t ask it with the other system.” So far, NASA has flown only two SIMPLEx missions, both of which failed in their primary science objectives, and neither of which was sent to targets outside the Earth-moon system. And only two of the 10 US and international cube sats that launched on the Artemis 1 moon mission flew without issue. There have also been great successes. One week before NASA’s DART intentionally crashed into the asteroid Dimorphos, it released a cube sat called LICIACube, which was built by the Italian Space Agency. The shoebox-size probe, having hitched a ride and thus evaded the tyranny of Tsiolkovsky, successfully imaged DART’s final moments. More missions are on the way. Annual budgets fluctuate, but NASA continues to fund small sats at or near the $100 million levels promised by Zurbuchen. Although most of the money goes to spacecraft studying Earth, the sun, and the stars, planetary science remains an agency priority. Among the funded projects is Lunar Trailblazer, a SIMPLEx mission to map water on the moon, which should launch next year. Other countries have upcoming projects of their own. Next year, for example, the European Space Agency will launch Hera, a follow-on to the DART mission. It will carry two cube sats to study the composition and structure of the asteroid—precisely the sort of mission that would be necessary if a killer asteroid were inbound. These may seem like small steps for small spacecraft. They are certainly still far from the vision of swarms skimming along Saturn’s rings and sampling the ice spewing from its moon Enceladus. But it is exploration nonetheless. David W. Brown is a writer based in New Orleans. His next book,The Outside Cats, is about a team of polar explorers and his expedition with them to Antarctica. It will be published by Mariner Books. This story was part of our November/December 2023 issue. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If we want online discourse to improve, we need to move beyond the big platforms.

Generative AI deployment: Strategies for smooth scaling
https://www.technologyreview.com/2023/10/10/1081117/generative-ai-deployment-strategies-for-smooth-scaling/
October 10, 2023
MIT Technology Review Insights
Sponsored Our global poll examines key decision points for putting AI to use in the enterprise. In association withAdobe, EY, Owkin After a procession of overhyped technologies like Web3, the metaverse, and blockchain, executives are bracing for the tidal wave of generative AI, a shift some consider to be on par with the advent of the internet or the desktop computer. But with power comes responsibility, and generative AI offers as much risk as reward. The technology is testing legal regimes in copyright and intellectual property, creating new cyber and data governance threats, and setting off automation anxiety in the workforce. Organizations need to move quickly to keep up with stakeholder expectations, yet they must proceed carefully to ensure they do not fall foul of regulations or ethical standards in areas like data privacy and bias. Operationally, enterprises need to reconfigure their workforce and forge partnerships with tech companies to design safe, effective, and reliable generative AI. To gauge the thinking of business decision-makers at this crossroads, MIT Technology Review Insights polled 1,000 executives about their current and expected generative AI use cases, implementation barriers, technology strategies, and workforce planning. Combined with insights from an expert interview panel, this poll offers a view into today’s major strategic considerations for generative AI, helping executives reason through the major decisions they are being called upon to make. Key findings from the poll and interviews include the following: Download the report. This content was produced by Insights, the custom content arm of MIT Technology Review. It was not written by MIT Technology Review’s editorial staff. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If OpenAI's new model can solve grade-school math, it could pave the way for more powerful systems. It outmatches GPT-4 in almost all ways—but only by a little. Was the buzz worth it?

Tapping into the ocean to combat climate change
https://www.technologyreview.com/2023/10/24/1080983/tapping-into-the-ocean-to-combat-climate-change/
No date
William von Herff, SM ’23
Chloe Dean, a grad student in the MIT–Woods Hole Oceanographic Institution joint program, is working to help harness the power of the ocean to safely sequester carbon. Chloe Dean traces her decision to go to graduate school to the day a wildfire blazed through southern Oregon. At the time, Dean was working as a lab technician at a hemp-processing startup in Oregon. She had studied environmental science as an undergraduate at the Oregon Institute of Technology but fell in love with chemistry. Working at the company was a good way to keep her lab skills sharp. Meanwhile, Oregon was experiencing its worst forest fire season in living memory. On September 8, 2020, the Almeda fire swept through Rogue Valley, destroying 2,600 homes in a single morning. Dean was living 15 miles down the road. For her, the fire was a wake-up call. It became “really hard for me to justify continuing working at this cannabis company,” she says. She’d been considering grad school, and that clinched it. So she ramped up her search for a program that would let her use her chemistry background to help combat climate change. She found it at Adam Subhas’s lab at the Woods Hole Oceanographic Institution (WHOI) in Falmouth, Massachusetts, and in 2021 she signed on as the lab’s first graduate student. Dean is now in her third year in the joint MIT-WHOI program, where she is studying ocean chemistry and working toward her lab’s lofty goal: turning the ocean into a weapon against climate change. The ocean serves as a vast reservoir of carbon, holding about 50 times more than the atmosphere does. And it plays a crucial role in the way carbon moves around on our planet. For example, seawater absorbs carbon dioxide from the atmosphere through molecular diffusion in colder waters such as those at high latitudes and releases it from warmer tropical waters. Biological mechanisms are also in play as photosynthetic plankton take in carbon dioxide and as dead organisms sink to the ocean floor and turn into carbon-rich sediment. All these processes, and many others, make up the oceanic carbon cycle. Around 31% of all the carbon humans have emitted has dissolved into the ocean, and this dissolved carbon has driven up the ocean’s acidity by 30% in the last 200 years. Ocean acidification, as this phenomenon is called, dissolves shells, limits growth of coral reefs, and ultimately threatens to disrupt food chains on a massive scale. And it’s feared that when the ocean becomes overly acidic, its ability to absorb carbon could diminish—and the ocean might even begin to release more into the atmosphere. But the ocean has ways of fighting back. Dean first began studying ocean acidification as an undergrad when she spent a summer at the University of California, Santa Barbara, researching how the phenomenon affects kelp. To her surprise, she found that the geochemistry of kelp forests makes them naturally resistant to acidification. The sediment around the seaweed is full of alkaline minerals that, when dissolved in the carbon-rich water, increase the water’s alkalinity to offset the acidity of the carbon. This neutralization process forms the basis of Dean’s research today. The Subhas lab studies the oceanic carbon cycle, which includes this “canceling out” of dissolved carbon by dissolved alkaline minerals. As sediment dissolves and alkalinity increases, the ocean is able to pull more carbon dioxide from the atmosphere. Over millions of years, this keeps atmospheric carbon in check. “The ocean has a bad case of heartburn,” Subhas says. “What we’re talking about is giving it Tums.” But what if you could accelerate that process on a large scale by introducing vast quantities of alkaline substances like lime to the ocean? Theoretically, it could help mitigate acidification. Researchers are now giving the idea, first proposed in the 1990s, serious consideration. “The ocean has a bad case of heartburn,” Subhas says. “What we’re talking about is giving it Tums.” What’s more, introducing alkaline substances to the ocean could bolster its natural tendency to sequester carbon. Researchers estimate that ocean alkalinity enhancement, or OAE, could pull several gigatons of carbon dioxide out of the atmosphere each year. But while OAEadvocatesargue that it could mitigate ocean acidification and anthropogenic climate change in one fell swoop, introducing large amounts of any substance into the ocean could have unintended ecological consequences. So Dean is studying whether it’s possible to implement OAE without collateral damage to marine life. Dean’s work focuses on coccolithophores, tiny plankton that sequester carbon by absorbing it from the atmosphere through photosynthesis. When they take in dissolved carbonate ions from the ocean to build their shells, however, they also effectively “use up” alkalinity that would otherwise increase the ocean’s ability to absorb carbon dioxide. Typically, photosynthesis absorbs more carbon than shell formation displaces, so coccolithophores generally serve as a carbon sink. But under certain conditions—such as low alkalinity—shell formation could hypothetically outpace photosynthesis and cause coccolithophores to make the ocean less efficient at absorbing carbon. This delicate balance means it’s critical to understand the intricacies of the role these plankton play in the carbon cycle. Currently, Dean is working to shed light on a phenomenon that has vexed ocean scientists since the 1970s. The top layer of the ocean, where coccolithophores are found, dissolves and recycles more calcium carbonate than thermodynamics would predict. For decades, no one was sure why, though some researchers had suggested that coccolithophores might be responsible. For her research, Dean grew coccolithophores in water that was rich in carbon-13, an isotope of carbon that is easy to track. Then she put them in a tank with microzooplankton known to prey on them and tracked where the carbon-13 went. She discovered that when coccolithophores are eaten by the microzooplankton, their calcium carbonate–rich shells are quickly dissolved within the acidic pouch that forms in the tiny animals’ bodies to help them digest their prey. After digestion, the predatory plankton jettison the pouch’s contents, including the dissolved shell. Dean found that this simple process may account for as much as 30% of the dissolved calcium carbonate near the ocean’s surface. So if adding huge volumes of alkaline substances to the ocean were to harm coccolithophores, it could alter the carbon cycle so dramatically that OAE might not work at all. Dean’s role will be to study ocean alkalinity enhancement’s effect on phytoplankton. “You can only learn so much from a lab study. At some point you have to go out,” she says. This concern prompted Dean to work with Subhas and Aaron MacDonald, an undergraduate summer researcher at WHOI, to investigate precisely how OAE affects these phytoplankton. The team grew coccolithophores for four generations under both normal conditions and the high-alkalinity conditions that would be expected with OAE. They compared the cultures’ growth rates, their photosynthetic efficiency, and the rates at which they formed shells. They found that the coccolithophores were unaffected by the increase in alkalinity, which bodes well for the technique’s potential. But it’s unclear whether the delicate balance that keeps these plankton from releasing more carbon than they take in will persist when the technique is scaled up. Dean hopes to be involved in answering that question soon. At press time, WHOI was seeking EPA approval to undertake the first open-ocean test of OAE in the summer of 2024. The LOC-NESS project (short for “Locking away Ocean Carbon in the North East Shelf and Slope”) calls for WHOI biologists, chemists, engineers, physicists, and geologists to release alkaline materials into federal waters south of Martha’s Vineyard alongside a harmless dye so they can track where it goes using satellite imaging. Then, on board a WHOI research vessel, they will simultaneously take measurements to quantify how much carbon dioxide can be pulled in through OAE and study the impact on the ocean. On shore, the team is using ocean models to guide the experiments and interpret the forthcoming results. Subhas is spearheading the project. Dean’s role will be to study OAE’s effect on phytoplankton communities, monitoring how the populations change and observing how it affects the crucial role these plankton play in the carbon cycle. “You can only learn so much from a lab study,” she says. “At some point you have to go out.” Dean hopes that her research both in the lab and at sea will help guide OAE implementation and keep the ocean safe. It’s important that we better understand OAE’s impact on marine life, she says, before we scale up and “just start dumping alkalinity out there.” Dean has always been a passionate environmentalist. Growing up in Central Oregon with a love of the forest, she looked up to forest rangers as heroes and petitioned her mother’s landlord to implement a recycling system in their building. But her family did not always share her enthusiasm. She remembers coming home from college, excited to tell them about the environmental science she’d learned, only to be met with indifference. When visiting relatives in Alabama, she told a family member about her major and the response was “Why don’t you study something that actually matters?” Coming from a family that didn’t believe in climate change until recently, Dean found ways to talk about science without pushing skeptics’ buttons. She says she learned how to avoid being “too pushy,” and how to separate the science of climate change from the politics. Her passion for sharing her excitement about science led her to begin volunteering with the nonprofit Zephyr Education Foundation in Woods Hole shortly after she started at WHOI. Zephyr takes middle school, high school, and college students on ocean “cruises” with Woods Hole researchers so they can watch marine science happen. The founder, Rob Reynolds, has been running the program for 14 years; his students explore the salt marsh around Woods Hole, take water quality measurements, and even use a dredge to collect and examine seafloor creatures. Dean has become a regular on Reynolds’s cruises, and she enjoys the chance to introduce these students, many of whom are from low-income backgrounds, to marine science. “A lot of [these] people have never even been on a boat,” she says. Some “have never even seen the ocean.” As a first-generation college student from a working-class background, Dean says, she feels a connection to them. And she sees such outreach programs as critical. “All this really cool science happens, but it really just stays in the ivory tower,” she says. “Scientists [need] to have these broader communication skills, where they can take their science outside of academia.” Introducing marine science to young people seems like a good way to do just that. Reynolds says Dean is a hit with the students. “It’s kind of funny. I mean, I put my heart and soul into these trips,” he says. “And then I get an email from the teachers. They say, ‘Oh, Chloe was terrific.’” Dean is grateful for the chance to work with Subhas (“I literally wouldn’t have wanted to work with anyone else,” she says). Living in Falmouth, however, can be difficult. Because of restrictive zoning and the profitability of short-term rentals, Cape Cod suffers from a serious housing crisis. Add to this the high price of gas and food, and the cost of living skyrockets. When Dean started at WHOI, students in the MIT-WHOI joint program got a minimum stipend of $45,480. Although that’s higher than most graduate stipends in the US, it can still be hard for grad students to make ends meet on Cape Cod. A survey by WHOI found that 16% of its graduate students have been homeless while working there. So Dean helped lead an effort to unionize the WHOI grad students. While WHOI was supportive, the issue of whether fellows could be part of the union proved complicated; as of early September, WHOI and the students were still seeking resolution on how to handle them. But WHOI announced a flat 10% stipend increase for all grad students in June, which it says was part of its annual review process and an effort to address the hardships faced by students living on Cape Cod. Dean, however, considers the raise a major victory for the unionization effort. When she reflects on her work as a researcher fighting climate change and as a union organizer, the parallels seem clear to Dean. “I just see climate change as being this issue that can bring people together, similar to how a union can,” she says. “It’s hard to be optimistic in today’s world,” she adds, but she’s convinced that the solution lies in community: “I want people to find ways to come together.” This story was part of our November/December 2023 issue. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If we want online discourse to improve, we need to move beyond the big platforms.

AI-tocracy
https://www.technologyreview.com/2023/10/24/1081074/ai-tocracy/
October 24, 2023
Peter Dizikes
The Chinese government has enhanced its power—and the country’s tech sector—by using AI-based facial recognition to help stifle dissent. It’s often believed that authoritarian governments resist technical innovation in a way that ultimately weakens them both politically and economically. But a more complicated story emerges from a newstudyon how China has embraced AI-driven facial recognition as a tool of repression. “What we found is that in regions of China where there is more unrest, that leads to greater government procurement of facial-recognition AI,” says coauthor Martin Beraja, an MIT economist. Not only has use of the technology apparently worked to suppress dissent, but it has spurred software development. The scholars call this mutually reinforcing situation an “AI-tocracy.” In fact, they found, firms that were granted a government contract for facial-recognition technologies produce about 49% more software products in the two years after gaining the contract than before. “We examine if this leads to greater innovation by facial-recognition AI firms, and indeed it does,” Beraja says. Adding it all up, the case of China indicates how autocratic governments can potentially find their political power enhanced, rather than upended, when they harness technological advances—and even generate more economic growth than they would have otherwise. The scholars are now studying the extent to which China is exporting facial-recognition tech around the world—highlighting a mechanism through which government repression could grow globally. This story was part of our November/December 2023 issue. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If we want online discourse to improve, we need to move beyond the big platforms.

Superhero U
https://www.technologyreview.com/2023/10/24/1081048/superhero-u/
October 24, 2023
Jade Durham ’25
From the invincible Iron Man to the diabolical Doctor Octopus, Marvel’s mightiest characters share ties to the Institute. Here’s why. In a workshop filled with robotic limbs and several expensive cars, the clanging of a hammer rings out over the blasting sounds of AC/DC. Amid the clamor, a man with a glowing arc reactor in his chest is hard at work with help from J.A.R.V.I.S., an AI program of his own creation. On the man’s right hand shines a golden brass rat. Tony Stark, better known as Iron Man, is perhaps the most iconic representative of MIT in Marvel comics and movies. In Iron Man’s first appearance, inTales of Suspense #39(1963), the millionaire inventor of high-end weapons and CEO of Stark Industries is kidnapped by a warlord who wants him to build a superweapon. After crafting an armored robotic suit instead—a suit that allows him to escape his captors—he realizes the harm his company is inflicting. So Stark eventually shifts his focus to more humanitarian projects and embraces the persona of a tech-assisted superhero. The brass rat Stark wears in the 2008 movieIron Manis no coincidence. Readers of the 1996 comicIron Man: The Legendlearn that he graduated from MIT at 17 with a double major in physics and engineering—an impressive but not completely beyond-the-realm-of-possibility feat at MIT in real life. (His status as valedictorian and summa cum laude graduate, however, is of course pure fiction; the real MIT doesn’t bestow these honors.) The Marvel Cinematic Universe (MCU) packs in multiple additional references to Stark’s MIT education. InIron Man, a montage shows Stark on the cover of MIT Technology Review as a student, and the Institute calls on him to give a commencement speech; inCaptain America: Civil War, Stark establishes the September Foundation, which funds the education of prodigies, in an MIT auditorium that looks a lot like Kresge. He also meets his best friend, James Rhodes, at MIT; both are seen wearing the iconic MIT rings known as brass rats inIron Man. It’s likely that Marvel’s writers gave Stark an MIT backstory to make his creation of such a sophisticated full-body robotic suit seem plausible, since such technology is as yet unachievable in the real world. How did MIT come to symbolize technological genius? While it may seem strange to ask such a question, the Institute wasn’t always as famous as it is today. “Scientists, engineers, and industrial leaders of course knew of MIT, but widespread public attention really accelerated in the mid-20th century,” says Deborah Douglas, director of collections and curator of science and technology at the MIT Museum. Doc Edgerton’s famous high-speed photos, MIT’s role in the development of radar during World War II, and the Instrumentation Lab’s contributions to the Apollo space program in the 1960s raised the Institute’s public profile, as did WGBH’s television seriesMIT Science Reporterin the late 1960s and early 1970s. Then media coverage of the MIT Daedalus Project, which in 1988 re-created the mythical flight of Daedalus by flying a human-powered aircraft between the islands of Crete and Santorini, “began to create awareness that extended well beyond the Boston Globe or the usual publications,” she says. “MIT had never received that kind of public attention, and it vaulted the Institute into a level of public recognition globally that it had never known before.” As MIT has become more widely known, it has also become a pop-culture icon. “It’s a kind of shorthand,” Douglas says. “The writer doesn’t have to go to great lengths to prove that this character is smart or knows something about science, at risk of boring the audience.” Marvel in particular uses this MIT shorthand to make fantastical technology, such as arc reactors and robotic suits that give their wearers the ability to fly, seem believable. Having an MIT genius behind these creations is an easy way to explain the unexplained. That’s particularly useful for comic book writers, who need to say a lot in a few words to leave plenty of room for illustrations. The convenience of the “MIT = genius” trope—and the popularity of Iron Man—led Marvel to create two other MIT heroes with robotic suits who recently made appearances on the silver screen. Shortly after being unmasked as Spider-Man and accused of murdering Mysterio, Peter Parker swings into a coffee shop, a piece of mail clutched tightly in his hand, to find his friends MJ Watson and Ned Leeds so they can all open their letters from MIT Admissions together. Their anticipation fades as they read, “In light of recent controversy, we are unable to consider your application at this time.” InSpider-Man: No Way Home(2021), Parker—a.k.a. Spider-Man—was confident that, having been mentored by Tony Stark, he had a good shot of getting into MIT. With access to Stark’s resources, Parker had gone from using homemade web shooters to designing a suit enhanced by nanotechnology. And he certainly had the academic credentials, having attended the highly competitive (fictional) Midtown School of Science and Technology and developed his skills as Spider-Man. So when he isn’t accepted, Parker launches a crusade to have MIT Admissions reconsider. The movie’s plot aligns with the reality that MIT does not consider a student’s connections or legacy status in admissions decisions. As Chris Peterson, SM ’13, of the MIT Admissions Office explained ina 2012 blog post, “There is only one way into (and out of) MIT, and that’s the hard way.” Another Iron Man protégé didn’t try to use the superhero’s help to get into MIT because she had already been a student there before meeting him. InInvincible Iron Man Vol. 3, #7 (2016), 15-year-old Riri Williams works late into the night in Simmons Hall, making a replica of the Iron Man suit. Much to the dismay of campus security, Riri helps herself to property from a robotics lab, skips classes, and draws several noise complaints for her work. To avoid repercussions, she flies off in her suit, meets Iron Man, and earns the moniker Ironheart. In real life, Marvel arranged to temporarily close Massachusetts Avenue in 2021 to film several scenes—including some with Riri—forBlack Panther: Wakanda Forever. The character is slated to have her own show on Disney+, but the timing of its release is uncertain, given the strikes in Hollywood. In between Tony Stark and Riri Williams, another set of MIT students also made a brief appearance in the Marvel universe. One late autumn night in Cambridge, Massachusetts, the MIT chairman’s office is abuzz with activity. It is not occupied by its owner, but instead by a group of students and a half-assembled tractor. Despite their precautions to avoid the night watchman, their professor catches the hackers mid-prank—and distracts the approaching chairman so that they can flee on the ledges of the building’s exterior. Nearly 20 years after Iron Man’s debut in the comics but long before he showed up on the silver screen as an MIT alum, MIT figured prominently in another Marvel series.Spitfire and the Troubleshooters, a short-lived 1980s title, introduces readers to a motley crew of five MIT undergraduates and their structural engineering professor, Jennifer Swensen. The students, known as the Troubleshooters, assist Swensen in her quest to rescue the robotic “Spitfire” suit that her father invented to help construction workers—before it becomes militarized by her father’s murderer. To do this, the Troubleshooters draw upon their hacking skills, which they’d previously honed to construct that tractor and later to sabotage the opposing crew team during an MIT-Yale race. The hacking hijinks chronicled inSpitfire and the Troubleshooterswere inspired by author Eliot R. Brown’s trips to MIT to visit a student who’d been his childhood friend. During those visits, they “would run around on the rooftops” and “break into rooms and swipe ladders and climb up the sides of buildings,” Brown told the comics magazine Back Issue! in 2009.In addition to depicting the MIT campus, Brown and his coauthors aimed to present a more realistic version of the whole robotic-suit idea. “It was more than ‘Iron Girl’—it was a different technological attitude, more reality-based. Iron Man was not unreal, but Iron Man as a concept did too much,” Brown explained in the Back Issue! interview. “I tried to do something that was more of a garbage can with legs and a good brain in it, something more mechanical.” While Brown’s depiction of MIT characters and their exploits was comparatively true to life, some of the Troubleshooters’ actions violate two of the hacking principles listed in MIT’sMind and Hand Book: leave no damage and do not steal anything. But then again, not all MIT characters in the Marvel Universe have the world’s best interests at heart. Inside a darkened MIT lab space with a “RESTRICTED” sign on the door, a student is filling chalkboards with complex equations inspired by his classroom lectures. A professor enters the room, outraged at this student’s unauthorized presence. But the ingenious calculations on the wall draw him to halt and lead him to offer the student, Otto Octavius, a research opportunity with his lab. Readers of theSpider-Man/Doctor Octopus: Year 1comic series learn that Octavius conducted extensive nuclear research while an undergraduate at MIT, just as MIT’s Undergraduate Research Opportunities Program (UROP) lets undergrads work on meaningful research in real life. Octavius’s work is funded in part by the government and gives him an accelerated track to a doctorate. But when one of his experiments goes awry, Octavius’s mind is warped, and he gains four robotic arms that assist in his new life of crime as the evil Doctor Octopus (or Doc Ock for short). Several other Marvel villains also have MIT credentials. InIron Man Vol. 3(1999), readers find out that Sunset Bain (Madame Menace) attended MIT at the same time as Stark. Bad guys Quentin Beck (a.k.a. Mysterio) and Dr. Jonathan Ohnn (a.k.a. the Spot, because he can teleport through spots on his body) are noted to have been roommates at MIT inSymbiote Spider-Man Vol. 1,#1. In an alternate universe in the Disney+ showWhat If ... ?, Tony Stark’s friend Rhodey does a background check into Erik Stevens (the villain known as Killmonger) and finds out that he attended MIT. Like most MIT alumni, these villains have a knack for science and engineering. Mysterio uses chemistry to develop convincing special effects for evil purposes, Killmonger leverages his extensive knowledge of weaponry, and Doc Ock wields his robotic arms to wreak havoc on New York, often attempting to steal research equipment. Since they use their technical skills for evil, it would seem that Marvel villains flout a core tenet of the MIT values statement: “Together we possess uncommon strengths, and we shoulder the responsibility to use them with wisdom and care for humanity and the natural world.” Not all these evil characters with MIT backstories are doomed to a life of villainy, though. In one story arc from 2013, Octavius switches minds with Peter Parker to escape his imminent death from cancer. But with the dying wish of Parker, Octavius adopts a new identity as Superior Spider-Man. In this second chance at life, he founds Parker Industries, a conglomerate whose work includes developing cybernetic prosthetics and biomedical devices, and finds a girlfriend, Anna Maria Marconi. Long after Octavius relinquishes Parker’s body to its original host to save Marconi, she asks Parker if Octavius was “a good man who did bad things, a bad man who did good things, or a bit of both.” Complex and nuanced characters like this show that even those with a track record of evil have the potential to change their ways and contribute to the world. Marvel’s sometimes extreme versions of MIT students make for engaging, and often inspiring, stories. And the Institute’s prominent role in the Marvel Cinematic Universe has clearly evoked pride in the MIT community. In 2019, students adorned the Lobby 10 dome with theCaptain America shieldwhenAvengers: Endgamehit the theaters. And in 2022, they raisedWakanda Forever bannersabove the entrance at 77 Mass. Ave. to mark the release of theBlack Panthersequel. Riri Williams, who appears on murals around campus and is depicted on the Class of 2020 brass rat, was featured in the MIT Admissions Office’sPi Day 2017video (in which she was played by Ayomide Fatunde ’18), generating a lot of media coverage and breaking Admissions Office video viewership records. Several professors have also referenced Marvel in their class assignments. For example, in 2023’s 2.007 Design and Manufacturing class, students designed robots that competed in a challenge called Machina Forever. WhenWakanda Foreverwas released, the MIT Lecture Series Committee screened it in 26-100, and class councils rented theaters to see the film. And Robert Downey Jr., the actor who played Iron Man, visited campus—and met with Professor Hugh Herr in MIT’s Y. Lisa Yang Center for Bionics—in the summer of 2022. The real-world impact of MIT’s recurring role in Marvel story arcs goes beyond scenes filmed on campus and Marvel-inspired hacks. The complexity of the MIT characters reminds viewers and readers that not all MIT students are cut from the same cloth. And the range of heroes, villains, and those in between shows that everyone can make an impact. But equally important, stories like those of Iron Man, Riri, and the Troubleshooters counterbalance what can be the unglamorous reality of working to advance knowledge and educate students. These tales highlight what is exciting about science and technology—and just might inspire the inventors and innovators of tomorrow. As Douglas says, “We don’t like our stories about committees. We like our stories about heroes and heroines.” This story was part of our November/December 2023 issue. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If we want online discourse to improve, we need to move beyond the big platforms.

Energy-storing concrete
https://www.technologyreview.com/2023/10/24/1081067/energy-storing-concrete/
October 24, 2023
David L. Chandler
A mix of cheap, abundant materials could hold electricity from wind or solar in foundations or roads. A supercapacitor made from cement and carbon black (a conductive material resembling fine charcoal) could form the basis for a low-cost way to store energy from renewable sources, according to MIT researchers. The amount of power a capacitor can store depends on the total surface area of its conductive plates. Professors Franz-Josef Ulm, Admir Masic, and Yang Shao-Horn and colleaguesfoundthat if carbon black is introduced into a mixture with cement powder and water, the water naturally forms a branching network of openings when the resulting concrete cures—and the carbon migrates into that network to make wire-like structures, yielding a conductive material with an extremely large internal surface area. Two electrodes made by soaking this material in a standard electrolyte, separated by a thin space or an insulating layer, form a very powerful supercapacitor, the researchers found. A cube about 3.5 meters across could store about 10 kilowatt-hours. The simple technology could eventually be incorporated into the concrete foundation of a house, where it could store a day’s worth of energy. The researchers also envision a roadway that could provide contactless recharging for electric cars as they travel. It’s “a new way of looking toward the future of concrete as part of the energy transition,” Ulm says. This story was part of our November/December 2023 issue. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If we want online discourse to improve, we need to move beyond the big platforms.

Low-power underwater communication
https://www.technologyreview.com/2023/10/24/1081070/low-power-underwater-communication/
October 24, 2023
Adam Zewe
A battery-free system could send signals across kilometer-scale distances to aid climate monitoring and more. MIT researchers have demonstrated a technology that can transmit underwater signals much farther than existing methods, using only about a millionth as much power. The system is based on backscatter communication, a method of encoding data in sound waves that are reflected from the sound source, or interrogator, back to a receiver in the same location. The underwater backscatter device uses nodes made from piezoelectric materials, which produce an electrical signal when a mechanical force—including sound waves—is applied. The nodes use that charge to scatter some of the acoustic energy back to the receiver. To make the system more efficient, the researchers used a 70-year-old technology called a Van Atta array, in which symmetric pairs of antennas are connected so that energy is reflected back in the direction it came from, and placed a transformer between pairs of connected nodes. It can be used with data-collecting sensors and send data to a ship or onshore station. In tests, the device achieved ranges of 300 meters, more than 15 times longer than previously demonstrated—and a model suggests that kilometer-scale ranges are possible. That could make it suitable for things like coastal hurricane prediction and climate modeling. “There are still a few interesting technical challenges to address, but there is a clear path from where we are now to deployment,” says Fadel Adib, director of the Signal Kinetics group in the MIT Media Lab and the senior author oftwopaperson the work. This story was part of our November/December 2023 issue. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If we want online discourse to improve, we need to move beyond the big platforms.

A clever shield against photo fakery
https://www.technologyreview.com/2023/10/24/1081064/a-clever-shield-against-photo-fakery/
October 24, 2023
Melissa Heikkilä
AI makes it easy to tamper with images online, but an MIT-built system subtly alters them to foil the manipulation. Remember that selfie you posted last week? There’s currently nothing stopping someone from taking it and editing it with AI—and it might be impossible to prove that the resulting image is fake. The good news is that a new tool created by researchers at MIT could prevent this. The tool, calledPhotoGuard, works like a protective shield by altering photos in tiny ways that are invisible to the human eye but prevent them from being manipulated. If someone tries to use an editing app based on a generative AI model such as Stable Diffusion to manipulate an image that has been “immunized” by PhotoGuard, the result will look unrealistic or warped. Right now, “anyone can take our image, modify it however they want, put us in very bad-looking situations, and blackmail us,” says Hadi Salman, a PhD student at MIT who contributed to the research. PhotoGuard is “an attempt to solve the problem of our images being manipulated maliciously by these models,” says Salman. The tool could, for example, help prevent women’s selfies from being made intononconsensual deepfake pornography. The MIT team used two different techniques to stop images from being edited using Stable Diffusion. In the first, PhotoGuard adds imperceptible signals to the image so that the AI model interprets it as something else, such as a block of pure gray. In the second, it disrupts the way the AI models generate images, essentially by encoding them with secret signals that alter how they’re processed by the model, so any edited image looks like that gray block. For now, the technique works reliably only on Stable Diffusion, an open-source image generation model. In theory, people could apply this protective shield to their images before they upload them online, says Aleksander Madry, SM ’09, PhD ’11, a professor of electrical engineering and computer science who contributed to the research. But a more effective approach, he adds, would be for tech companies to add it to images that people upload into their platforms automatically—though it’s an arms race, because new AI models that might be able to override any new protections are always coming out. This story was part of our November/December 2023 issue. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If we want online discourse to improve, we need to move beyond the big platforms.

The tale of a carbon wrangler
https://www.technologyreview.com/2023/10/24/1081099/the-tale-of-a-carbon-wrangler/
No date
Kathryn M. O’Neill
Julio Friedmann ’88, SM ’90 Humanity has dumped more than 2.2 trillion tons of carbon dioxide into Earth’s air and oceans, and Julio Friedmann ’88, SM ’90, has made it his job to undo the climate damage. The solution is not only to stop putting CO2into the environment but also to take it out, says Friedmann, who calls himself a “carbon wrangler.” He is working on this goal as chief scientist at Carbon Direct, which invests in companies focused on carbon capture, carbon removal, and clean fuels. Trained in geoscience and music at MIT, Friedmann was introduced to climate science in 1993 while earning his PhD in geology at the University of Southern California. His first job after USC, though, was as a scientist at ExxonMobil. “It seems counterintuitive, but it was wonderful,” he says. He gained collaboration skills and learned how an energy company operates. In 2002 he took an adjunct faculty position at the University of Maryland, where he discovered the emerging field of carbon capture and storage (CCS). “This is where the puzzle pieces fit together,” he says. “I could take everything I learned at Exxon, run it backwards, and solve a major problem.” CCS was so new that he became an international expert in 18 months. That led him to Lawrence Livermore National Laboratory, where he was chief energy technologist before becoming principal deputy assistant secretary for the Office of Fossil Energy in the US Department of Energy. Over the years, Friedmann has adjusted his career focus. “In 2002, I saw that we did not have the science and technology we needed to do carbon management at scale,” he says. So his research spanned such topics as smart grids, geothermal power, and CCS. When he realized that the technology problem had been substantially addressed at the end of his government stint, he joined Columbia University’s Center on Global Energy Policy and investigated policies to decarbonize manufacturing, support green hydrogen, and incentivize CCS. Once the policy problem seemed well in hand, he moved on to his current position. “Now it’s about deployment,” he says. “That’s done in the private sector.” Carbon Direct tackles what he calls “the hardest parts of the problem,” like decarbonizing steel and concrete production, which together account for roughly 15% of global CO2emissions. One company it supports developed a novel way to make low-carbon cement, and another uses minerals as a sorbent for direct air capture of CO2. “Those technologies weren’t real five years ago, but they’re real now,” he says, “so you better put money to work on them.” This story was part of our November/December 2023 issue. The tool, called Nightshade, messes up training data in ways that could cause serious damage to image-generating AI models. New neuroscience is challenging our understanding of the dying process—bringing opportunities for the living. An exclusive conversation with Ilya Sutskever on his fears for the future of AI and why they’ve made him change the focus of his life’s work. If we want online discourse to improve, we need to move beyond the big platforms.

The latest iteration of a legacy

Advertise with MIT Technology Review

